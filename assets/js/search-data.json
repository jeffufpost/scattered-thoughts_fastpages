{
  
    
        "post0": {
            "title": "Scraping COVID-19 data from data.gouv.fr",
            "content": ". Introduction . As the summary explains, this blog post will very quickly explain how to automatically download French government data on hospitalization and testing pertaining to COVID‚Åª19. . Data sources . Hospitalization data | . The various datasets concerning hospitalization data are found here. . If you follow the link you will find 4 csv datasets concerning hospitalization data along with 5 other csv files with metadata and documentation. . Testing data | . The various datasets concerning testing data are found here. . If you follow the link you will find 2 csv datasets concerning testing data along with 2 other csv files with metadata and documentation. . In both cases we want to download the first of the links since they contain the pertinent daily updated data (do have a look manually at the metadata and documentation files to make sure this is what you want). . Code . #collapse_show # Import libraries used below import requests import urllib.request import urllib.parse import time import io from bs4 import BeautifulSoup import pandas as pd import datetime import os . . Getting the main page . Let&#39;s first have a look ath the main landing page that I provided above. . # Store URL for each page url_cases = &#39;https://www.data.gouv.fr/fr/datasets/donnees-hospitalieres-relatives-a-lepidemie-de-covid-19/&#39; url_tests = &#39;https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-tests-de-depistage-de-covid-19-realises-en-laboratoire-de-ville/&#39; . # Get response for each URL response_cases = requests.get(url_cases) response_tests = requests.get(url_tests) . The response here should be 200 (see life of codes here). . print(response_cases, response_tests) . &lt;Response [200]&gt; &lt;Response [200]&gt; . # Save the actual content of the page returned with BeautifulSoup soupcases = BeautifulSoup(response_cases.text, &quot;html.parser&quot;) souptests = BeautifulSoup(response_tests.text, &quot;html.parser&quot;) . # Let&#39;s look at the links in the main page (for testing data - if you want cases, replace souptests with soupcases below) for i in range(len(souptests.find_all(&#39;a&#39;, class_=&quot;btn btn-sm btn-primary&quot;))): print(souptests.find_all(&#39;a&#39;, class_=&quot;btn btn-sm btn-primary&quot;)[i].get(&#39;href&#39;)) . None https://www.data.gouv.fr/fr/datasets/r/b4ea7b4b-b7d1-4885-a099-71852291ff20 None https://www.data.gouv.fr/fr/datasets/r/72050bc8-9959-4bb1-88a0-684ff8db5fb5 None https://www.data.gouv.fr/fr/datasets/r/971c5cbd-cd80-4492-b2b3-c3deff8c1f5e None https://www.data.gouv.fr/fr/datasets/r/db378f2a-83a1-40fd-a16c-06c4c8c3535d https://www.data.gouv.fr/fr/datasets/r/49ba79e6-0153-40b1-b050-821e102959eb None https://www.data.gouv.fr/fr/datasets/r/59e82d52-e07a-4ae8-9a49-2d1fd2d2ec21 . We see that the petrtinent file in each cases (testing or hospitalization data) are the first links in their page. So we save only this one as donw below: . # If we want to save that first URL we can do as follows casescsvurl = soupcases.find_all(&#39;a&#39;, class_=&quot;btn btn-sm btn-primary&quot;)[1].get(&#39;href&#39;) testscsvurl = souptests.find_all(&#39;a&#39;, class_=&quot;btn btn-sm btn-primary&quot;)[1].get(&#39;href&#39;) . Getting the CSV data . We now have the URL for the CSV files we want so we&#39;ll do similar steps as above to download these files. . # Similaraly as above, requests.get the CSV URL: rectests = requests.get(testscsvurl) reccases = requests.get(casescsvurl) . What to do with the CSV data . Now that you have the data, what to do with it? . It depends on your purpose I guess: . First write the data to a CSV file which you then read | Directly read the data | . By first writing the CSV file to drive . # This will write the data into cases.csv file # Of course you need to replace the actual path to the folder you want in the code below: with open(os.path.join(&quot;/path/to/folder&quot;, &quot;cases.csv&quot;), &#39;wb&#39;) as f: f.write(reccases.content) . # Same thing for testing data # This will write the data into tests.csv file # Of course you need to replace the actual path to the folder you want in the code below: with open(os.path.join(&quot;/path/to/folder&quot;, &quot;tests.csv&quot;), &#39;wb&#39;) as f: f.write(rectests.content) . # You can then read that csv file to use in your data analysis: tests = pd.read_csv(&#39;tests.csv&#39;, sep=&#39;;&#39;, dtype={&#39;dep&#39;: str, &#39;jour&#39;: str, &#39;clage_covid&#39;: str, &#39;nb_test&#39;: int, &#39;nb_pos&#39;: int, &#39;nb_test_h&#39;: int, &#39;nb_pos_h&#39;: int, &#39;nb_test_f&#39;: int, &#39;nb_pos_f&#39;: int}, parse_dates = [&#39;jour&#39;]) cases = pd.read_csv(&#39;cases.csv&#39;, sep=&#39;;&#39;, dtype={&#39;dep&#39;: str, &#39;jour&#39;: str, &#39;hosp&#39;: int, &#39;rea&#39;: int, &#39;rad&#39;: int, &#39;dc&#39;: int}, parse_dates = [&#39;jour&#39;]) . Note in the code above I had previously looked through the raw csv data to underdstand how to parse it. . Directly reading the data (bypassing the writing CSV file step) . cases = pd.read_csv(io.StringIO(requests.get(casescsvurl).content.decode(&#39;utf-8&#39;)), sep=&#39;;&#39;, dtype={&#39;dep&#39;: str, &#39;jour&#39;: str, &#39;hosp&#39;: int, &#39;rea&#39;: int, &#39;rad&#39;: int, &#39;dc&#39;: int}, parse_dates = [&#39;jour&#39;]) tests = pd.read_csv(io.StringIO(requests.get(testscsvurl).content.decode(&#39;utf-8&#39;)), sep=&#39;;&#39;, dtype={&#39;dep&#39;: str, &#39;jour&#39;: str, &#39;hosp&#39;: int, &#39;rea&#39;: int, &#39;rad&#39;: int, &#39;dc&#39;: int}, parse_dates = [&#39;jour&#39;]) . Other stuff . Parsing/Converting URI into readable format . It sometimes happends that links are provided in URI (URL symbols encoded into % symbols...) . You generally need to convert those back to correct URLs, example below: . # Example URI testurl = &#39;https%3A%2F%2Fstatic.data.gouv.fr%2Fresources%2Fdonnees-hospitalieres-relatives-a-lepidemie-de-covid-19%2F20200505-190040%2Fdonnees-hospitalieres-covid19-2020-05-05-19h00.csv&#39; . # Convert with following line: urllib.parse.unquote(testurl) . &#39;https://static.data.gouv.fr/resources/donnees-hospitalieres-relatives-a-lepidemie-de-covid-19/20200505-190040/donnees-hospitalieres-covid19-2020-05-05-19h00.csv&#39; . A quick look at French testing data from scratch . Let&#39;s quickly see how, from scratch, we can use code above to scrape testing data and plot it quickly. . Note the data only includes city testing centers and does not include hospital testing. . # Use main page URL url_tests = &#39;https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-tests-de-depistage-de-covid-19-realises-en-laboratoire-de-ville/&#39; response_tests = requests.get(url_tests) . # Find correct CSV file URL souptests = BeautifulSoup(response_tests.text, &quot;html.parser&quot;) testscsvurl = souptests.find_all(&#39;a&#39;, class_=&quot;btn btn-sm btn-primary&quot;)[1].get(&#39;href&#39;) . # Read CSV file into tests variable rectests = requests.get(testscsvurl) tests = pd.read_csv(io.StringIO(requests.get(testscsvurl).content.decode(&#39;utf-8&#39;)), sep=&#39;;&#39;, dtype={&#39;dep&#39;: str, &#39;jour&#39;: str, &#39;hosp&#39;: int, &#39;rea&#39;: int, &#39;rad&#39;: int, &#39;dc&#39;: int}, parse_dates = [&#39;jour&#39;]) . #collapse_hide import plotly.express as px import plotly.graph_objects as go # We want overall testing for France, se we groupby Day and sum: (filtering for clage_covid = 0 means not differentiated between age groups) df = tests[tests.clage_covid==&#39;0&#39;].groupby([&#39;jour&#39;]).sum() fig = go.Figure(data=[ go.Bar(name=&#39;Positive tests&#39;, x=df.index, y=df.nb_pos, marker_color=&#39;red&#39;), go.Bar(name=&#39;Total tests&#39;, x=df.index, y=df.nb_test, marker_color=&#39;blue&#39;) ]) fig.update_layout( title= &#39;Daily positive and total testing data in France&#39;, xaxis_title = &#39;Date&#39;, yaxis_title = &#39;Number of tests (total and positive)&#39;, barmode=&#39;group&#39; ) fig.show() . . . . Conclusion . Very easy to incorporate this into a python script to automate. . This is only the very basic of scraping, a lot more could be done, maybe in another blog post. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/scraping/covid-19/2020/04/15/web-scraping.html",
            "relUrl": "/scraping/covid-19/2020/04/15/web-scraping.html",
            "date": " ‚Ä¢ Apr 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Testing - what to be aware of",
            "content": ". Motivation for write-up . The real-world motivation for this write-up can be found under Story Time section, but I first wanted to give a bit of theoretical background here. . The importance of testing has been greatly talked about these last few weeks/months with the emergence of the COVID-19 pandemic with numerous articles being published, all underlining the importance of testing. The part emphasized is the fact that early testing allows for quick isolation of sick individuals and tracing of their potential contacts, and thus limiting the potential for spread. . The kind of test for this are called virologic testing and test directly for the presence of virus in an individual (active infection). This is done with Nucleic Acid Tests, or NAT, usually after amplification of the very small amount of genetic material present via Polymerase Chain Reaction. Results are available within hours or days and require diagnostic machinery and specialists. . Knowing who has been infected is also important as it could allow already recovered patients (who are thought to gain immunity from COVID-19) to return safely to work and live basically normally. Tests that check for past infections exist, and are called serology or antibody tests. They check for specific antibodies that match those deveopped during an immune response response against SARS-CoV-2. . This is all good in theory, but with a disease that can cause such serious conditions as COVID-19 can, we need to be sure a positive test means for certain that person is now immmune, or we risk allowing individuals with false positives to return to normal when they should not, and continue the damaging spread of the disease. . The aim of this short right-up is to clear up some misconceptions around testing protocols, discuss the importance of false positives, false negatives, and its importance to guiding public health policies. The idea is basically to answer the following questions: . How many tests should return positive for a person to be, say 95% or 99% person sure he is now immune? | What if a different test is negative? | . Specificity, Sensitivity, False positives, False negatives? . As briefly explained above, neither virological and serological tests are infallible. False positives i.e. healthy individuals with a positive test, and false negatives i.e. infected indiviuals with negative tests, can, and do happen. . There are numerous reasons how and why this can happen, but that is not the point of this write-up. Here, we acknowledge the fact non-perfect tests are a reality and establish testing protocol to deal with that fact. . Thankfully, before being shipped out, the various laboratories test their tests. They are able to characterize them rather precisely and give an indiction of how useful they may be with two important values: . Specificity | Sensitivity | . Specificity . Specificity is the true negative rate - i.e. the percentage of healthy people correctly identified as such (for antibody testing, it is the percentage of people not having antibodies correctly identified as such). . In other words, if a test was used on 100 people who do not have antibodies, the number of people correctly identified as not hvaing antibodies is the specificity. . A perfect test with 100% specificity, means there are no false positives. This has major implications in the current context of COVID-19 pandemic as having an anitbody test with 100% specificity would allow immune people to know so for certain (as long as research showed antibodies gave immunity). . Mathematically, we pose specificity as follows: . $Specificity = frac{True negatives}{True negatives + False posiives}$ . Sensitivity . Sensitivity is the true positive rate - i.e. the percentage of infected people correctly identified as such (for antibody tests, it is the percentage of people having antibodies correctly identified as such). . In other words, if an antibody test was used on 100 people with antibodies, the number of people correctly identified as having anitbodies is the sensitivity. . A perfect test with 100% sensitivity, means there are no false negatives. . Mathematically, we pose specificity as follows: . $Sensitivity = frac{True positives}{True positives + False negatives}$ . Prevalence . Prevalence is simply the proportion of a population that has a certain characteistic. In the current context of antibody testing, the prevalence will be defined as the proportion of people who have antibody conferring immunity to COVID-19 (i.e. the proportion that has had the disease). . $Prevalence = frac{ # People with antibodies}{Total number of people}$ . Where $Total number of people$ is simply $ # People with antibodies + # People without antibodies$ . Story time - Part 1 . Specificity, sensitivity, prevalence, false negatives, false positives.. This is all good, but it can be a bit abstract outside of a specific testing context. . Let&#39;s use the current COVID-19 pandemic as an example. . Antibody tests are finally becoming available to the general population, and you want to know if you&#39;ve had the disease (developped antibodies against it). . Now let&#39;s say you had influenza like symptoms back in January or February, would you expect a positive or negative result on the test? | What if you haven&#39;t been sick but want to check out of curiosity, what result would you expect? | If it does come back positive, how certain would you be that you actually have those antibodies and it wasn&#39;t a false positive? | You decide to use a second test to make sure, again it comes positive. Now how certain are you that you have antibodies? | Out of extreme precaution you decide to try a test from another laboratory (different specificity and sensitivity), and this time the test comes back negative. It&#39;s become a bit more complex to evaluate your situation now. | So how about another test from this second laboratory? Again, negative.. Two positives, two negatives - what can you make of this information? | . However far fetched this scenario may seem, it is exactly what happened to this Florida physician: In January I got very sick (flu like illness but much worse), no sleep for 2 days and almost checked myself into the ED. Had no idea what it was. Today I checked my #COVID19 Antibody status......IgG+ Only (sign of past infection). Mixed emotionsüôÑ. Will retest tomorrow. pic.twitter.com/ExLYi5qPBx . &mdash; Peter Antevy (@HandtevyMD) April 2, 2020 . There are two questions that come out of this story: . After those 4 tests, what is the probability that Dr. Antevy has those antibodies - or more generally, can we calculate the probability of someone having antibodies given their test results? | What should be the threshold of such a probability to minimize the risk of someone without antibodies going out in nature thinking he does ? (obviously if someone has 10 positive tests in a row, it seems sure enough that person has antibodies) This pushes for the need of rigorous testing protocol. | . Calculating probabilites given test results . Clearly, our objective is to calculate the probability that a person has antibodies, or: . $P(seropositive)$ . Conditional probabilities . Baye&#39;s theorem describes probabilities when given evidence. . Say a person has had some COVID-19 symptoms (dry cough, fever, loss of smell, slight fever) a few weeks ago. He might say there is a 75% chance that he had contracted COVID-19, and 25% chance it was another disease. In this case: . $P(seropositive) = 0.75$ . Now this person goes to get an antibody test. What is the probability he is seropositive given a positive or negative result? Baye&#39;s theorem allows us to write it as follows: . $P(seropositive | positive test) = frac{P(positive test | seropositive) * P(seropositive)}{P(positive test)}$ . and . $P(seropositive | negative test) = frac{P(negative test | seropositive) * P(seropositive)}{P(negative test)}$ . Note: . $P(seropositive)$ is called the prior. . $P(seropositive | positive test)$ and $P(seropositive | negative test)$ are called the posterior. . $P(Positive test)$ . Let&#39;s have a look at the probability of getting a positive test - there are 2 ways to get a positive result : . A false positive | A true positive | . $P(False positive) = P(Positive test | seronegative)*P(seronegative)$ . And . $P(True positive) = P(Positive test | seropositive)*P(seropositive)$ . So: . $P(Positive test) = P(Positive test | seropositive)*P(seropositive) + P(Positive test | seronegative)*P(seronegative)$ . Sensitivity and Specificity revisited . Earlier we saw: . $Sensitivity = frac{True positives}{True positives + False negatives}$ . And that . $Specificity = frac{True negatives}{True negatives + False positives}$ . But we can rewrite these equations as follows: . $Sensitivity = P(Positive test | seropositive)$ . And . $Specificity = P(Negative test | seronegative) = 1-P(Positive test | seronegative)$ . Re-writing the posterior probability . Using Baye&#39;s rule and the calculations above we can re-write the posterior equations as follows: . $P(seropositive | Positive test) = frac{Sensitivity*P(seropositive)}{Sensitivity*P(seropositive)+ (1-Specificity)*(1-P(seropositive))}$ . And: . $P(seronegative | Negative test) = frac{Specificity*(1-P(seropositive))}{Specificity*(1-P(seropositive))+(1-Sensitivity)*P(seropositive)}$ . The role of prevalence in these calculations . The equations above describe the probability for an individual given a test result and their prior probability. This prior probability can be estimated given presence or not of symptoms, contact with other infected individuals, location, other diagnostics, etc... . However, on a population level, if we were to test a random individual, this prior becomes the prevalence and for a random individual, the equations become: . $P(seropositive | Positive test) = frac{Sensitivity*Prevalence}{Sensitivity*Prevalence+(1-Specificity)*(1-Prevalence)}$ . And: . $P(seronegative | Negative test) = frac{Specificity*(1-Prevalence)}{Specificity*(1-Prevalence)+(1-Sensitivity)*Prevalence}$ . Serology testing simulation . Let&#39;s see what these equations look like in practice. . #collapse_hide import numpy as np import plotly.express as px import plotly.graph_objects as go . . #collapse_hide # Let&#39;s write a function to output the posterior probability given prior, test result, and test characteristics (sensitivity and specificity) def Pposterior(Pprior, test_res, Sn, Sp): if test_res: return ((Sn * Pprior) / (Sn * Pprior + (1-Sp) * (1-Pprior))) else: return (1-((Sp * (1-Pprior))/(1-(Sn * Pprior + (1-Sp) * (1-Pprior))))) . . Say we have an antibody test with 90% sensitivity and 90% specificity - meaning we have 90% true positives and 90% true negatives, we obtain a graph as below: . #collapse_hide # Below is the prior probability of being infected: num=10000 Pprior = np.linspace((1/num),(num-1)/num,num=num) # Graph the results fig = go.Figure(data=[ go.Scatter(name=&#39;Test negative&#39;, x=100*Pprior, y=100*Pposterior(Pprior, False, 0.9, 0.9), line_color=&quot;green&quot;), go.Scatter(name=&#39;Test positive&#39;, x=100*Pprior, y=100*Pposterior(Pprior, True, 0.9, 0.9), line_color=&quot;red&quot;), go.Scatter(name=&#39;No test&#39;, x=100*Pprior, y=100*Pprior, line_color=&quot;blue&quot;) ]) fig.update_layout( xaxis_title = &#39;Prior probability of being infected&#39;, yaxis_title = &#39;Posterior probability of being infected given test result&lt;br&gt;Specificity=90.0&lt;br&gt;Sensitivity=90.0&#39; ) fig.show() . . . . If you hover the mouse over the lines you can see the exact numbers. . As you can see, a positive or negative test does give more information than no test, but it doesn&#39;t quite give you certainty. . Story time - Part 2 . Let&#39;s circle back to our Dr. Antevy with his two positive tests and the two negative tests. . Prior to any tests, he was about 50% certain of having contracted COVID-19 based on his assesment of his symptoms, location, contact with other people, etc.. . Let&#39;s go through his test results to see what his posterior probability of having antibodies is. . #collapse_hide # Let&#39;s make a new function for multiple tests in a row def PposteriorM(Pprior, test_res): x = Pprior for tr, sn, sp in test_res: if tr == 1: x = (sn * x) / (sn * x + (1-sp) * (1-x)) elif tr == 0: x = (1-((sp * (1-x))/(1-(sn * x + (1-sp) * (1-x))))) return x . . Let&#39;s say these are the characteristics of the tests he used: . Test 1 and 2: Specificity = 0.90 | Sensitivity = 0.99 | . | Test 3 and 4: Specificity = 0.97 | Sensitivity = 0.95 | . | . So a highly sensitive first test followed by a rather good allround test, a bit more specific than the first. . #collapse_hide # Below is the prior probability of being infected: num=10000 Pprior = np.linspace((1/num),(num-1)/num,num=num) # Test characteristics test_results = [(1, 0.99, 0.90),(1, 0.99, 0.90),(0,0.95,0.97),(0,0.95,0.97)] # Graph the results fig = go.Figure(data=[ go.Scatter(name=&#39;1 - 1st positive test&#39;, x=100*Pprior, y=100*PposteriorM(Pprior, [test_results[0]])), go.Scatter(name=&#39;2 - 2nd positive test&#39;, x=100*Pprior, y=100*PposteriorM(Pprior, test_results[0:2])), go.Scatter(name=&#39;3 - 1st negative test&#39;, x=100*Pprior, y=100*PposteriorM(Pprior, test_results[0:3])), go.Scatter(name=&#39;4 - 2nd negative test&#39;, x=100*Pprior, y=100*PposteriorM(Pprior, test_results[0:4])) ]) fig.update_layout( xaxis_title = &#39;Prior probability of being infected&#39;, yaxis_title = &#39;Posterior probability of being infected given test results&#39; ) fig.show() . . . . So let&#39;s go through step-by-step: . Before any test, he was about 50% sure he contracted COVID-19 | After the 1st positive test, this goes up to 90.8% sure | After the 2nd positive test, up to 99.0% sure | But the 1st negative test drops it back to 83.5% | And the 2nd negative all the way down to 20.7% | . What if this was done on a random person in France for example, and all 4 tests were positive. . Then the prior would be the prevalence in France (0.2%) instead of 50%, and the step by step would be as follows: . Before any test, about 0.20% | After 1st positive: still only 1.9% chance of being seropositive | After 2nd positive test: only 16.4% chance of seropositive | After 3rd positive: 86% | And after 4th positive test 99.5% | . So it took about 4 positive tests for a random person in France to become confident enough to be seropositive. . Discussion . The results above strongly underline the need for clear testing protocols and clear understanding of the interpretation of test results. . Wtih a disease that can be so devastating as COVID-19, a few things should be kept in mind: . A high treshold should be used to hedge the risk a false positive | Multiple tests should be taken | Multiple tests with different characteristics (ideally at least one with high sensitivity, and one with high specificity) | .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/covid-19/testing/serology/2020/04/12/on-testing.html",
            "relUrl": "/covid-19/testing/serology/2020/04/12/on-testing.html",
            "date": " ‚Ä¢ Apr 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "COVID-19 Map",
            "content": ". Link to tracker . Follow this link. . About . There are already many great ways to visualize the spread of sars-cov-2 around the world. I‚Äôve made another one for family and friends that has the world on one page and France on another. . You can click on the countries for more details on the world map. . Likewise you can click on specific departments to see more detail on the French map. . The world map uses data aggregated by the John Hopkins University CSSE. . The french map uses hospitalization and intensive care data from Sante Publique France. . Future blog post about building the site . The site is built using Plotly Dash and deployed to Heroku. . I will likely make a blog post detailing how to build the app and deploy it. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/covid-19/2020/03/12/COVID-19-Tracker.html",
            "relUrl": "/covid-19/2020/03/12/COVID-19-Tracker.html",
            "date": " ‚Ä¢ Mar 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/jupyter/2020/01/07/test.html",
            "relUrl": "/jupyter/2020/01/07/test.html",
            "date": " ‚Ä¢ Jan 7, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Electrical engineer by background, but enthusiast of all things data, electronics, governance, development, global health, skiing, football, and many other things. .",
          "url": "https://jeffufpost.github.io/scattered-thoughts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://jeffufpost.github.io/scattered-thoughts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}