{
  
    
        "post0": {
            "title": "Epidemic modeling - Part 8",
            "content": ". #collapse_hide # This code wrangles the data from JHU !pip install plotly==4.6.0 !pip install dash==1.12.0 import pandas as pd import numpy as np import math from scipy import signal import plotly.graph_objects as go import plotly.express as px from scipy.stats import expon from scipy.stats import gamma from scipy.stats import weibull_min from numpy.random import default_rng rng = default_rng() import dash import dash_core_components as dcc import dash_html_components as html import datetime # Import confirmed cases conf_df = pd.read_csv(&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&#39;) #Import deaths data deaths_df = pd.read_csv(&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv&#39;) # Import recovery data rec_df = pd.read_csv(&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv&#39;) #iso_alpha = pd.read_csv(&#39;https://raw.githubusercontent.com/jeffufpost/sars-cov-2-world-tracker/master/data/iso_alpha.csv&#39;, index_col=0, header=0).T.iloc[0] iso_alpha = pd.read_csv(&#39;https://raw.githubusercontent.com/jeffufpost/sars-cov-2-world-tracker/master/data/iso_alpha.csv&#39;, index_col=0, header=0) # Wrangle the data #print(&quot;Wrangling data by country.......&quot;) # Consolidate countries (ie. frenc dom tom are included in France, etc..) conf_df = conf_df.groupby(&quot;Country/Region&quot;) conf_df = conf_df.sum().reset_index() conf_df = conf_df.set_index(&#39;Country/Region&#39;) deaths_df = deaths_df.groupby(&quot;Country/Region&quot;) deaths_df = deaths_df.sum().reset_index() deaths_df = deaths_df.set_index(&#39;Country/Region&#39;) rec_df = rec_df.groupby(&quot;Country/Region&quot;) rec_df = rec_df.sum().reset_index() rec_df = rec_df.set_index(&#39;Country/Region&#39;) # Remove Lat and Long columns conf_df = conf_df.iloc[:,2:] deaths_df = deaths_df.iloc[:,2:] rec_df = rec_df.iloc[:,2:] # Convert country names to correct format for search with pycountry conf_df = conf_df.rename(index={&#39;Congo (Brazzaville)&#39;: &#39;Congo&#39;, &#39;Congo (Kinshasa)&#39;: &#39;Congo, the Democratic Republic of the&#39;, &#39;Burma&#39;: &#39;Myanmar&#39;, &#39;Korea, South&#39;: &#39;Korea, Republic of&#39;, &#39;Laos&#39;: &quot;Lao People&#39;s Democratic Republic&quot;, &#39;Taiwan*&#39;: &#39;Taiwan&#39;, &quot;West Bank and Gaza&quot;:&quot;Palestine, State of&quot;}) # Convert country names to correct format for search with pycountry deaths_df = deaths_df.rename(index={&#39;Congo (Brazzaville)&#39;: &#39;Congo&#39;, &#39;Congo (Kinshasa)&#39;: &#39;Congo, the Democratic Republic of the&#39;, &#39;Burma&#39;: &#39;Myanmar&#39;, &#39;Korea, South&#39;: &#39;Korea, Republic of&#39;, &#39;Laos&#39;: &quot;Lao People&#39;s Democratic Republic&quot;, &#39;Taiwan*&#39;: &#39;Taiwan&#39;, &quot;West Bank and Gaza&quot;:&quot;Palestine, State of&quot;}) # Convert country names to correct format for search with pycountry rec_df = rec_df.rename(index={&#39;Congo (Brazzaville)&#39;: &#39;Congo&#39;, &#39;Congo (Kinshasa)&#39;: &#39;Congo, the Democratic Republic of the&#39;, &#39;Burma&#39;: &#39;Myanmar&#39;, &#39;Korea, South&#39;: &#39;Korea, Republic of&#39;, &#39;Laos&#39;: &quot;Lao People&#39;s Democratic Republic&quot;, &#39;Taiwan*&#39;: &#39;Taiwan&#39;, &quot;West Bank and Gaza&quot;:&quot;Palestine, State of&quot;}) # Convert dates to datime format conf_df.columns = pd.to_datetime(conf_df.columns).date deaths_df.columns = pd.to_datetime(deaths_df.columns).date rec_df.columns = pd.to_datetime(rec_df.columns).date # Create a per day dataframe #print(&quot;Creating new per day dataframes......&quot;) # Create per day dataframes for cases, deaths, and recoveries - by pd.DatafRame.diff conf_df_pd = conf_df.diff(axis=1) deaths_df_pd = deaths_df.diff(axis=1) rec_df_pd = rec_df.diff(axis=1) #print(&quot;Create infected dataframe = conf - deaths - recoveries&quot;) inf_df = conf_df - deaths_df - rec_df conf_df_pd.iloc[:,0] = 0 rec_df_pd.iloc[:,0] = 0 deaths_df_pd.iloc[:,0] = 0 inf_df.iloc[:,0] = 0 #print(&quot;Adding dataframes of 1st, 2nd, and 3rd derivatives of number of infected&quot;) firstdev = inf_df.apply(np.gradient, axis=1) seconddev = firstdev.apply(np.gradient) thirddev = seconddev.apply(np.gradient) #print(&quot;Create series of first date above 100 confirmed cases.....&quot;) # Create a column containing date at which 100 confirmed cases were reached, NaN if not reached yet fda100 = conf_df[conf_df &gt; 100].apply(pd.Series.first_valid_index, axis=1) # Create dataframe for probability plot probevent = iso_alpha.join(inf_df) probevent[&#39;prev&#39;] = probevent.iloc[:,-1] / probevent[&#39;SP.POP.TOTL&#39;] . . Requirement already satisfied: plotly==4.6.0 in /usr/local/lib/python3.6/dist-packages (4.6.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.12.0) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.3.3) Requirement already satisfied: dash==1.12.0 in /usr/local/lib/python3.6/dist-packages (1.12.0) Requirement already satisfied: Flask&gt;=1.0.2 in /usr/local/lib/python3.6/dist-packages (from dash==1.12.0) (1.1.2) Requirement already satisfied: flask-compress in /usr/local/lib/python3.6/dist-packages (from dash==1.12.0) (1.5.0) Requirement already satisfied: dash-core-components==1.10.0 in /usr/local/lib/python3.6/dist-packages (from dash==1.12.0) (1.10.0) Requirement already satisfied: dash-table==4.7.0 in /usr/local/lib/python3.6/dist-packages (from dash==1.12.0) (4.7.0) Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from dash==1.12.0) (0.16.0) Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from dash==1.12.0) (4.6.0) Requirement already satisfied: dash-renderer==1.4.1 in /usr/local/lib/python3.6/dist-packages (from dash==1.12.0) (1.4.1) Requirement already satisfied: dash-html-components==1.0.3 in /usr/local/lib/python3.6/dist-packages (from dash==1.12.0) (1.0.3) Requirement already satisfied: itsdangerous&gt;=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask&gt;=1.0.2-&gt;dash==1.12.0) (1.1.0) Requirement already satisfied: click&gt;=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask&gt;=1.0.2-&gt;dash==1.12.0) (7.1.2) Requirement already satisfied: Jinja2&gt;=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask&gt;=1.0.2-&gt;dash==1.12.0) (2.11.2) Requirement already satisfied: Werkzeug&gt;=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask&gt;=1.0.2-&gt;dash==1.12.0) (1.0.1) Requirement already satisfied: brotli in /usr/local/lib/python3.6/dist-packages (from flask-compress-&gt;dash==1.12.0) (1.0.7) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly-&gt;dash==1.12.0) (1.3.3) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly-&gt;dash==1.12.0) (1.12.0) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2&gt;=2.10.1-&gt;Flask&gt;=1.0.2-&gt;dash==1.12.0) (1.1.1) . #collapse_hide # This code is the lowpass filter def lowpass(x, fc=0.05): fs = 1 # Sampling frequency t = np.arange(len(x.T)) #select number of days done in SEIR model signala = x.T #fc = 0.05 # Cut-off frequency of the filter w = fc / (fs / 2) # Normalize the frequency b, a = signal.butter(5, w, &#39;low&#39;) return signal.filtfilt(b, a, signala), w . . #collapse_hide # This code creates the impulse responses days = np.arange(100) cdf = pd.DataFrame({ &#39;T_Latent&#39;: gamma.cdf(days, 1.8,loc=0.9,scale=(5.2-1.8)/0.9), &#39;T_Infectious&#39;: weibull_min.cdf(days, 2.3,loc=2,scale=20.11) }) h_L = cdf.diff().T_Latent h_I = cdf.diff().T_Infectious h_L[0] = 0 h_I[0] = 0 . . #collapse_hide # This code is for the iterative deconvolution # Let&#39;s define an iteration function: def iter_deconv(alpha, impulse_response, input_signal, delay, comparator): conv=signal.fftconvolve(impulse_response, input_signal, mode=&#39;full&#39;) correction=np.roll(comparator-conv[:len(comparator)], delay) input_signal=np.floor(lowpass(alpha*correction+input_signal)[0]) input_signal[input_signal&lt;0]=0 return input_signal # Define a function to return MSE between two signals as a measure of goodness of fit def msecalc(A, B): return ((A - B)**2).mean(axis=0) . . Motivation for write-up . This is the 8th part of a multi-part series blog post on modeling in epidemiology. . The COVID-19 pandemic has brought a lot of attention to the study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. . While we have studied in detail the stochastc SEIR model, we have not compared this to actual real-world data. . The goal of this 8th installment is to see how we can use the techniques from the previous blog posts to estimate the various parameters from the real data we have. . Data available . We will use the JHU data which is updated daily and displayed graphically on my tracker here. . A lot of countries have had difficulty reporting reliable data, but a few have done so rather well. . We will have a closer look at these contries: . Austria | Germany | Iceland | Russia | South Korea | Switzerland | . The JHU datasets include the following data: . Daily cumulative confirmed cases | Daily cumulative recoveries | Daily cumulative deaths | . From which we can calculate the following: . Daily new confirmed cases | Daily new recoveries | Daily new deaths | Current number of cases | . Analysis . We have seen previously how we can get $E_{pd}$ from deconvolution of $I_{pd}$. . We also know that: $$E_{pd} = beta ~ I ~ frac{S}{N}$$ . Hence: $$ beta = frac{N ~ E_{pd}}{I ~ S}$$ . And if we assume that in the real-world S ~ N (which is roughly the case where $S=0.9~N$), then: $$ beta = frac{E_{pd}}{I}$$ . Furthermore we know: $$R_0 = frac{ beta}{ gamma}$$ and $$R=R_0 frac{S}{N}$$ So: $$E_{pd}= gamma~R~I$$ $$ leftrightarrow R = frac{E_{pd}}{ gamma~I}$$ . Our aim is to find this graphically in the real-world data. . Real-world analysis . Austria . Let&#39;s first have a look at Austria as a template for the analysis we will do for the other countries. . We have daily data $I_{pd}$ and $R_{pd}$ (where $R_{pd}$ is the sum of deaths and recoveries) | We have our assumed $T_L$ and $T_I$ | . Analysis steps: . The first thing we want to check is whether $h_I circledast I_{pd} [j]$ gives us something close to $R_{pd}$ If not, why not? | Can we get an estimated $E_{pd}$ by deconvolution of $I_{pd}$ ? | What can that tell us about $R$ and $ beta$ ? | 1. Checking $h_I$ . #collapse_hide fig = go.Figure(data=[ go.Bar(name=&#39;Ipd&#39;, x=conf_df_pd.loc[&#39;Austria&#39;].index, y=conf_df_pd.loc[&#39;Austria&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=conf_df_pd.loc[&#39;Austria&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Austria&#39;])[0]), go.Bar(name=&#39;Rpd&#39;, x=rec_df_pd.loc[&#39;Austria&#39;].index, y=rec_df_pd.loc[&#39;Austria&#39;]), go.Scatter(name=&#39;Rpd=lowpass(Rpd)&#39;, x=rec_df_pd.loc[&#39;Austria&#39;].index, y=lowpass(rec_df_pd.loc[&#39;Austria&#39;])[0]), go.Scatter(name=&#39;Rpd=conv(Ipd)&#39;, x=conf_df_pd.loc[&#39;Austria&#39;].index, y=signal.fftconvolve(h_I, conf_df_pd.loc[&#39;Austria&#39;], mode=&#39;full&#39;)) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Austria: Actual } R_{pd} text{ vs. } h_I[j] circledast I_{pd}[j]$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see the actual $R_{pd}$ leads $h_I[j] circledast I_{pd}[j]$ by about 5 days. . There is a 20 day lag between peak $I_{pd}$ and $h_I[j] circledast I_{pd}[j]$ as is expected since $E[T_I] = 20.11$. . But there is only a 15 day lag between peak $I_{pd}$ and actual $R_{pd}$. . There are a few possibilites for why this is the case, including that maybe we haven&#39;t assumed the correct distribution for $T_I$. . However there is another reason why this could be. Testing, especially in the early days, took time, and it took time for a patient showing symptoms before he could be tested. This may simply explain the 5 day difference. . 2. Estimating $E_{pd}$ by deconvolution of $I_{pd}$ . #collapse_hide #Settting up for deconvolution of Ipd #regularization parameter alpha=2 # Setup up the resultant Ipd we want to compare our guess with Ipd=np.floor(lowpass(conf_df_pd.loc[&#39;Austria&#39;])[0]) Ipd[Ipd&lt;0]=0 # Pad with last value i=0 while i &lt; 100: Ipd=np.append(Ipd, Ipd[-1]) i=i+1 # Find delay caused by h_L delay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode=&#39;full&#39;).argmax() # We want initial guess to simply be the result of the convolution delayed initial_guess=np.roll(Ipd,delay) Enext = initial_guess # AN array to record MSE between result we want and our iterated guess mse=np.array([]) mse=np.append(mse, 10000000) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Austria&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Austria&#39;])])) itercount=0 while mse[-1] &lt; mse[-2]: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Austria&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Austria&#39;])])) print(&quot;Iteration #&quot; + str(itercount) +&quot;: MSE= &quot;+str(mse[itercount])) print(&quot;Iteration #&quot; + str(itercount+1) +&quot;: MSE= &quot;+str(mse[-1])+&quot; so we use the result of the previous iteration.&quot;) . . Iteration #1: MSE= 994.1480206933169 Iteration #2: MSE= 223.4606324754608 Iteration #3: MSE= 155.95604332037303 Iteration #4: MSE= 138.69774483406667 Iteration #5: MSE= 94.47398861194564 Iteration #6: MSE= 94.27210338177136 Iteration #7: MSE= 72.93041537376978 Iteration #8: MSE= 76.01675231549818 so we use the result of the previous iteration. . #collapse_hide # We can keep going the iteration until lowest MSE #change alpha if you like #alpha=2 i=0 while i &lt; 10: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) print(msecalc(Ipd[:len(conf_df_pd.loc[&#39;Austria&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Austria&#39;])])) i=i+1 . . 55.6161301776942 55.411698307523224 55.6161301776942 55.411698307523224 55.6161301776942 55.411698307523224 55.6161301776942 55.411698307523224 55.6161301776942 55.411698307523224 . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Austria&#39;].index, y=Enext), go.Scatter(name=&#39;Ipd=conv(deconv(Ipd))&#39;, x=inf_df.loc[&#39;Austria&#39;].index, y=signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)), go.Bar(name=&#39;Ipd&#39;, x=inf_df.loc[&#39;Austria&#39;].index, y=conf_df_pd.loc[&#39;Austria&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=inf_df.loc[&#39;Austria&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Austria&#39;])[0]) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Austria: Actual } I_{pd} text{ vs. convolution of deconvolution of } I_{pd}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see that our estimate for $ hat{E}_{pd}$ must be close to the reality of $E_{pd}$ as $I_{pd}$ is almost identical to $ hat{E}_{pd} circledast h_L$. . Of course, this holds only as long as our estimate of $h_L$ is close to reality. . 3. $ beta$ and $R$ from $E_{pd}$ and $I$ . As described above: $$ beta = frac{N~E_{pd}}{S~I}$$ and if $S$~$N$, then $$ beta = frac{E_{pd}}{I}$$ It is easier to simply say : $$R = frac{E_{pd}}{ gamma~I}$$ . #collapse_hide # Calculate R gam = 1/15 # As we say gamma is 1/20.11 R = Enext[:len(inf_df.loc[&#39;Austria&#39;])]*(1/gam)/inf_df.loc[&#39;Austria&#39;] fig = go.Figure(data=[ go.Scatter(name=&#39;R&#39;, x=inf_df.loc[&#39;Austria&#39;].index, y=R), go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Austria&#39;].index, y=Enext), go.Scatter(name=&#39;Inf&#39;, x=inf_df.loc[&#39;Austria&#39;].index, y=inf_df.loc[&#39;Austria&#39;]), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;R&#39;, title={ &#39;text&#39;:r&#39;$ text{Austria: R }$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . France . We have daily data $I_{pd}$ and $R_{pd}$ (where $R_{pd}$ is the sum of deaths and recoveries) | We have our assumed $T_L$ and $T_I$ | . Analysis steps: . The first thing we want to check is whether $h_I circledast I_{pd} [j]$ gives us something close to $R_{pd}$ If not, why not? | Can we get an estimated $E_{pd}$ by deconvolution of $I_{pd}$ ? | What can that tell us about $R$ and $ beta$ ? | 1. Checking $h_I$ . #collapse_hide fig = go.Figure(data=[ go.Bar(name=&#39;Ipd&#39;, x=conf_df_pd.loc[&#39;France&#39;].index, y=conf_df_pd.loc[&#39;France&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=conf_df_pd.loc[&#39;France&#39;].index, y=lowpass(conf_df_pd.loc[&#39;France&#39;]+deaths_df_pd.loc[&#39;France&#39;])[0]), go.Bar(name=&#39;Rpd&#39;, x=rec_df_pd.loc[&#39;France&#39;].index, y=rec_df_pd.loc[&#39;France&#39;]), go.Scatter(name=&#39;Rpd=lowpass(Rpd)&#39;, x=rec_df_pd.loc[&#39;France&#39;].index, y=lowpass(rec_df_pd.loc[&#39;France&#39;])[0]), go.Scatter(name=&#39;Rpd=conv(Ipd)&#39;, x=conf_df_pd.loc[&#39;France&#39;].index, y=signal.fftconvolve(h_I, conf_df_pd.loc[&#39;France&#39;]+deaths_df_pd.loc[&#39;France&#39;], mode=&#39;full&#39;)) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{France: Actual } R_{pd} text{ vs. } h_I[j] circledast I_{pd}[j]$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . Data is not very good. . 2. Estimating $E_{pd}$ by deconvolution of $I_{pd}$ . #collapse_hide #Settting up for deconvolution of Ipd #regularization parameter alpha=2 # Setup up the resultant Ipd we want to compare our guess with Ipd=np.floor(lowpass(conf_df_pd.loc[&#39;France&#39;])[0]) Ipd[Ipd&lt;0]=0 # Pad with last value i=0 while i &lt; 100: Ipd=np.append(Ipd, Ipd[-1]) i=i+1 # Find delay caused by h_L delay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode=&#39;full&#39;).argmax() # We want initial guess to simply be the result of the convolution delayed initial_guess=np.roll(Ipd,delay) Enext = initial_guess # AN array to record MSE between result we want and our iterated guess mse=np.array([]) mse=np.append(mse, 10000000) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;France&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;France&#39;])])) itercount=0 while mse[-1] &lt; mse[-2]: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;France&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;France&#39;])])) print(&quot;Iteration #&quot; + str(itercount) +&quot;: MSE= &quot;+str(mse[itercount])) print(&quot;Iteration #&quot; + str(itercount+1) +&quot;: MSE= &quot;+str(mse[-1])+&quot; so we use the result of the previous iteration.&quot;) . . Iteration #1: MSE= 154260.80035828718 Iteration #2: MSE= 54034.481374599236 Iteration #3: MSE= 53691.31465777912 Iteration #4: MSE= 45144.53386180947 Iteration #5: MSE= 39709.43731711392 Iteration #6: MSE= 37860.12268122137 Iteration #7: MSE= 31523.621058356155 Iteration #8: MSE= 32481.060508017406 so we use the result of the previous iteration. . #collapse_hide # We can keep going the iteration until lowest MSE #change alpha if you like alpha=2 i=0 while i &lt; 10: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) print(msecalc(Ipd[:len(conf_df_pd.loc[&#39;France&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;France&#39;])])) i=i+1 . . 25956.38089132117 25965.124359422345 25956.38089132117 25965.124359422345 25956.38089132117 25965.124359422345 25956.38089132117 25965.124359422345 25956.38089132117 25965.124359422345 . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;France&#39;].index, y=Enext), go.Scatter(name=&#39;Ipd=conv(deconv(Ipd))&#39;, x=inf_df.loc[&#39;France&#39;].index, y=signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)), go.Bar(name=&#39;Ipd&#39;, x=inf_df.loc[&#39;France&#39;].index, y=conf_df_pd.loc[&#39;France&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=inf_df.loc[&#39;France&#39;].index, y=lowpass(conf_df_pd.loc[&#39;France&#39;])[0]) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{France: Actual } I_{pd} text{ vs. convolution of deconvolution of } I_{pd}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . 3. $ beta$ and $R$ from $E_{pd}$ and $I$ . As described above: $$ beta = frac{N~E_{pd}}{S~I}$$ and if $S$~$N$, then $$ beta = frac{E_{pd}}{I}$$ It is easier to simply say : $$R = frac{E_{pd}}{ gamma~I}$$ . #collapse_hide # Calculate R gam = 1/20.11 # As we say gamma is 1/20.11 R = Enext[:len(inf_df.loc[&#39;France&#39;])]*(1/gam)/inf_df.loc[&#39;France&#39;] fig = go.Figure(data=[ go.Scatter(name=&#39;R&#39;, x=inf_df.loc[&#39;France&#39;].index, y=R), go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;France&#39;].index, y=Enext), go.Scatter(name=&#39;Inf&#39;, x=inf_df.loc[&#39;France&#39;].index, y=inf_df.loc[&#39;France&#39;]), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;R&#39;, title={ &#39;text&#39;:r&#39;$ text{France: R }$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . After March 15th we see a rapi decline in $R$ until $R&lt;1$ since April 12th, however the last 2 weeks of May have seen an increase and it is now close to 1 in early June. . Germany . We have daily data $I_{pd}$ and $R_{pd}$ (where $R_{pd}$ is the sum of deaths and recoveries) | We have our assumed $T_L$ and $T_I$ | . Analysis steps: . The first thing we want to check is whether $h_I circledast I_{pd} [j]$ gives us something close to $R_{pd}$ If not, why not? | Can we get an estimated $E_{pd}$ by deconvolution of $I_{pd}$ ? | What can that tell us about $R$ and $ beta$ ? | 1. Checking $h_I$ . #collapse_hide fig = go.Figure(data=[ go.Bar(name=&#39;Ipd&#39;, x=conf_df_pd.loc[&#39;Germany&#39;].index, y=conf_df_pd.loc[&#39;Germany&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=conf_df_pd.loc[&#39;Germany&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Germany&#39;]+deaths_df_pd.loc[&#39;Germany&#39;])[0]), go.Bar(name=&#39;Rpd&#39;, x=rec_df_pd.loc[&#39;Germany&#39;].index, y=rec_df_pd.loc[&#39;Germany&#39;]), go.Scatter(name=&#39;Rpd=lowpass(Rpd)&#39;, x=rec_df_pd.loc[&#39;Germany&#39;].index, y=lowpass(rec_df_pd.loc[&#39;Germany&#39;])[0]), go.Scatter(name=&#39;Rpd=conv(Ipd)&#39;, x=conf_df_pd.loc[&#39;Germany&#39;].index, y=signal.fftconvolve(h_I, conf_df_pd.loc[&#39;Germany&#39;]+deaths_df_pd.loc[&#39;Germany&#39;], mode=&#39;full&#39;)) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Germany: Actual } R_{pd} text{ vs. } h_I[j] circledast I_{pd}[j]$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see the actual $R_{pd}$ lags $h_I[j] circledast I_{pd}[j]$ by about 10 days. . Although they are pretty close still. . 2. Estimating $E_{pd}$ by deconvolution of $I_{pd}$ . #collapse_hide #Settting up for deconvolution of Ipd #regularization parameter alpha=2 # Setup up the resultant Ipd we want to compare our guess with Ipd=np.floor(lowpass(conf_df_pd.loc[&#39;Germany&#39;])[0]) Ipd[Ipd&lt;0]=0 # Pad with last value i=0 while i &lt; 100: Ipd=np.append(Ipd, Ipd[-1]) i=i+1 # Find delay caused by h_L delay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode=&#39;full&#39;).argmax() # We want initial guess to simply be the result of the convolution delayed initial_guess=np.roll(Ipd,delay) Enext = initial_guess # AN array to record MSE between result we want and our iterated guess mse=np.array([]) mse=np.append(mse, 10000000) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Germany&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Germany&#39;])])) itercount=0 while mse[-1] &lt; mse[-2]: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Germany&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Germany&#39;])])) print(&quot;Iteration #&quot; + str(itercount) +&quot;: MSE= &quot;+str(mse[itercount])) print(&quot;Iteration #&quot; + str(itercount+1) +&quot;: MSE= &quot;+str(mse[-1])+&quot; so we use the result of the previous iteration.&quot;) . . Iteration #1: MSE= 32342.166054726593 Iteration #2: MSE= 9520.818482545094 Iteration #3: MSE= 5480.074863698488 Iteration #4: MSE= 3736.4520218065554 Iteration #5: MSE= 2918.142469998744 Iteration #6: MSE= 2360.7817893148813 Iteration #7: MSE= 1989.5578460711724 Iteration #8: MSE= 1725.1502755073097 Iteration #9: MSE= 1500.7619212142413 Iteration #10: MSE= 1349.7618585161513 Iteration #11: MSE= 1197.9409125553852 Iteration #12: MSE= 1099.740851284486 Iteration #13: MSE= 994.817986262571 Iteration #14: MSE= 924.3422942105037 Iteration #15: MSE= 844.8573516897526 Iteration #16: MSE= 794.3268717988868 Iteration #17: MSE= 739.8998411142202 Iteration #18: MSE= 701.9484322568269 Iteration #19: MSE= 662.5577082814119 Iteration #20: MSE= 631.0944523504336 Iteration #21: MSE= 602.5541705643316 Iteration #22: MSE= 578.5943011308373 Iteration #23: MSE= 557.771540913623 Iteration #24: MSE= 538.213918985831 Iteration #25: MSE= 523.5865826611572 Iteration #26: MSE= 504.6500345303785 Iteration #27: MSE= 495.0313679838475 Iteration #28: MSE= 481.0470432814702 Iteration #29: MSE= 473.9125369451784 Iteration #30: MSE= 460.3596321809197 Iteration #31: MSE= 456.15021339132414 Iteration #32: MSE= 445.2086268455976 Iteration #33: MSE= 441.99650238006024 Iteration #34: MSE= 430.37214275681214 Iteration #35: MSE= 429.54734158025946 Iteration #36: MSE= 421.7644150213061 Iteration #37: MSE= 420.9015137766196 Iteration #38: MSE= 412.14371502030235 Iteration #39: MSE= 412.2907082954131 so we use the result of the previous iteration. . #collapse_hide # We can keep going the iteration until lowest MSE #change alpha if you like alpha=2 i=0 while i &lt; 10: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) print(msecalc(Ipd[:len(conf_df_pd.loc[&#39;Germany&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Germany&#39;])])) i=i+1 . . 346.90109659914947 348.7463987321059 346.90109659914947 348.7463987321059 346.90109659914947 348.7463987321059 346.90109659914947 348.7463987321059 346.90109659914947 348.7463987321059 . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Germany&#39;].index, y=Enext), go.Scatter(name=&#39;Ipd=conv(deconv(Ipd))&#39;, x=inf_df.loc[&#39;Germany&#39;].index, y=signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)), go.Bar(name=&#39;Ipd&#39;, x=inf_df.loc[&#39;Germany&#39;].index, y=conf_df_pd.loc[&#39;Germany&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=inf_df.loc[&#39;Germany&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Germany&#39;])[0]) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Germany: Actual } I_{pd} text{ vs. convolution of deconvolution of } I_{pd}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see that our estimate for $ hat{E}_{pd}$ must be close to the reality of $E_{pd}$ as $I_{pd}$ is almost identical to $ hat{E}_{pd} circledast h_L$. . Of course, this holds only as long as our estimate of $h_L$ is close to reality. . 3. $ beta$ and $R$ from $E_{pd}$ and $I$ . As described above: $$ beta = frac{N~E_{pd}}{S~I}$$ and if $S$~$N$, then $$ beta = frac{E_{pd}}{I}$$ It is easier to simply say : $$R = frac{E_{pd}}{ gamma~I}$$ . #collapse_hide # Calculate R gam = 1/20.11 # As we say gamma is 1/20.11 R = Enext[:len(inf_df.loc[&#39;Germany&#39;])]*(1/gam)/inf_df.loc[&#39;Germany&#39;] fig = go.Figure(data=[ go.Scatter(name=&#39;R&#39;, x=inf_df.loc[&#39;Germany&#39;].index, y=R), go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Germany&#39;].index, y=Enext), go.Scatter(name=&#39;Inf&#39;, x=inf_df.loc[&#39;Germany&#39;].index, y=inf_df.loc[&#39;Germany&#39;]), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;R&#39;, title={ &#39;text&#39;:r&#39;$ text{Germany: R }$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see $R&lt;1$ since April 3rd, however the peak $E_{pd}$ is slightly ahead of that March 23rd. . Iceland . We have daily data $I_{pd}$ and $R_{pd}$ (where $R_{pd}$ is the sum of deaths and recoveries) | We have our assumed $T_L$ and $T_I$ | . Analysis steps: . The first thing we want to check is whether $h_I circledast I_{pd} [j]$ gives us something close to $R_{pd}$ If not, why not? | Can we get an estimated $E_{pd}$ by deconvolution of $I_{pd}$ ? | What can that tell us about $R$ and $ beta$ ? | 1. Checking $h_I$ . #collapse_hide fig = go.Figure(data=[ go.Bar(name=&#39;Ipd&#39;, x=conf_df_pd.loc[&#39;Iceland&#39;].index, y=conf_df_pd.loc[&#39;Iceland&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=conf_df_pd.loc[&#39;Iceland&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Iceland&#39;]+deaths_df_pd.loc[&#39;Iceland&#39;])[0]), go.Bar(name=&#39;Rpd&#39;, x=rec_df_pd.loc[&#39;Iceland&#39;].index, y=rec_df_pd.loc[&#39;Iceland&#39;]), go.Scatter(name=&#39;Rpd=lowpass(Rpd)&#39;, x=rec_df_pd.loc[&#39;Iceland&#39;].index, y=lowpass(rec_df_pd.loc[&#39;Iceland&#39;])[0]), go.Scatter(name=&#39;Rpd=conv(Ipd)&#39;, x=conf_df_pd.loc[&#39;Iceland&#39;].index, y=signal.fftconvolve(h_I, conf_df_pd.loc[&#39;Iceland&#39;]+deaths_df_pd.loc[&#39;Iceland&#39;], mode=&#39;full&#39;)) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Iceland: Actual } R_{pd} text{ vs. } h_I[j] circledast I_{pd}[j]$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see the actual $R_{pd}$ leads $h_I[j] circledast I_{pd}[j]$ by about 5 days. . 2. Estimating $E_{pd}$ by deconvolution of $I_{pd}$ . #collapse_hide #Settting up for deconvolution of Ipd #regularization parameter alpha=2 # Setup up the resultant Ipd we want to compare our guess with Ipd=np.floor(lowpass(conf_df_pd.loc[&#39;Iceland&#39;])[0]) Ipd[Ipd&lt;0]=0 # Pad with last value i=0 while i &lt; 100: Ipd=np.append(Ipd, Ipd[-1]) i=i+1 # Find delay caused by h_L delay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode=&#39;full&#39;).argmax() # We want initial guess to simply be the result of the convolution delayed initial_guess=np.roll(Ipd,delay) Enext = initial_guess # AN array to record MSE between result we want and our iterated guess mse=np.array([]) mse=np.append(mse, 10000000) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Iceland&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Iceland&#39;])])) itercount=0 while mse[-1] &lt; mse[-2]: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Iceland&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Iceland&#39;])])) print(&quot;Iteration #&quot; + str(itercount) +&quot;: MSE= &quot;+str(mse[itercount])) print(&quot;Iteration #&quot; + str(itercount+1) +&quot;: MSE= &quot;+str(mse[-1])+&quot; so we use the result of the previous iteration.&quot;) . . Iteration #1: MSE= 7.549921432879913 Iteration #2: MSE= 2.1777016171612535 Iteration #3: MSE= 1.4032075531939128 Iteration #4: MSE= 0.7097902831182038 Iteration #5: MSE= 0.6538394434040017 Iteration #6: MSE= 0.35241350030804025 Iteration #7: MSE= 0.4814099480915245 so we use the result of the previous iteration. . #collapse_hide # We can keep going the iteration until lowest MSE #change alpha if you like alpha=2 i=0 while i &lt; 10: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) print(msecalc(Ipd[:len(conf_df_pd.loc[&#39;Iceland&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Iceland&#39;])])) i=i+1 . . 0.22104707759253456 0.329023021163164 0.17949206914108762 0.24168412384812277 0.16826563216815357 0.2356731199869026 0.16926773827689567 0.2251716954513438 0.17950404246036664 0.22699309635497517 . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Iceland&#39;].index, y=Enext), go.Scatter(name=&#39;Ipd=conv(deconv(Ipd))&#39;, x=inf_df.loc[&#39;Iceland&#39;].index, y=signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)), go.Bar(name=&#39;Ipd&#39;, x=inf_df.loc[&#39;Russia&#39;].index, y=conf_df_pd.loc[&#39;Iceland&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=inf_df.loc[&#39;Iceland&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Iceland&#39;])[0]) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Iceland: Actual } I_{pd} text{ vs. convolution of deconvolution of } I_{pd}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see that our estimate for $ hat{E}_{pd}$ must be close to the reality of $E_{pd}$ as $I_{pd}$ is almost identical to $ hat{E}_{pd} circledast h_L$. . Of course, this holds only as long as our estimate of $h_L$ is close to reality. . 3. $ beta$ and $R$ from $E_{pd}$ and $I$ . As described above: $$ beta = frac{N~E_{pd}}{S~I}$$ and if $S$~$N$, then $$ beta = frac{E_{pd}}{I}$$ It is easier to simply say : $$R = frac{E_{pd}}{ gamma~I}$$ . #collapse_hide # Calculate R gam = 1/20.11 # As we say gamma is 1/20.11 R = Enext[:len(inf_df.loc[&#39;Iceland&#39;])]*(1/gam)/inf_df.loc[&#39;Iceland&#39;] fig = go.Figure(data=[ go.Scatter(name=&#39;R&#39;, x=inf_df.loc[&#39;Iceland&#39;].index, y=R), go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Iceland&#39;].index, y=Enext), go.Scatter(name=&#39;Inf&#39;, x=inf_df.loc[&#39;Iceland&#39;].index, y=inf_df.loc[&#39;Iceland&#39;]), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;R&#39;, title={ &#39;text&#39;:r&#39;$ text{Iceland: R }$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see $R&lt;1$ since March 30th and dropped significantly within the last 2 weeks of March. . Russia . We have daily data $I_{pd}$ and $R_{pd}$ (where $R_{pd}$ is the sum of deaths and recoveries) | We have our assumed $T_L$ and $T_I$ | . Analysis steps: . The first thing we want to check is whether $h_I circledast I_{pd} [j]$ gives us something close to $R_{pd}$ If not, why not? | Can we get an estimated $E_{pd}$ by deconvolution of $I_{pd}$ ? | What can that tell us about $R$ and $ beta$ ? | 1. Checking $h_I$ . #collapse_hide fig = go.Figure(data=[ go.Bar(name=&#39;Ipd&#39;, x=conf_df_pd.loc[&#39;Russia&#39;].index, y=conf_df_pd.loc[&#39;Russia&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=conf_df_pd.loc[&#39;Russia&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Russia&#39;]+deaths_df_pd.loc[&#39;Russia&#39;])[0]), go.Bar(name=&#39;Rpd&#39;, x=rec_df_pd.loc[&#39;Russia&#39;].index, y=rec_df_pd.loc[&#39;Russia&#39;]), go.Scatter(name=&#39;Rpd=lowpass(Rpd)&#39;, x=rec_df_pd.loc[&#39;Russia&#39;].index, y=lowpass(rec_df_pd.loc[&#39;Russia&#39;])[0]), go.Scatter(name=&#39;Rpd=conv(Ipd)&#39;, x=conf_df_pd.loc[&#39;Russia&#39;].index, y=signal.fftconvolve(h_I, conf_df_pd.loc[&#39;Russia&#39;]+deaths_df_pd.loc[&#39;Russia&#39;], mode=&#39;full&#39;)) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Russia: Actual } R_{pd} text{ vs. } h_I[j] circledast I_{pd}[j]$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see the actual $R_{pd}$ lags $h_I[j] circledast I_{pd}[j]$ by about 4 days. . Again there are a few possibilites for why this is the case, including that maybe we haven&#39;t assumed the correct distribution for $T_I$. . However, it can easily be assumed that deaths are vastly underestimated and are the reason for this. . 2. Estimating $E_{pd}$ by deconvolution of $I_{pd}$ . #collapse_hide #Settting up for deconvolution of Ipd #regularization parameter alpha=2 # Setup up the resultant Ipd we want to compare our guess with Ipd=np.floor(lowpass(conf_df_pd.loc[&#39;Russia&#39;])[0]) Ipd[Ipd&lt;0]=0 # Pad with last value i=0 while i &lt; 100: Ipd=np.append(Ipd, Ipd[-1]) i=i+1 # Find delay caused by h_L delay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode=&#39;full&#39;).argmax() # We want initial guess to simply be the result of the convolution delayed initial_guess=np.roll(Ipd,delay) Enext = initial_guess # AN array to record MSE between result we want and our iterated guess mse=np.array([]) mse=np.append(mse, 10000000) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Russia&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Russia&#39;])])) itercount=0 while mse[-1] &lt; mse[-2]: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Russia&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Russia&#39;])])) print(&quot;Iteration #&quot; + str(itercount) +&quot;: MSE= &quot;+str(mse[itercount])) print(&quot;Iteration #&quot; + str(itercount+1) +&quot;: MSE= &quot;+str(mse[-1])+&quot; so we use the result of the previous iteration.&quot;) . . Iteration #1: MSE= 39099.91006982222 Iteration #2: MSE= 16106.103319769038 Iteration #3: MSE= 11017.503739943835 Iteration #4: MSE= 9211.564403068674 Iteration #5: MSE= 8031.942249915613 Iteration #6: MSE= 7057.11955393414 Iteration #7: MSE= 6659.220896111151 Iteration #8: MSE= 6060.561604177258 Iteration #9: MSE= 5976.364288743826 Iteration #10: MSE= 5525.740411388326 Iteration #11: MSE= 5588.427919549833 so we use the result of the previous iteration. . #collapse_hide # We can keep going the iteration until lowest MSE #change alpha if you like alpha=2 i=0 while i &lt; 10: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) print(msecalc(Ipd[:len(conf_df_pd.loc[&#39;Russia&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Russia&#39;])])) i=i+1 . . 4386.5897924660985 4471.91717180048 4384.034207273343 4470.893463246848 4382.898268213621 4471.548314093039 4383.598844116114 4468.01306793667 4381.51864607549 4464.993011420806 . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Russia&#39;].index, y=Enext), go.Scatter(name=&#39;Ipd=conv(deconv(Ipd))&#39;, x=inf_df.loc[&#39;Russia&#39;].index, y=signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)), go.Bar(name=&#39;Ipd&#39;, x=inf_df.loc[&#39;Russia&#39;].index, y=conf_df_pd.loc[&#39;Russia&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=inf_df.loc[&#39;Russia&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Russia&#39;])[0]) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Russia: Actual } I_{pd} text{ vs. convolution of deconvolution of } I_{pd}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see that our estimate for $ hat{E}_{pd}$ must be close to the reality of $E_{pd}$ as $I_{pd}$ is almost identical to $ hat{E}_{pd} circledast h_L$. . Of course, this holds only as long as our estimate of $h_L$ is close to reality. . 3. $ beta$ and $R$ from $E_{pd}$ and $I$ . As described above: $$ beta = frac{N~E_{pd}}{S~I}$$ and if $S$~$N$, then $$ beta = frac{E_{pd}}{I}$$ It is easier to simply say : $$R = frac{E_{pd}}{ gamma~I}$$ . #collapse_hide # Calculate R gam = 1/20.11 # As we say gamma is 1/20.11 R = Enext[:len(inf_df.loc[&#39;Russia&#39;])]*(1/gam)/inf_df.loc[&#39;Russia&#39;] fig = go.Figure(data=[ go.Scatter(name=&#39;R&#39;, x=inf_df.loc[&#39;Russia&#39;].index, y=R), go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Russia&#39;].index, y=Enext), go.Scatter(name=&#39;Inf&#39;, x=inf_df.loc[&#39;Russia&#39;].index, y=inf_df.loc[&#39;Russia&#39;]), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;R&#39;, title={ &#39;text&#39;:r&#39;$ text{Russia: R }$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see $R&lt;1$ since May 12th. . South Korea . We have daily data $I_{pd}$ and $R_{pd}$ (where $R_{pd}$ is the sum of deaths and recoveries) | We have our assumed $T_L$ and $T_I$ | . Analysis steps: . The first thing we want to check is whether $h_I circledast I_{pd} [j]$ gives us something close to $R_{pd}$ If not, why not? | Can we get an estimated $E_{pd}$ by deconvolution of $I_{pd}$ ? | What can that tell us about $R$ and $ beta$ ? | 1. Checking $h_I$ . #collapse_hide fig = go.Figure(data=[ go.Bar(name=&#39;Ipd&#39;, x=conf_df_pd.loc[&#39;Korea, Republic of&#39;].index, y=conf_df_pd.loc[&#39;Korea, Republic of&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=conf_df_pd.loc[&#39;Korea, Republic of&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Korea, Republic of&#39;]+deaths_df_pd.loc[&#39;Korea, Republic of&#39;])[0]), go.Bar(name=&#39;Rpd&#39;, x=rec_df_pd.loc[&#39;Korea, Republic of&#39;].index, y=rec_df_pd.loc[&#39;Korea, Republic of&#39;]), go.Scatter(name=&#39;Rpd=lowpass(Rpd)&#39;, x=rec_df_pd.loc[&#39;Korea, Republic of&#39;].index, y=lowpass(rec_df_pd.loc[&#39;Korea, Republic of&#39;])[0]), go.Scatter(name=&#39;Rpd=conv(Ipd)&#39;, x=conf_df_pd.loc[&#39;Korea, Republic of&#39;].index, y=signal.fftconvolve(h_I, conf_df_pd.loc[&#39;Korea, Republic of&#39;]+deaths_df_pd.loc[&#39;Korea, Republic of&#39;], mode=&#39;full&#39;)) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Korea, Republic of: Actual } R_{pd} text{ vs. } h_I[j] circledast I_{pd}[j]$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see the actual $R_{pd}$ lags $h_I[j] circledast I_{pd}[j]$ by about 2 days. . 2. Estimating $E_{pd}$ by deconvolution of $I_{pd}$ . #collapse_hide #Settting up for deconvolution of Ipd #regularization parameter alpha=2 # Setup up the resultant Ipd we want to compare our guess with Ipd=np.floor(lowpass(conf_df_pd.loc[&#39;Korea, Republic of&#39;])[0]) Ipd[Ipd&lt;0]=0 # Pad with last value i=0 while i &lt; 100: Ipd=np.append(Ipd, Ipd[-1]) i=i+1 # Find delay caused by h_L delay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode=&#39;full&#39;).argmax() # We want initial guess to simply be the result of the convolution delayed initial_guess=np.roll(Ipd,delay) Enext = initial_guess # AN array to record MSE between result we want and our iterated guess mse=np.array([]) mse=np.append(mse, 10000000) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Korea, Republic of&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Korea, Republic of&#39;])])) itercount=0 while mse[-1] &lt; mse[-2]: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Korea, Republic of&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Korea, Republic of&#39;])])) print(&quot;Iteration #&quot; + str(itercount) +&quot;: MSE= &quot;+str(mse[itercount])) print(&quot;Iteration #&quot; + str(itercount+1) +&quot;: MSE= &quot;+str(mse[-1])+&quot; so we use the result of the previous iteration.&quot;) . . Iteration #1: MSE= 1002.4468501850523 Iteration #2: MSE= 322.3563402426279 Iteration #3: MSE= 232.31445613577606 Iteration #4: MSE= 225.86390268488017 Iteration #5: MSE= 179.66814661742652 Iteration #6: MSE= 182.39958075753384 so we use the result of the previous iteration. . #collapse_hide # We can keep going the iteration until lowest MSE #change alpha if you like alpha=2 i=0 while i &lt; 10: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) print(msecalc(Ipd[:len(conf_df_pd.loc[&#39;Korea, Republic of&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Korea, Republic of&#39;])])) i=i+1 . . 149.3491402956683 149.80157409402977 149.3549551002663 149.80157409402977 149.3549551002661 149.80157409402977 149.3549551002663 149.80157409402977 149.3549551002661 149.80157409402977 . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Korea, Republic of&#39;].index, y=Enext), go.Scatter(name=&#39;Ipd=conv(deconv(Ipd))&#39;, x=inf_df.loc[&#39;Korea, Republic of&#39;].index, y=signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)), go.Bar(name=&#39;Ipd&#39;, x=inf_df.loc[&#39;Korea, Republic of&#39;].index, y=conf_df_pd.loc[&#39;Korea, Republic of&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=inf_df.loc[&#39;Korea, Republic of&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Korea, Republic of&#39;])[0]) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Korea, Republic of: Actual } I_{pd} text{ vs. convolution of deconvolution of } I_{pd}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see that our estimate for $ hat{E}_{pd}$ must be close to the reality of $E_{pd}$ as $I_{pd}$ is almost identical to $ hat{E}_{pd} circledast h_L$. . Of course, this holds only as long as our estimate of $h_L$ is close to reality. . 3. $ beta$ and $R$ from $E_{pd}$ and $I$ . As described above: $$ beta = frac{N~E_{pd}}{S~I}$$ and if $S$~$N$, then $$ beta = frac{E_{pd}}{I}$$ It is easier to simply say : $$R = frac{E_{pd}}{ gamma~I}$$ . #collapse_hide # Calculate R gam = 1/20.11 # As we say gamma is 1/20.11 R = Enext[:len(inf_df.loc[&#39;Korea, Republic of&#39;])]*(1/gam)/inf_df.loc[&#39;Korea, Republic of&#39;] fig = go.Figure(data=[ go.Scatter(name=&#39;R&#39;, x=inf_df.loc[&#39;Korea, Republic of&#39;].index, y=R), go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Korea, Republic of&#39;].index, y=Enext), go.Scatter(name=&#39;Inf&#39;, x=inf_df.loc[&#39;Korea, Republic of&#39;].index, y=inf_df.loc[&#39;Korea, Republic of&#39;]), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;R&#39;, title={ &#39;text&#39;:r&#39;$ text{Korea, Republic of: R }$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see $R&lt;1$ since early March, but has snce climbed back up to 1 and even slightly above that. . Switzerland . We have daily data $I_{pd}$ and $R_{pd}$ (where $R_{pd}$ is the sum of deaths and recoveries) | We have our assumed $T_L$ and $T_I$ | . Analysis steps: . The first thing we want to check is whether $h_I circledast I_{pd} [j]$ gives us something close to $R_{pd}$ If not, why not? | Can we get an estimated $E_{pd}$ by deconvolution of $I_{pd}$ ? | What can that tell us about $R$ and $ beta$ ? | 1. Checking $h_I$ . #collapse_hide fig = go.Figure(data=[ go.Bar(name=&#39;Ipd&#39;, x=conf_df_pd.loc[&#39;Switzerland&#39;].index, y=conf_df_pd.loc[&#39;Switzerland&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=conf_df_pd.loc[&#39;Switzerland&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Switzerland&#39;]+deaths_df_pd.loc[&#39;Switzerland&#39;])[0]), go.Bar(name=&#39;Rpd&#39;, x=rec_df_pd.loc[&#39;Switzerland&#39;].index, y=rec_df_pd.loc[&#39;Switzerland&#39;]), go.Scatter(name=&#39;Rpd=lowpass(Rpd)&#39;, x=rec_df_pd.loc[&#39;Switzerland&#39;].index, y=lowpass(rec_df_pd.loc[&#39;Switzerland&#39;])[0]), go.Scatter(name=&#39;Rpd=conv(Ipd)&#39;, x=conf_df_pd.loc[&#39;Switzerland&#39;].index, y=signal.fftconvolve(h_I, conf_df_pd.loc[&#39;Switzerland&#39;]+deaths_df_pd.loc[&#39;Switzerland&#39;], mode=&#39;full&#39;)) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Switzerland: Actual } R_{pd} text{ vs. } h_I[j] circledast I_{pd}[j]$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see the actual $R_{pd}$ leads $h_I[j] circledast I_{pd}[j]$ by about 7 days. . 2. Estimating $E_{pd}$ by deconvolution of $I_{pd}$ . #collapse_hide #Settting up for deconvolution of Ipd #regularization parameter alpha=2 # Setup up the resultant Ipd we want to compare our guess with Ipd=np.floor(lowpass(conf_df_pd.loc[&#39;Switzerland&#39;])[0]) Ipd[Ipd&lt;0]=0 # Pad with last value i=0 while i &lt; 100: Ipd=np.append(Ipd, Ipd[-1]) i=i+1 # Find delay caused by h_L delay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode=&#39;full&#39;).argmax() # We want initial guess to simply be the result of the convolution delayed initial_guess=np.roll(Ipd,delay) Enext = initial_guess # AN array to record MSE between result we want and our iterated guess mse=np.array([]) mse=np.append(mse, 10000000) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Switzerland&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Switzerland&#39;])])) itercount=0 while mse[-1] &lt; mse[-2]: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) mse=np.append(mse, msecalc(Ipd[:len(conf_df_pd.loc[&#39;Russia&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Switzerland&#39;])])) print(&quot;Iteration #&quot; + str(itercount) +&quot;: MSE= &quot;+str(mse[itercount])) print(&quot;Iteration #&quot; + str(itercount+1) +&quot;: MSE= &quot;+str(mse[-1])+&quot; so we use the result of the previous iteration.&quot;) . . Iteration #1: MSE= 1470.6459337193169 Iteration #2: MSE= 394.3484792946301 Iteration #3: MSE= 293.667188764407 Iteration #4: MSE= 241.69003136951196 Iteration #5: MSE= 173.82451248640552 Iteration #6: MSE= 163.79909744743827 Iteration #7: MSE= 122.78528193323216 Iteration #8: MSE= 122.56820945779762 Iteration #9: MSE= 96.98263162163703 Iteration #10: MSE= 99.46363623766511 so we use the result of the previous iteration. . #collapse_hide # We can keep going the iteration until lowest MSE #change alpha if you like alpha=2 i=0 while i &lt; 10: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) print(msecalc(Ipd[:len(conf_df_pd.loc[&#39;Switzerland&#39;])], signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(conf_df_pd.loc[&#39;Switzerland&#39;])])) i=i+1 . . 58.62220454752213 58.748515359200475 58.62220454752213 58.748515359200475 58.62220454752213 58.748515359200475 58.62220454752213 58.748515359200475 58.62220454752213 58.748515359200475 . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Switzerland&#39;].index, y=Enext), go.Scatter(name=&#39;Ipd=conv(deconv(Ipd))&#39;, x=inf_df.loc[&#39;Switzerland&#39;].index, y=signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)), go.Bar(name=&#39;Ipd&#39;, x=inf_df.loc[&#39;Switzerland&#39;].index, y=conf_df_pd.loc[&#39;Switzerland&#39;]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=inf_df.loc[&#39;Switzerland&#39;].index, y=lowpass(conf_df_pd.loc[&#39;Switzerland&#39;])[0]) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Switzerland: Actual } I_{pd} text{ vs. convolution of deconvolution of } I_{pd}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see that our estimate for $ hat{E}_{pd}$ must be close to the reality of $E_{pd}$ as $I_{pd}$ is almost identical to $ hat{E}_{pd} circledast h_L$. . Of course, this holds only as long as our estimate of $h_L$ is close to reality. . 3. $ beta$ and $R$ from $E_{pd}$ and $I$ . As described above: $$ beta = frac{N~E_{pd}}{S~I}$$ and if $S$~$N$, then $$ beta = frac{E_{pd}}{I}$$ It is easier to simply say : $$R = frac{E_{pd}}{ gamma~I}$$ . #collapse_hide # Calculate R gam = 1/20.11 # As we say gamma is 1/20.11 R = Enext[:len(inf_df.loc[&#39;Switzerland&#39;])]*(1/gam)/inf_df.loc[&#39;Switzerland&#39;] fig = go.Figure(data=[ go.Scatter(name=&#39;R&#39;, x=inf_df.loc[&#39;Switzerland&#39;].index, y=R), go.Scatter(name=&#39;Epd=deconv(Ipd)&#39;, x=inf_df.loc[&#39;Switzerland&#39;].index, y=Enext), go.Scatter(name=&#39;Inf&#39;, x=inf_df.loc[&#39;Switzerland&#39;].index, y=inf_df.loc[&#39;Switzerland&#39;]), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;R&#39;, title={ &#39;text&#39;:r&#39;$ text{Switzerland: R }$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see $R$ declined rapidly after March 12th to $R&lt;1$ on May 30th, but has since grown back to close to 1. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/modeling/seir/epidemiology/stochastic/covid-19/real-world/2020/06/02/real-world.html",
            "relUrl": "/modeling/seir/epidemiology/stochastic/covid-19/real-world/2020/06/02/real-world.html",
            "date": "  Jun 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Scraping COVID-19 data from data.gouv.fr",
            "content": ". Introduction . As the summary explains, this blog post will very quickly explain how to automatically download French government data on hospitalization and testing pertaining to COVID19. . Data sources . Hospitalization data | . The various datasets concerning hospitalization data are found here. . If you follow the link you will find 4 csv datasets concerning hospitalization data along with 5 other csv files with metadata and documentation. . Testing data | . The various datasets concerning testing data are found here. . If you follow the link you will find 2 csv datasets concerning testing data along with 2 other csv files with metadata and documentation. . In both cases we want to download the first of the links since they contain the pertinent daily updated data (do have a look manually at the metadata and documentation files to make sure this is what you want). . Code . #collapse_show # Import libraries used below import requests import urllib.request import urllib.parse import time import io from bs4 import BeautifulSoup import pandas as pd import datetime import os . . Getting the main page . Let&#39;s first have a look ath the main landing page that I provided above. . # Store URL for each page url_cases = &#39;https://www.data.gouv.fr/fr/datasets/donnees-hospitalieres-relatives-a-lepidemie-de-covid-19/&#39; url_tests = &#39;https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-tests-de-depistage-de-covid-19-realises-en-laboratoire-de-ville/&#39; . # Get response for each URL response_cases = requests.get(url_cases) response_tests = requests.get(url_tests) . The response here should be 200 (see life of codes here). . print(response_cases, response_tests) . &lt;Response [200]&gt; &lt;Response [200]&gt; . # Save the actual content of the page returned with BeautifulSoup soupcases = BeautifulSoup(response_cases.text, &quot;html.parser&quot;) souptests = BeautifulSoup(response_tests.text, &quot;html.parser&quot;) . # Let&#39;s look at the links in the main page (for testing data - if you want cases, replace souptests with soupcases below) for i in range(len(souptests.find_all(&#39;a&#39;, class_=&quot;btn btn-sm btn-primary&quot;))): print(souptests.find_all(&#39;a&#39;, class_=&quot;btn btn-sm btn-primary&quot;)[i].get(&#39;href&#39;)) . None https://www.data.gouv.fr/fr/datasets/r/b4ea7b4b-b7d1-4885-a099-71852291ff20 None https://www.data.gouv.fr/fr/datasets/r/72050bc8-9959-4bb1-88a0-684ff8db5fb5 None https://www.data.gouv.fr/fr/datasets/r/971c5cbd-cd80-4492-b2b3-c3deff8c1f5e None https://www.data.gouv.fr/fr/datasets/r/db378f2a-83a1-40fd-a16c-06c4c8c3535d https://www.data.gouv.fr/fr/datasets/r/49ba79e6-0153-40b1-b050-821e102959eb None https://www.data.gouv.fr/fr/datasets/r/59e82d52-e07a-4ae8-9a49-2d1fd2d2ec21 . We see that the petrtinent file in each cases (testing or hospitalization data) are the first links in their page. So we save only this one as donw below: . # If we want to save that first URL we can do as follows casescsvurl = soupcases.find_all(&#39;a&#39;, class_=&quot;btn btn-sm btn-primary&quot;)[1].get(&#39;href&#39;) testscsvurl = souptests.find_all(&#39;a&#39;, class_=&quot;btn btn-sm btn-primary&quot;)[1].get(&#39;href&#39;) . Getting the CSV data . We now have the URL for the CSV files we want so we&#39;ll do similar steps as above to download these files. . # Similaraly as above, requests.get the CSV URL: rectests = requests.get(testscsvurl) reccases = requests.get(casescsvurl) . What to do with the CSV data . Now that you have the data, what to do with it? . It depends on your purpose I guess: . First write the data to a CSV file which you then read | Directly read the data | . By first writing the CSV file to drive . # This will write the data into cases.csv file # Of course you need to replace the actual path to the folder you want in the code below: with open(os.path.join(&quot;/path/to/folder&quot;, &quot;cases.csv&quot;), &#39;wb&#39;) as f: f.write(reccases.content) . # Same thing for testing data # This will write the data into tests.csv file # Of course you need to replace the actual path to the folder you want in the code below: with open(os.path.join(&quot;/path/to/folder&quot;, &quot;tests.csv&quot;), &#39;wb&#39;) as f: f.write(rectests.content) . # You can then read that csv file to use in your data analysis: tests = pd.read_csv(&#39;tests.csv&#39;, sep=&#39;;&#39;, dtype={&#39;dep&#39;: str, &#39;jour&#39;: str, &#39;clage_covid&#39;: str, &#39;nb_test&#39;: int, &#39;nb_pos&#39;: int, &#39;nb_test_h&#39;: int, &#39;nb_pos_h&#39;: int, &#39;nb_test_f&#39;: int, &#39;nb_pos_f&#39;: int}, parse_dates = [&#39;jour&#39;]) cases = pd.read_csv(&#39;cases.csv&#39;, sep=&#39;;&#39;, dtype={&#39;dep&#39;: str, &#39;jour&#39;: str, &#39;hosp&#39;: int, &#39;rea&#39;: int, &#39;rad&#39;: int, &#39;dc&#39;: int}, parse_dates = [&#39;jour&#39;]) . Note in the code above I had previously looked through the raw csv data to underdstand how to parse it. . Directly reading the data (bypassing the writing CSV file step) . cases = pd.read_csv(io.StringIO(requests.get(casescsvurl).content.decode(&#39;utf-8&#39;)), sep=&#39;;&#39;, dtype={&#39;dep&#39;: str, &#39;jour&#39;: str, &#39;hosp&#39;: int, &#39;rea&#39;: int, &#39;rad&#39;: int, &#39;dc&#39;: int}, parse_dates = [&#39;jour&#39;]) tests = pd.read_csv(io.StringIO(requests.get(testscsvurl).content.decode(&#39;utf-8&#39;)), sep=&#39;;&#39;, dtype={&#39;dep&#39;: str, &#39;jour&#39;: str, &#39;hosp&#39;: int, &#39;rea&#39;: int, &#39;rad&#39;: int, &#39;dc&#39;: int}, parse_dates = [&#39;jour&#39;]) . Other stuff . Parsing/Converting URI into readable format . It sometimes happends that links are provided in URI (URL symbols encoded into % symbols...) . You generally need to convert those back to correct URLs, example below: . # Example URI testurl = &#39;https%3A%2F%2Fstatic.data.gouv.fr%2Fresources%2Fdonnees-hospitalieres-relatives-a-lepidemie-de-covid-19%2F20200505-190040%2Fdonnees-hospitalieres-covid19-2020-05-05-19h00.csv&#39; . # Convert with following line: urllib.parse.unquote(testurl) . &#39;https://static.data.gouv.fr/resources/donnees-hospitalieres-relatives-a-lepidemie-de-covid-19/20200505-190040/donnees-hospitalieres-covid19-2020-05-05-19h00.csv&#39; . A quick look at French testing data from scratch . Let&#39;s quickly see how, from scratch, we can use code above to scrape testing data and plot it quickly. . Note the data only includes city testing centers and does not include hospital testing. . # Use main page URL url_tests = &#39;https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-tests-de-depistage-de-covid-19-realises-en-laboratoire-de-ville/&#39; response_tests = requests.get(url_tests) . # Find correct CSV file URL souptests = BeautifulSoup(response_tests.text, &quot;html.parser&quot;) testscsvurl = souptests.find_all(&#39;a&#39;, class_=&quot;btn btn-sm btn-primary&quot;)[1].get(&#39;href&#39;) . # Read CSV file into tests variable rectests = requests.get(testscsvurl) tests = pd.read_csv(io.StringIO(requests.get(testscsvurl).content.decode(&#39;utf-8&#39;)), sep=&#39;;&#39;, dtype={&#39;dep&#39;: str, &#39;jour&#39;: str, &#39;hosp&#39;: int, &#39;rea&#39;: int, &#39;rad&#39;: int, &#39;dc&#39;: int}, parse_dates = [&#39;jour&#39;]) . #collapse_hide import plotly.express as px import plotly.graph_objects as go # We want overall testing for France, se we groupby Day and sum: (filtering for clage_covid = 0 means not differentiated between age groups) df = tests[tests.clage_covid==&#39;0&#39;].groupby([&#39;jour&#39;]).sum() fig = go.Figure(data=[ go.Bar(name=&#39;Positive tests&#39;, x=df.index, y=df.nb_pos, marker_color=&#39;red&#39;), go.Bar(name=&#39;Total tests&#39;, x=df.index, y=df.nb_test, marker_color=&#39;blue&#39;) ]) fig.update_layout( title= &#39;Daily positive and total testing data in France&#39;, xaxis_title = &#39;Date&#39;, yaxis_title = &#39;Number of tests (total and positive)&#39;, barmode=&#39;group&#39; ) fig.show() . . . . Conclusion . Very easy to incorporate this into a python script to automate. . This is only the very basic of scraping, a lot more could be done, maybe in another blog post. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/scraping/covid-19/2020/04/15/web-scraping.html",
            "relUrl": "/scraping/covid-19/2020/04/15/web-scraping.html",
            "date": "  Apr 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Testing - what to be aware of",
            "content": ". Motivation for write-up . The real-world motivation for this write-up can be found under Story Time section, but I first wanted to give a bit of theoretical background here. . The importance of testing has been greatly talked about these last few weeks/months with the emergence of the COVID-19 pandemic with numerous articles being published, all underlining the importance of testing. The part emphasized is the fact that early testing allows for quick isolation of sick individuals and tracing of their potential contacts, and thus limiting the potential for spread. . The kind of test for this are called virologic testing and test directly for the presence of virus in an individual (active infection). This is done with Nucleic Acid Tests, or NAT, usually after amplification of the very small amount of genetic material present via Polymerase Chain Reaction. Results are available within hours or days and require diagnostic machinery and specialists. . Knowing who has been infected is also important as it could allow already recovered patients (who are thought to gain immunity from COVID-19) to return safely to work and live basically normally. Tests that check for past infections exist, and are called serology or antibody tests. They check for specific antibodies that match those deveopped during an immune response response against SARS-CoV-2. . This is all good in theory, but with a disease that can cause such serious conditions as COVID-19 can, we need to be sure a positive test means for certain that person is now immmune, or we risk allowing individuals with false positives to return to normal when they should not, and continue the damaging spread of the disease. . The aim of this short right-up is to clear up some misconceptions around testing protocols, discuss the importance of false positives, false negatives, and its importance to guiding public health policies. The idea is basically to answer the following questions: . How many tests should return positive for a person to be, say 95% or 99% person sure he is now immune? | What if a different test is negative? | . Specificity, Sensitivity, False positives, False negatives? . As briefly explained above, neither virological and serological tests are infallible. False positives i.e. healthy individuals with a positive test, and false negatives i.e. infected indiviuals with negative tests, can, and do happen. . There are numerous reasons how and why this can happen, but that is not the point of this write-up. Here, we acknowledge the fact non-perfect tests are a reality and establish testing protocol to deal with that fact. . Thankfully, before being shipped out, the various laboratories test their tests. They are able to characterize them rather precisely and give an indiction of how useful they may be with two important values: . Specificity | Sensitivity | . Specificity . Specificity is the true negative rate - i.e. the percentage of healthy people correctly identified as such (for antibody testing, it is the percentage of people not having antibodies correctly identified as such). . In other words, if a test was used on 100 people who do not have antibodies, the number of people correctly identified as not hvaing antibodies is the specificity. . A perfect test with 100% specificity, means there are no false positives. This has major implications in the current context of COVID-19 pandemic as having an anitbody test with 100% specificity would allow immune people to know so for certain (as long as research showed antibodies gave immunity). . Mathematically, we pose specificity as follows: . $Specificity = frac{True negatives}{True negatives + False posiives}$ . Sensitivity . Sensitivity is the true positive rate - i.e. the percentage of infected people correctly identified as such (for antibody tests, it is the percentage of people having antibodies correctly identified as such). . In other words, if an antibody test was used on 100 people with antibodies, the number of people correctly identified as having anitbodies is the sensitivity. . A perfect test with 100% sensitivity, means there are no false negatives. . Mathematically, we pose specificity as follows: . $Sensitivity = frac{True positives}{True positives + False negatives}$ . Prevalence . Prevalence is simply the proportion of a population that has a certain characteistic. In the current context of antibody testing, the prevalence will be defined as the proportion of people who have antibody conferring immunity to COVID-19 (i.e. the proportion that has had the disease). . $Prevalence = frac{ # People with antibodies}{Total number of people}$ . Where $Total number of people$ is simply $ # People with antibodies + # People without antibodies$ . Story time - Part 1 . Specificity, sensitivity, prevalence, false negatives, false positives.. This is all good, but it can be a bit abstract outside of a specific testing context. . Let&#39;s use the current COVID-19 pandemic as an example. . Antibody tests are finally becoming available to the general population, and you want to know if you&#39;ve had the disease (developped antibodies against it). . Now let&#39;s say you had influenza like symptoms back in January or February, would you expect a positive or negative result on the test? | What if you haven&#39;t been sick but want to check out of curiosity, what result would you expect? | If it does come back positive, how certain would you be that you actually have those antibodies and it wasn&#39;t a false positive? | You decide to use a second test to make sure, again it comes positive. Now how certain are you that you have antibodies? | Out of extreme precaution you decide to try a test from another laboratory (different specificity and sensitivity), and this time the test comes back negative. It&#39;s become a bit more complex to evaluate your situation now. | So how about another test from this second laboratory? Again, negative.. Two positives, two negatives - what can you make of this information? | . However far fetched this scenario may seem, it is exactly what happened to this Florida physician: In January I got very sick (flu like illness but much worse), no sleep for 2 days and almost checked myself into the ED. Had no idea what it was. Today I checked my #COVID19 Antibody status......IgG+ Only (sign of past infection). Mixed emotions. Will retest tomorrow. pic.twitter.com/ExLYi5qPBx . &mdash; Peter Antevy (@HandtevyMD) April 2, 2020 . There are two questions that come out of this story: . After those 4 tests, what is the probability that Dr. Antevy has those antibodies - or more generally, can we calculate the probability of someone having antibodies given their test results? | What should be the threshold of such a probability to minimize the risk of someone without antibodies going out in nature thinking he does ? (obviously if someone has 10 positive tests in a row, it seems sure enough that person has antibodies) This pushes for the need of rigorous testing protocol. | . Calculating probabilites given test results . Clearly, our objective is to calculate the probability that a person has antibodies, or: . $P(seropositive)$ . Conditional probabilities . Baye&#39;s theorem describes probabilities when given evidence. . Say a person has had some COVID-19 symptoms (dry cough, fever, loss of smell, slight fever) a few weeks ago. He might say there is a 75% chance that he had contracted COVID-19, and 25% chance it was another disease. In this case: . $P(seropositive) = 0.75$ . Now this person goes to get an antibody test. What is the probability he is seropositive given a positive or negative result? Baye&#39;s theorem allows us to write it as follows: . $P(seropositive | positive test) = frac{P(positive test | seropositive) * P(seropositive)}{P(positive test)}$ . and . $P(seropositive | negative test) = frac{P(negative test | seropositive) * P(seropositive)}{P(negative test)}$ . Note: . $P(seropositive)$ is called the prior. . $P(seropositive | positive test)$ and $P(seropositive | negative test)$ are called the posterior. . $P(Positive test)$ . Let&#39;s have a look at the probability of getting a positive test - there are 2 ways to get a positive result : . A false positive | A true positive | . $P(False positive) = P(Positive test | seronegative)*P(seronegative)$ . And . $P(True positive) = P(Positive test | seropositive)*P(seropositive)$ . So: . $P(Positive test) = P(Positive test | seropositive)*P(seropositive) + P(Positive test | seronegative)*P(seronegative)$ . Sensitivity and Specificity revisited . Earlier we saw: . $Sensitivity = frac{True positives}{True positives + False negatives}$ . And that . $Specificity = frac{True negatives}{True negatives + False positives}$ . But we can rewrite these equations as follows: . $Sensitivity = P(Positive test | seropositive)$ . And . $Specificity = P(Negative test | seronegative) = 1-P(Positive test | seronegative)$ . Re-writing the posterior probability . Using Baye&#39;s rule and the calculations above we can re-write the posterior equations as follows: . $P(seropositive | Positive test) = frac{Sensitivity*P(seropositive)}{Sensitivity*P(seropositive)+ (1-Specificity)*(1-P(seropositive))}$ . And: . $P(seronegative | Negative test) = frac{Specificity*(1-P(seropositive))}{Specificity*(1-P(seropositive))+(1-Sensitivity)*P(seropositive)}$ . The role of prevalence in these calculations . The equations above describe the probability for an individual given a test result and their prior probability. This prior probability can be estimated given presence or not of symptoms, contact with other infected individuals, location, other diagnostics, etc... . However, on a population level, if we were to test a random individual, this prior becomes the prevalence and for a random individual, the equations become: . $P(seropositive | Positive test) = frac{Sensitivity*Prevalence}{Sensitivity*Prevalence+(1-Specificity)*(1-Prevalence)}$ . And: . $P(seronegative | Negative test) = frac{Specificity*(1-Prevalence)}{Specificity*(1-Prevalence)+(1-Sensitivity)*Prevalence}$ . Serology testing simulation . Let&#39;s see what these equations look like in practice. . #collapse_hide import numpy as np import plotly.express as px import plotly.graph_objects as go . . #collapse_hide # Let&#39;s write a function to output the posterior probability given prior, test result, and test characteristics (sensitivity and specificity) def Pposterior(Pprior, test_res, Sn, Sp): if test_res: return ((Sn * Pprior) / (Sn * Pprior + (1-Sp) * (1-Pprior))) else: return (1-((Sp * (1-Pprior))/(1-(Sn * Pprior + (1-Sp) * (1-Pprior))))) . . Say we have an antibody test with 90% sensitivity and 90% specificity - meaning we have 90% true positives and 90% true negatives, we obtain a graph as below: . #collapse_hide # Below is the prior probability of being infected: num=10000 Pprior = np.linspace((1/num),(num-1)/num,num=num) # Graph the results fig = go.Figure(data=[ go.Scatter(name=&#39;Test negative&#39;, x=100*Pprior, y=100*Pposterior(Pprior, False, 0.9, 0.9), line_color=&quot;green&quot;), go.Scatter(name=&#39;Test positive&#39;, x=100*Pprior, y=100*Pposterior(Pprior, True, 0.9, 0.9), line_color=&quot;red&quot;), go.Scatter(name=&#39;No test&#39;, x=100*Pprior, y=100*Pprior, line_color=&quot;blue&quot;) ]) fig.update_layout( xaxis_title = &#39;Prior probability of being infected&#39;, yaxis_title = &#39;Posterior probability of being infected given test result&lt;br&gt;Specificity=90.0&lt;br&gt;Sensitivity=90.0&#39; ) fig.show() . . . . If you hover the mouse over the lines you can see the exact numbers. . As you can see, a positive or negative test does give more information than no test, but it doesn&#39;t quite give you certainty. . Story time - Part 2 . Let&#39;s circle back to our Dr. Antevy with his two positive tests and the two negative tests. . Prior to any tests, he was about 50% certain of having contracted COVID-19 based on his assesment of his symptoms, location, contact with other people, etc.. . Let&#39;s go through his test results to see what his posterior probability of having antibodies is. . #collapse_hide # Let&#39;s make a new function for multiple tests in a row def PposteriorM(Pprior, test_res): x = Pprior for tr, sn, sp in test_res: if tr == 1: x = (sn * x) / (sn * x + (1-sp) * (1-x)) elif tr == 0: x = (1-((sp * (1-x))/(1-(sn * x + (1-sp) * (1-x))))) return x . . Let&#39;s say these are the characteristics of the tests he used: . Test 1 and 2: Specificity = 0.90 | Sensitivity = 0.99 | . | Test 3 and 4: Specificity = 0.97 | Sensitivity = 0.95 | . | . So a highly sensitive first test followed by a rather good allround test, a bit more specific than the first. . #collapse_hide # Below is the prior probability of being infected: num=10000 Pprior = np.linspace((1/num),(num-1)/num,num=num) # Test characteristics test_results = [(1, 0.99, 0.90),(1, 0.99, 0.90),(0,0.95,0.97),(0,0.95,0.97)] # Graph the results fig = go.Figure(data=[ go.Scatter(name=&#39;1 - 1st positive test&#39;, x=100*Pprior, y=100*PposteriorM(Pprior, [test_results[0]])), go.Scatter(name=&#39;2 - 2nd positive test&#39;, x=100*Pprior, y=100*PposteriorM(Pprior, test_results[0:2])), go.Scatter(name=&#39;3 - 1st negative test&#39;, x=100*Pprior, y=100*PposteriorM(Pprior, test_results[0:3])), go.Scatter(name=&#39;4 - 2nd negative test&#39;, x=100*Pprior, y=100*PposteriorM(Pprior, test_results[0:4])) ]) fig.update_layout( xaxis_title = &#39;Prior probability of being infected&#39;, yaxis_title = &#39;Posterior probability of being infected given test results&#39; ) fig.show() . . . . So let&#39;s go through step-by-step: . Before any test, he was about 50% sure he contracted COVID-19 | After the 1st positive test, this goes up to 90.8% sure | After the 2nd positive test, up to 99.0% sure | But the 1st negative test drops it back to 83.5% | And the 2nd negative all the way down to 20.7% | . What if this was done on a random person in France for example, and all 4 tests were positive. . Then the prior would be the prevalence in France (0.2%) instead of 50%, and the step by step would be as follows: . Before any test, about 0.20% | After 1st positive: still only 1.9% chance of being seropositive | After 2nd positive test: only 16.4% chance of seropositive | After 3rd positive: 86% | And after 4th positive test 99.5% | . So it took about 4 positive tests for a random person in France to become confident enough to be seropositive. . Discussion . The results above strongly underline the need for clear testing protocols and clear understanding of the interpretation of test results. . Wtih a disease that can be so devastating as COVID-19, a few things should be kept in mind: . A high treshold should be used to hedge the risk a false positive | Multiple tests should be taken | Multiple tests with different characteristics (ideally at least one with high sensitivity, and one with high specificity) | .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/covid-19/testing/serology/2020/04/12/on-testing.html",
            "relUrl": "/covid-19/testing/serology/2020/04/12/on-testing.html",
            "date": "  Apr 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Epidemic modeling - Part 7",
            "content": ". . #collapse_hide !pip install plotly==4.9.0 import pandas as pd import numpy as np import math from scipy import signal import plotly.graph_objects as go import plotly.express as px from scipy.stats import expon from scipy.stats import gamma from scipy.stats import weibull_min from numpy.random import default_rng rng = default_rng() # Let&#39;s build a numerical solution def seir_model(init, parms, days): S_0, E_0, I_0, R_0 = init Epd, Ipd, Rpd = [0], [0], [0] S, E, I, R = [S_0], [E_0], [I_0], [R_0] dt=0.1 t = np.linspace(0,days,int(days/dt)) sigma, beta, gam = parms for _ in t[1:]: next_S = S[-1] - beta*S[-1]*I[-1]*dt Epd.append(beta*S[-1]*I[-1]*dt) next_E = E[-1] + (beta*S[-1]*I[-1] - sigma*E[-1])*dt Ipd.append(sigma*E[-1]*dt) next_I = I[-1] + (sigma*E[-1] - gam*I[-1])*dt Rpd.append(gam*I[-1]*dt) next_R = R[-1] + (gam*I[-1])*dt S.append(next_S) E.append(next_E) I.append(next_I) R.append(next_R) return np.stack([S, E, I, R, Epd, Ipd, Rpd]).T . . Collecting plotly==4.9.0 Downloading https://files.pythonhosted.org/packages/bf/5f/47ab0d9d843c5be0f5c5bd891736a4c84fa45c3b0a0ddb6b6df7c098c66f/plotly-4.9.0-py2.py3-none-any.whl (12.9MB) || 12.9MB 313kB/s Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.9.0) (1.15.0) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.9.0) (1.3.3) Installing collected packages: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-4.9.0 . #collapse_hide # Need this new function for model below: def make_df(p,num_E, num_I, num_R): df = pd.DataFrame(np.full((p,1), &#39;S&#39;).T[0], columns=[&#39;State&#39;]) df[&#39;Day&#39;] = 0 tochange=df.loc[rng.choice(p, size=num_E+num_I+num_R, replace=False),&#39;State&#39;].index df.loc[tochange[0:num_E],&#39;State&#39;] = &#39;E&#39; df.loc[tochange[num_E:num_I+num_E],&#39;State&#39;] = &#39;I&#39; df.loc[tochange[num_E+num_I:num_E+num_I+num_R],&#39;State&#39;] = &#39;R&#39; return df . . #collapse_hide def seir_model_stoch(beta, p, num_E, num_I, num_R, days): # Initialize population dataframe with data given by user df = make_df(p,num_E, num_I, num_R) # This variable is used to track daily value of beta if it varies over time xxbeta=np.array([],dtype=float) # Initialize the arrays to return # Below are numbers of S, E, I, R total S=np.array([],dtype=int) E=np.array([],dtype=int) I=np.array([],dtype=int) R=np.array([],dtype=int) # Below are the daily additions in S, E, I, R Spd=np.array([],dtype=int) Epd=np.array([],dtype=int) Ipd=np.array([],dtype=int) Rpd=np.array([],dtype=int) b=beta # Stochastic model so use random values to decide on progression rand = np.random.random(size=(p,days)) # Depending if you want exponential or gamma distribution for T_Latent EtoI = gamma.rvs(1.8,loc=0.9,scale=(5.2-1.8)/0.9,size=p) # Depending if you want exponential, gamma, or Weibull distribution for T_Infectious # Uses distributions found on blog part 3 ItoR = weibull_min.rvs(2.3, loc=2, scale=20.11, size=p) # Iterate over every day the simulation is run for j in range(0,days-1): # Record daily beta values xxbeta=np.append(beta, b) # First we get the index of the individuals that will change state today: # Random number tells you which &#39;S&#39; have been exposed on this day StoE_index = df.loc[(df.State == &#39;S&#39;) &amp; (rand[:,j] &lt; b[j]*len(np.where(df.State==&#39;I&#39;)[0])/p)].index # For each row, if a person has been a certain number of days in E, they will go to I # This follows EtoI variable which is either exponential or gamma distributed according to above EtoI_index = df.loc[(df.State == &#39;E&#39;) &amp; (j-df.Day &gt;= EtoI)].index # Similaraly as above # For each row, if a person has been a certain number of days in I, they will go to R # This follows EtoI variable which is either exponential or gamma distributed according to above ItoR_index = df.loc[(df.State == &#39;I&#39;) &amp; (j-df.Day &gt;= ItoR)].index # Use indexes collected above to populate per day values Epd = np.append(Epd,len(StoE_index)) Ipd = np.append(Ipd,len(EtoI_index)) Rpd = np.append(Rpd,len(ItoR_index)) # Now we use the indexes collected above randomly to change the actual population dataframe to the new states df.loc[ItoR_index, &#39;State&#39;] = &#39;R&#39; df.loc[EtoI_index, &#39;State&#39;] = &#39;I&#39; df.loc[StoE_index, &#39;State&#39;] = &#39;E&#39; df.loc[ItoR_index, &#39;Day&#39;] = j df.loc[EtoI_index, &#39;Day&#39;] = j df.loc[StoE_index, &#39;Day&#39;] = j df.loc[ItoR_index, &#39;DayR&#39;] = j df.loc[EtoI_index, &#39;DayI&#39;] = j df.loc[StoE_index, &#39;DayE&#39;] = j # Append the S, E, I, and R arrays S=np.append(S,len(np.where(df.State==&#39;S&#39;)[0])) E=np.append(E,len(np.where(df.State==&#39;E&#39;)[0])) I=np.append(I,len(np.where(df.State==&#39;I&#39;)[0])) R=np.append(R,len(np.where(df.State==&#39;R&#39;)[0])) # Code below for control measures to reduce beta values # if ((I[-1] &gt; 1000) &amp; (Ipd[-1] &gt; 399)): # b = beta2 # elif ((I[-1] &gt; 1000) &amp; (Ipd[-1] &lt; 400)): # b = beta3 Epd[0]+=num_E Ipd[0]+=num_I Rpd[0]+=num_R return S,E,I,R, Epd, Ipd, Rpd, xxbeta, df . . Motivation for write-up . This is the 7th part of a multi-part series blog post on modeling in epidemiology. . The goal of this 7th installment is to expand on the notions seen in part 5. . Most notably, the idea is to expand on the notion of convolution and deconvolution and to see how it can be useful to describe an epidemic. . Available data . If you look at the the COVID-19 trackers around the web, or even mine, you can get a sense of what data is available for study. . Generally speaking we have the following: . Total number of positive individuals | Daily number of newly diagnosed individuals | Daily number of recovered individuals | Daily number of deaths | . Research has also shown some vague numbers on both $T_{Latent}$ and $T_{Infectious}$. . About the data: . We don&#39;t have data for exposure - how can we get it using deconvolution ?? | The data for some regions can be very sparse | . Convolution . Convolution to get $I_{pd}$ . We have seen that daily new infectious individuals is given by: . $$I_{pd}[j] = sum_{n_L=0}^{M_L-1}h_L[n_L]~E_{pd}[j-n_L]= h_L[j] circledast E_{pd}[j]$$ . Where $h_L[j]$ describes the distribution of $T_{Latent}$. . Convolution to get $R_{pd}$ . Similarly, we have seen that daily new recovered individuals (deaths + recoveries in fact) is given by: . $$R_{pd}[j] = sum_{n_I=0}^{M_I-1}h_I[n_I]~I_{pd}[j-n_I]= h_I[j] circledast I_{pd}[j]$$ . Where $h_I[j]$ describes the distribution of $T_{Infectious}$. . Finding $h_L[j]$ and $h_I[j]$ . $h_L[j]$ is simply the probability of an individual having a latent period of j days | $h_I[j]$ is similarly the probability of an individual having an infectious period of j days | . #collapse_show days = np.arange(100) cdf = pd.DataFrame({ &#39;T_Latent&#39;: gamma.cdf(days, 1.8,loc=0.9,scale=(5.2-1.8)/0.9), &#39;T_Infectious&#39;: weibull_min.cdf(days, 2.3,loc=2,scale=20.11) }) h_L = cdf.diff().T_Latent h_I = cdf.diff().T_Infectious h_L[0] = 0 h_I[0] = 0 . . Convolution in practice . Comparing $I_{pd}$ from the model to $I_{pd}$ obtained by convolving $R_{pd}$ . First run the SEIR model to obtain the actual $I_{pd}$: . #collapse_hide # Define parameters for stochastic model days = 300 p = 10000 num_E = 1 num_I = 0 num_R = 0 beta_stoch = 0.5*np.ones(days) # Run 2 stochastic simulations results_stoch0 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days) . . Now obtain $h_L[j] circledast E_{pd}[j]$: . #collapse_hide Ipd=signal.fftconvolve(h_L, results_stoch0[4], mode=&#39;full&#39;) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Ipd_Actual&#39;, x=np.arange(len(results_stoch0[5])), y=results_stoch0[5]), go.Scatter(name=&#39;Ipd_convolved&#39;, x=np.arange(len(Ipd)), y=Ipd) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Actual } I_{pd} text{ vs. } h_L[j] circledast E_{pd}[j]$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . Comparing $R_{pd}$ from the model to $R_{pd}$ obtained by convolving $I_{pd}$ . We can use the actual $R_{pd}$ from the SEIR model above. . Now obtain $h_I[j] circledast I_{pd}[j]$: . #collapse_hide Rpd=signal.fftconvolve(h_I, results_stoch0[5], mode=&#39;full&#39;) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Rpd_Actual&#39;, x=np.arange(len(results_stoch0[6])), y=results_stoch0[6]), go.Scatter(name=&#39;Rpd_convolved&#39;, x=np.arange(len(Rpd)), y=Rpd) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Actual } R_{pd} text{ vs. } h_I[j] circledast I_{pd}[j]$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . De-convolution . We get very close matches when convlving, can we obtain similar results by deconvolving? . Problems with de-convolution . Illl-posed . Unfortunately de-convolution is not as straightforward as the convolution. . A quick recap of the math, in the frequency domain we have (Fourier or z transform) $$F { f  g } = F { f } ~ F { g }$$ And so if we have: $$y= f  g$$ Then: $$F { y } = F { f } ~ F { g }$$ And: $$g = F^{-1} left( frac{F {y }}{F {f }} right) $$ . But this last equation is often ill-posed. . Noise in data . We can see above that daily new data varies a lot from day to day due to the stochasticity of the model. This results in high-frequency decomposition in the frequency domain. . Initial conditions and boundary-value problems . Boundary values refer to the values on day 0 and the last day we have data for. Due to the way data is reported on JHU, we someimes have crayz values here, most commonly a value of 0 for the latest day if the data isn0t reported on time for example. These gaps can create issues for deconvolution. . In the next blog post I go around this by simplz averaging the last 3 days instead of using only the last day. . Solutions to these problems . Iterative deconvoluton . Multiple iterative deconvolution algorithms exist: Lucy_Richardson, Gold, or Van Cittert among others. . LR has already been used to get daily incidence of the Spanish flu based on death records in Philadelphia at the time for example 1. . An adaptation of those above is used here with the basics below (see code for details): . Let&#39;s use the equation for $I_{pd}$ as an example: $$I_{pd}[j] = sum_{n_L=0}^{M_L-1}h_L[n_L]~E_{pd}[j-n_L]= h_L[j] circledast E_{pd}[j]$$ . The idea is that after an initial guess for $E_{pd}$ we can iteraively find a better one. . With the $n^{th}$ guess for $E_{pd}$ written as $E_{pd, n}$ we have: $$I_{pd}[j]= h_L[j] circledast E_{pd, n}[j]$$ $$ leftrightarrow 0 = I_{pd}[j] - h_L[j] circledast E_{pd, n}[j]$$ $$ leftrightarrow E_{pd,n+1} = E_{pd,n} + I_{pd}[j] - h_L[j] circledast E_{pd, n}[j]$$ . Where $O[j] = I_{pd}[j] - h_L[j] circledast E_{pd, n}[j]$ is the error term. . Hence we want to minimze O[j]. . Initial guess . We know our $h_L$ and $h_I$ are in part simply delay functions, so an initial first guess is to simply use the same signal delayed by the time difference caused by the impulse response. . Low-pass filter . To go around those high-frequency notes in the data, we can siply use a lowpass filter implemented as a butterworth filter below: . The basic effect is that it smoothes the daily values as we have when convolving from the previous state daily values. . #collapse_show def lowpass(x, fc=0.05): fs = 1 # Sampling frequency t = np.arange(len(x)) #select number of days done in SEIR model signala = x #fc = 0.05 # Cut-off frequency of the filter w = fc / (fs / 2) # Normalize the frequency b, a = signal.butter(5, w, &#39;low&#39;) return signal.filtfilt(b, a, signala) . . Deconvolution in practice . Coding the iterative deconvolution . #collapse_show # Let&#39;s define an iteration function: def iter_deconv(alpha, impulse_response, input_signal, delay, comparator): conv=signal.fftconvolve(impulse_response, input_signal, mode=&#39;full&#39;) correction=np.roll(comparator-conv[:len(comparator)], delay) input_signal=np.floor(lowpass(alpha*correction+input_signal)) input_signal[input_signal&lt;0]=0 return input_signal # Define a function to return MSE between two signals as a measure of goodness of fit def msecalc(A, B): return ((A - B)**2).mean(axis=0) . . Comparing $E_{pd}$ from the model to $E_{pd}$ obtained by de-convolving $I_{pd}$ . #collapse_show #regularization parameter alpha=2 # Setup up the resultant Ipd we want to compare our guess with Ipd=np.floor(lowpass(results_stoch0[5])) Ipd[Ipd&lt;0]=0 # Find delay caused by h_L delay=Ipd.argmax()-signal.fftconvolve(Ipd, h_L, mode=&#39;full&#39;).argmax() # We want initial guess to simply be the result of the convolution delayed initial_guess=np.roll(Ipd,delay) Enext = initial_guess # AN array to record MSE between result we want and our iterated guess mse=np.array([]) mse=np.append(mse, 100000) mse=np.append(mse, msecalc(Ipd, signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(Ipd)])) itercount=0 while mse[-1] &lt; mse[-2]: itercount=itercount+1 Enext=iter_deconv(alpha, h_L, Enext, delay, Ipd) mse=np.append(mse, msecalc(Ipd, signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(Ipd)])) print(&quot;Iteration #&quot; + str(itercount) +&quot;: MSE= &quot;+str(mse[itercount])) print(&quot;Iteration #&quot; + str(itercount+1) +&quot;: MSE= &quot;+str(mse[-1])+&quot; so we use the result of the previous iteration.&quot;) . . Iteration #1: MSE= 128.253113864647 Iteration #2: MSE= 29.12630153955291 Iteration #3: MSE= 20.66117839168287 Iteration #4: MSE= 14.827667889972124 Iteration #5: MSE= 11.903142433438639 Iteration #6: MSE= 9.752160210380993 Iteration #7: MSE= 7.427715483375404 Iteration #8: MSE= 6.485147598975144 Iteration #9: MSE= 4.774163310379314 Iteration #10: MSE= 4.219980970725028 Iteration #11: MSE= 3.2288913917683457 Iteration #12: MSE= 2.9803815164048726 Iteration #13: MSE= 2.384262857175685 Iteration #14: MSE= 2.239378745592882 Iteration #15: MSE= 1.8410465823606756 Iteration #16: MSE= 1.858781105754766 so we use the result of the previous iteration. . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;E_pd&#39;, x=np.arange(150), y=results_stoch0[4]), go.Scatter(name=&#39;Epd=lowpass(E_pd)&#39;, x=np.arange(150), y=lowpass(results_stoch0[4])), go.Scatter(name=&#39;Epd=deconv(I_pd)&#39;, x=np.arange(150), y=Enext), go.Scatter(name=&#39;I_pd&#39;, x=np.arange(150), y=results_stoch0[5]), go.Scatter(name=&#39;Ipd=lowpass(I_pd)&#39;, x=np.arange(150), y=lowpass(results_stoch0[5])), go.Scatter(name=&#39;Ipd=conv(E_pd)&#39;, x=np.arange(150), y=signal.fftconvolve(h_L, lowpass(results_stoch0[4]), mode=&#39;full&#39;)), go.Scatter(name=&#39;Ipd=conv(deconv(Ipd))&#39;, x=np.arange(150), y=signal.fftconvolve(h_L, Enext, mode=&#39;full&#39;)[:len(Ipd)]) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Actual } E_{pd} text{ vs. deconvolution of } I_{pd}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . The iterative deconvolution seems to work nicely and we get close results. . Comparing $I_{pd}$ from the model to $I_{pd}$ obtained by de-convolving $R_{pd}$ . Let&#39;s try the same thing but using $R_{pd}$ to get $I_{pd}$: . #collapse_hide #regularization parameter alpha=2 # Setup up the resultant Ipd we want to compare our guess with Rpd=np.floor(lowpass(results_stoch0[6])) Rpd[Rpd&lt;0]=0 # Find delay caused by h_I delay=Rpd.argmax()-signal.fftconvolve(Rpd, h_I, mode=&#39;full&#39;).argmax() # We want initial guess to simply be the result of the convolution delayed initial_guess=np.roll(Rpd,delay) Inext = initial_guess # AN array to record MSE between result we want and our iterated guess mse=np.array([]) mse=np.append(mse, 100000) mse=np.append(mse, msecalc(Rpd, signal.fftconvolve(h_I, Inext, mode=&#39;full&#39;)[:len(Rpd)])) itercount=0 while mse[-1] &lt; mse[-2]: itercount=itercount+1 Inext=iter_deconv(alpha, h_I, Inext, delay, Rpd) mse=np.append(mse, msecalc(Rpd, signal.fftconvolve(h_I, Inext, mode=&#39;full&#39;)[:len(Rpd)])) print(&quot;Iteration #&quot; + str(itercount) +&quot;: MSE= &quot;+str(mse[itercount])) print(&quot;Iteration #&quot; + str(itercount+1) +&quot;: MSE= &quot;+str(mse[-1])+&quot; so we use the result of the previous iteration.&quot;) . . Iteration #1: MSE= 166.72190714942613 Iteration #2: MSE= 25.569889450476406 Iteration #3: MSE= 13.843314263045501 Iteration #4: MSE= 8.628213904576954 Iteration #5: MSE= 8.000003180274252 Iteration #6: MSE= 5.92374081714056 Iteration #7: MSE= 5.79711361370541 Iteration #8: MSE= 4.215547918598343 Iteration #9: MSE= 4.298363884056574 so we use the result of the previous iteration. . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Ipd&#39;, x=np.arange(150), y=results_stoch0[5]), go.Scatter(name=&#39;Ipd=lowpass(Ipd)&#39;, x=np.arange(150), y=lowpass(results_stoch0[5])), go.Scatter(name=&#39;Ipd=deconv(Rpd)&#39;, x=np.arange(150), y=Inext), go.Scatter(name=&#39;Rpd&#39;, x=np.arange(150), y=results_stoch0[6]), go.Scatter(name=&#39;Rpd=conv(Inext)&#39;, x=np.arange(150), y=signal.fftconvolve(h_I, Inext, mode=&#39;full&#39;)), go.Scatter(name=&#39;Rpd=lowpass(Rpd)&#39;, x=np.arange(150), y=lowpass(results_stoch0[6])), go.Scatter(name=&#39;Rpd=conv(deconv(Rpd))&#39;, x=np.arange(150), y=signal.fftconvolve(h_I, Inext, mode=&#39;full&#39;)[:len(Rpd)]) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Actual } I_{pd} text{ vs. deconvolution of } R_{pd}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . Again our algorithm works nicely. . Discussion . There are a lot of papers and research done in the field of deconvolution in general but I have found only one using it in epidemic models (link above). . The theory is skipped here but it is required if we want to use more advanced techniques. I may write another blog post with more details. . After doing this you might be left wondering, well what&#39;s the point ? . A few things: . If we have daily data of infectious $I_{pd}$ we can then use the deconvolution technique to find an approximation for $E_{pd}$ | If we have a close estimate for $E_{pd}$ we can in turn estimate the value of $ beta$ or $R$ | We can verify the robusteness of $h_L$ and $h_I$ against real world data and how these vary geographically in a pandemic | . These points will be studied a bit more in the next blog post. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/modeling/seir/epidemiology/stochastic/covid-19/2020/04/02/convolution.html",
            "relUrl": "/modeling/seir/epidemiology/stochastic/covid-19/2020/04/02/convolution.html",
            "date": "  Apr 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Epidemic modeling - Part 6",
            "content": ". . Motivation for write-up . This is the 6th part of a multi-part series blog post on modeling in epidemiology. . The COVID-19 pandemic has brought a lot of attention to the study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points. . Following the first 5 parts of this blog series, we are left wondering what possible measures can be put in place to control the epidemic. . This 6th installment focuses on this and attempts to elucidate the subject. . Why do we need control measures during an epidemic? . In the previous sections we have seen that without control measures in place i.e. when injecting an exposed person into the simulation, depending on the values of $ beta$ and $ gamma$, the virus will spread until it has infected everyone. . There are many reasons why this is bad with a virus as virulent as SARS-CoV-2: . Even with a low Case Fatality Rate (CFR), the total death toll will be unacceptable | The strain on sanitary resources of peak sick people will lead to an increase in all-cause deaths | The strain on sanitary resources will lead to increased long-term morbidity | Many economic implications | Many many others... | . Control measures: the basics . While the previous sections might have left us wondering what we can possibly do to control the pread of the epidemic, the evidence left from those studies in fact leave us with a lot of clues as to where to begin. . Only one way to stop an epidemic . We have seen there is only one way to stop an epidemic, and that is by having: $$R leq 1$$ $$ leftrightarrow R_0~s(t) leq 1$$ $$ leftrightarrow frac{ beta}{ gamma}~s(t) leq 1$$ . Two methods to save lives . From the equation above, we see an epidemic will either continue until herd immunity is reached: $$s(t) leq frac{1}{R_0}$$ $$ leftrightarrow Immune(t) geq 1- frac{1}{R_0}$$ . Or until measures are in place so that: $$R_0 leq frac{1}{s(t)}$$ . In the worst case scenario where $s(t) = 1$ (completely susceptible population), we need: $$R_0 leq 1$$ $$ leftrightarrow beta leq gamma$$ . From this results two main ideas of control: . Reaching herd immunity all while controlling peak infectious individuals to within hospital and sanitary resources so as to limit morbidity and mortality . | Reducing $ beta$ to smaller than $ gamma$ to stop the epidemic even before herd immunity . | While the difference between the two is noted here, in practice they are effectively the same. . Option 1 is the same as option 2, with the difference that in 1 we don&#39;t quite have $$R leq 1$$ and so the difference between the two results from different levels of imlementation. . Control measures in practice . We have seen three things influence the total number of people that end up infected with the virus: . Proportion of Susceptible in population $s(t)$ | Value $ beta$ | Value of $ gamma$ ($T_{Infectious}$) | . We have also seen that the peak of infectious individuals could be affected by: . Value of $ sigma$ ($T_{Latent}$) | . And indeed, reducing the peak of infectious at any point comes down to either: . Reducing the number of S: vaccination | prophylactic treatment when potentially exposed | . | Reducing $ beta = r * rho$ by: reducing $r$ i.e. reducing the average number of contacts a person has per day: lockdown measures | work from home | closing places where people gather (restaurants, bars, places of worship, etc) | . | reducing $ rho$ - reducing the pobability of transmitting infection from an infectious to a susceptible via: physical distancing | hygiene measures | wearing personal protective equipment (PPE i.e. masks, gloves, etc) | . | | Reducing $ gamma$ by: Isolation of the sick (mass testing of symptomatic and immediate isolation of positive cases) | Contact-tracing: tracing and quarantining all contacts from infectious people as to quarantine potential exposed before they become infectious | Chemotherapy: treatment to shorten duration of sickness and infectious period | . | Increasing $ sigma$ by: prophylactic treatment when potentially exposed | . | . All these are the same requirements to reduce $R leq 1$. . Let&#39;s try to quantify the impact of each measure below. . #collapse_hide !pip install plotly==4.8.2 import pandas as pd import numpy as np import plotly.graph_objects as go import plotly.express as px from scipy.stats import expon from scipy.stats import gamma from scipy.stats import weibull_min from numpy.random import default_rng rng = default_rng() . . Collecting plotly==4.8.2 Downloading https://files.pythonhosted.org/packages/27/99/9794bcd22fae2e12b689759d53fe26939a4d11b8b44b0b7056e035c64529/plotly-4.8.2-py2.py3-none-any.whl (11.5MB) || 11.5MB 3.3MB/s Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.8.2) (1.12.0) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.8.2) (1.3.3) Installing collected packages: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-4.8.2 . #collapse_hide # Need this new function for model below: def make_df(p,num_E, num_I, num_R): df = pd.DataFrame(np.full((p,1), &#39;S&#39;).T[0], columns=[&#39;State&#39;]) df[&#39;Day&#39;] = 0 tochange=df.loc[rng.choice(p, size=num_E+num_I+num_R, replace=False),&#39;State&#39;].index df.loc[tochange[0:num_E],&#39;State&#39;] = &#39;E&#39; df.loc[tochange[num_E:num_I+num_E],&#39;State&#39;] = &#39;I&#39; df.loc[tochange[num_E+num_I:num_E+num_I+num_R],&#39;State&#39;] = &#39;R&#39; return df . . #collapse_hide # regular Stoachastic SEIR model below: def seir_model_stoch_ctrl(beta, p, num_E, num_I, num_R, days, isolation, iso_n, contact_tracing, lockdown): # Initialize population dataframe with data given by user df = make_df(p,num_E, num_I, num_R) # This variable is used to track daily value of beta xxbeta=np.array([],dtype=float) # Initialize the arrays to return # Below are numbers of S, E, I, R total S=np.array([],dtype=int) E=np.array([],dtype=int) I=np.array([],dtype=int) R=np.array([],dtype=int) # Below are the daily additions in S, E, I, R Spd=np.array([],dtype=int) Epd=np.array([],dtype=int) Ipd=np.array([],dtype=int) Rpd=np.array([],dtype=int) b=beta beta2=b/10 beta3=b/10 lockdown_date=0 # Stochastic model so use random values to decide on progression rand = np.random.random(size=(p,days)) # Depending if you want exponential or gamma distribution for sigma EtoI = gamma.rvs(1.8,loc=0.9,scale=(5.2-1.8)/0.9,size=p) # Depending if you want exponential or gamma distribution for gamma and if you have isolation or not # Uses distributiosn found on blog part 3 if isolation is True: ItoR = iso_n*np.ones(p) else: ItoR = weibull_min.rvs(2.3, loc=2, scale=20.11, size=p) # Iterate over every day the simulation is run for j in range(0,days): # Record daily beta values xxbeta=np.append(xxbeta, b[j]) # First we get the index of the individuals that will change state today: # Random number tells you which &#39;S&#39; have been exposed on this day StoE_index = df.loc[(df.State == &#39;S&#39;) &amp; (rand[:,j] &lt; b[j]*len(np.where(df.State==&#39;I&#39;)[0])/p)].index # For each row, if a person has been a certain number of days in E, they will go to I # This follows EtoI variable which is either exponential or gamma distributed according to above EtoI_index = df.loc[(df.State == &#39;E&#39;) &amp; (j-df.Day &gt;= EtoI)].index # Similaraly as above # For each row, if a person has been a certain number of days in I, they will go to R # This follows EtoI variable which is either exponential or gamma distributed according to above ItoR_index = df.loc[(df.State == &#39;I&#39;) &amp; (j-df.Day &gt;= ItoR)].index # Use indexes collected above to populate per day values Epd = np.append(Epd,len(StoE_index)) Ipd = np.append(Ipd,len(EtoI_index)) Rpd = np.append(Rpd,len(ItoR_index)) # Append the S, E, I, and R arrays S=np.append(S,len(np.where(df.State==&#39;S&#39;)[0])) E=np.append(E,len(np.where(df.State==&#39;E&#39;)[0])) I=np.append(I,len(np.where(df.State==&#39;I&#39;)[0])) R=np.append(R,len(np.where(df.State==&#39;R&#39;)[0])) # Now we use the indexes collected above randomly to change the actual population dataframe to the new states df.iloc[ItoR_index] = [&#39;R&#39;, j] df.iloc[EtoI_index] = [&#39;I&#39;, j] df.iloc[StoE_index] = [&#39;E&#39;, j] # Code below for control measures to reduce beta values if lockdown is True: if ((I[-1] &gt; 100) &amp; (Ipd[-1] &gt; 39)): if lockdown_date == 0: lockdown_date = j+1 b = beta2 elif ((I[-1] &gt; 100) &amp; (Ipd[-1] &lt; 40)): b = beta3 Epd[0]+=num_E Ipd[0]+=num_I Rpd[0]+=num_R return S,E,I,R, Epd, Ipd, Rpd, xxbeta, lockdown_date . . Reducing the proportion of Susceptible in the population . As we have seen, reducing the proportion of susceptible in the population helps reduce the impact of the epidemic. . In the first blog post of the series, we derived the threshold for herd immunity as being $1- frac{1}{R_0}$. . In our simulations we have $R_0 = frac{ beta}{ gamma} = frac{0.5}{ frac{1}{20.62}} = 10.31$ . And so, the herd immunity threshold in our simulation should be: $$HIT = frac{9.31}{10.31}~100 % = 90.3 %$$ . Let&#39;s run the model with the initial condition that 92% are in the R state already. . #collapse_hide # Define parameters for stochastic model days = 300 p = 10000 num_E = 1 num_I = 0 # Run 2 simulations, one above HIT, and one below: num_R1 = 8000 num_R2 = 9200 beta_stoch = 0.5*np.ones(days) # Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigmalation, iso_n, contact_tracing, lockdown results_stoch0 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R1, days, isolation=False, iso_n=iso_n, contact_tracing=False, lockdown=False) results_stoch1 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R2, days, isolation=False, iso_n=iso_n1, contact_tracing=False, lockdown=False) . . NameError Traceback (most recent call last) &lt;ipython-input-4-b1cb4735df63&gt; in &lt;module&gt;() 12 13 # Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigmalation, iso_n, contact_tracing, lockdown &gt; 14 results_stoch0 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R1, days, isolation=False, iso_n=iso_n, contact_tracing=False, lockdown=False) 15 results_stoch1 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R2, days, isolation=False, iso_n=iso_n1, contact_tracing=False, lockdown=False) NameError: name &#39;iso_n&#39; is not defined . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;I_below_HIT&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[2]/p), go.Scatter(name=&#39;I_above_HIT&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Proportion of population&#39;, title={ &#39;text&#39;:r&#39;$ text{Effect of herd immunity on SEIR model}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . Reducing $ beta$ . See blog post 2 for effect of $ beta$ on the SEIR model. . Reducing $ gamma$ . Isolating positive tests . Say you isolate positive tests and are able to test everyone. . What if tests are positive $n$ days after people are infectious ? . You can isolate people after $n$ days and this effectively reduces $T_{Infectious}$ to $n$ days. . Let&#39;s plot the total infectious individuals with: . n = 10 days | n = 7 days | n = 5 days | n = 2 days | . #collapse_hide # Define parameters for stochastic model days = 200 p = 10000 num_E = 1 num_I = 0 num_R = 0 beta_stoch = 0.5*np.ones(days) # Isolate after iso_n days iso_n1 = 10 # Test and isolate all infectious after 10 days iso_n2 = 7 iso_n3 = 5 iso_n4 = 2 # Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigmalation, iso_n, contact_tracing, lockdown results_stoch0 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R, days, isolation=False, iso_n=iso_n, contact_tracing=False, lockdown=False) results_stoch1 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R, days, isolation=True, iso_n=iso_n1, contact_tracing=False, lockdown=False) results_stoch2 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R, days, isolation=True, iso_n=iso_n2, contact_tracing=False, lockdown=False) results_stoch3 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R, days, isolation=True, iso_n=iso_n3, contact_tracing=False, lockdown=False) results_stoch4 = seir_model_stoch_ctrl(beta_stoch, p, num_E, num_I, num_R, days, isolation=True, iso_n=iso_n4, contact_tracing=False, lockdown=False) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;I_no_isolation&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[2]/p), go.Scatter(name=&#39;I_iso10&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p), go.Scatter(name=&#39;I_iso7&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p), go.Scatter(name=&#39;I_iso5&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p), go.Scatter(name=&#39;I_iso2&#39;, x=np.arange(len(results_stoch4[0])), y=results_stoch4[2]/p), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Proportion of population&#39;, title={ &#39;text&#39;:r&#39;$ text{Effect of early testing and isolation on SEIR model}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . Clearly, early testing and isolation of cases reduces transmission and this the peak of infectious individuals. The faster we do this the better. . If we catch infectious individuals early enough, we can completely control the epidemic (see I_iso_2 above). . Discussion . This was a quick introduction to the basics of control measures and their impact on the SEIR model. . While there are many different possible approaches, the accumulation of the different solutions is the way to achieve control over an epidemic. . Maybe I will make an interactive app to see the effect of each effect. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/modeling/seir/epidemiology/stochastic/covid-19/real-world/2020/04/02/control-measures.html",
            "relUrl": "/modeling/seir/epidemiology/stochastic/covid-19/real-world/2020/04/02/control-measures.html",
            "date": "  Apr 2, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Epidemic modeling - Part 5",
            "content": ". Motivation for write-up . This is the 5th part of a multi-part series blog post on modeling in epidemiology. . The COVID-19 pandemic has brought a lot of attention to the study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points. . The 1st part of the blog series showed an epidemic could occur when $R_0 &gt; 1$ and that it was fully characerized by the average $ beta$ and the average $T_{Infectious}$. . We have also seen that the higher $R_0$, the faster and the higher the peak infectious will be. . The latest blog post however showed the importance of the distribution of $T_{Infectious}$ to simulate the SEIR model and how it impacted the spread of the disease and the peak of infectious individuals. Even with lower $R_0$ values, the infectious peak were higher when using Gamma or Weibull distributions for $T_{Infectious}$. . This 5th installment examines this discrepancy further. . #collapse_hide !pip install plotly==4.6.0 import pandas as pd import numpy as np import math import plotly.graph_objects as go import plotly.express as px from scipy.stats import expon from scipy.stats import gamma from scipy.stats import weibull_min from numpy.random import default_rng rng = default_rng() # Let&#39;s build a numerical solution def seir_model(init, parms, days): S_0, E_0, I_0, R_0 = init Epd, Ipd, Rpd = [0], [0], [0] S, E, I, R = [S_0], [E_0], [I_0], [R_0] dt=0.1 t = np.linspace(0,days,int(days/dt)) sigma, beta, gam = parms for _ in t[1:]: next_S = S[-1] - beta*S[-1]*I[-1]*dt Epd.append(beta*S[-1]*I[-1]*dt) next_E = E[-1] + (beta*S[-1]*I[-1] - sigma*E[-1])*dt Ipd.append(sigma*E[-1]*dt) next_I = I[-1] + (sigma*E[-1] - gam*I[-1])*dt Rpd.append(gam*I[-1]*dt) next_R = R[-1] + (gam*I[-1])*dt S.append(next_S) E.append(next_E) I.append(next_I) R.append(next_R) return np.stack([S, E, I, R, Epd, Ipd, Rpd]).T . . Requirement already satisfied: plotly==4.6.0 in /usr/local/lib/python3.6/dist-packages (4.6.0) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.3.3) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.12.0) . #collapse_hide # Need this new function for model below: def make_df(p,num_E, num_I, num_R): df = pd.DataFrame(np.full((p,1), &#39;S&#39;).T[0], columns=[&#39;State&#39;]) df[&#39;Day&#39;] = 0 tochange=df.loc[rng.choice(p, size=num_E+num_I+num_R, replace=False),&#39;State&#39;].index df.loc[tochange[0:num_E],&#39;State&#39;] = &#39;E&#39; df.loc[tochange[num_E:num_I+num_E],&#39;State&#39;] = &#39;I&#39; df.loc[tochange[num_E+num_I:num_E+num_I+num_R],&#39;State&#39;] = &#39;R&#39; return df . . #collapse_hide def seir_model_stoch(beta, p, num_E, num_I, num_R, days, T_Latent, T_Infectious): # Initialize population dataframe with data given by user df = make_df(p,num_E, num_I, num_R) # This variable is used to track daily value of beta if it varies over time xxbeta=np.array([],dtype=float) # Initialize the arrays to return # Below are numbers of S, E, I, R total S=np.array([],dtype=int) E=np.array([],dtype=int) I=np.array([],dtype=int) R=np.array([],dtype=int) # Below are the daily additions in S, E, I, R Spd=np.array([],dtype=int) Epd=np.array([],dtype=int) Ipd=np.array([],dtype=int) Rpd=np.array([],dtype=int) b=beta # Stochastic model so use random values to decide on progression rand = np.random.random(size=(p,days)) # Depending if you want exponential or gamma distribution for T_Latent if T_Latent == &#39;expon&#39;: EtoI = expon.rvs(loc=0,scale=5.2,size=p) else: EtoI = gamma.rvs(1.8,loc=0.9,scale=(5.2-1.8)/0.9,size=p) # Depending if you want exponential, gamma, or Weibull distribution for T_Infectious # Uses distributions found on blog part 3 if T_Infectious == &#39;expon&#39;: ItoR = expon.rvs(loc=0,scale=28.85,size=p) elif T_Infectious == &#39;gamma&#39;: ItoR = gamma.rvs(4,loc=3,scale=4.25,size=p) else: ItoR = weibull_min.rvs(2.3, loc=2, scale=20.11, size=p) # Iterate over every day the simulation is run for j in range(0,days-1): # Record daily beta values xxbeta=np.append(beta, b) # First we get the index of the individuals that will change state today: # Random number tells you which &#39;S&#39; have been exposed on this day StoE_index = df.loc[(df.State == &#39;S&#39;) &amp; (rand[:,j] &lt; b[j]*len(np.where(df.State==&#39;I&#39;)[0])/p)].index # For each row, if a person has been a certain number of days in E, they will go to I # This follows EtoI variable which is either exponential or gamma distributed according to above EtoI_index = df.loc[(df.State == &#39;E&#39;) &amp; (j-df.Day &gt;= EtoI)].index # Similaraly as above # For each row, if a person has been a certain number of days in I, they will go to R # This follows EtoI variable which is either exponential or gamma distributed according to above ItoR_index = df.loc[(df.State == &#39;I&#39;) &amp; (j-df.Day &gt;= ItoR)].index # Use indexes collected above to populate per day values Epd = np.append(Epd,len(StoE_index)) Ipd = np.append(Ipd,len(EtoI_index)) Rpd = np.append(Rpd,len(ItoR_index)) # Now we use the indexes collected above randomly to change the actual population dataframe to the new states df.loc[ItoR_index, &#39;State&#39;] = &#39;R&#39; df.loc[EtoI_index, &#39;State&#39;] = &#39;I&#39; df.loc[StoE_index, &#39;State&#39;] = &#39;E&#39; df.loc[ItoR_index, &#39;Day&#39;] = j df.loc[EtoI_index, &#39;Day&#39;] = j df.loc[StoE_index, &#39;Day&#39;] = j df.loc[ItoR_index, &#39;DayR&#39;] = j df.loc[EtoI_index, &#39;DayI&#39;] = j df.loc[StoE_index, &#39;DayE&#39;] = j # Append the S, E, I, and R arrays S=np.append(S,len(np.where(df.State==&#39;S&#39;)[0])) E=np.append(E,len(np.where(df.State==&#39;E&#39;)[0])) I=np.append(I,len(np.where(df.State==&#39;I&#39;)[0])) R=np.append(R,len(np.where(df.State==&#39;R&#39;)[0])) # Code below for control measures to reduce beta values # if ((I[-1] &gt; 1000) &amp; (Ipd[-1] &gt; 399)): # b = beta2 # elif ((I[-1] &gt; 1000) &amp; (Ipd[-1] &lt; 400)): # b = beta3 Epd[0]+=num_E Ipd[0]+=num_I Rpd[0]+=num_R return S,E,I,R, Epd, Ipd, Rpd, xxbeta, df . . Peak of infectious individuals . Let&#39;s first try to characterize when the peak of infectious indiviuals occurs in the SEIR model. . The peak of infectious individuals occurs when the number of individuals that recover per day ($R_{pd}$) becomes greater than the number of new infectious individuals per day ($I_{pd}$), i.e. when: $$R_{pd} geq I_{pd}$$ . Convolution and daily numbers . Mathematical derivation of $I_{pd}$: . In the stochastic model, the number of new infectious individuals per day is the sum of the daily new exposures per day of the previous days multiplied by the probability that they become infectious after so many days. . We write: $$I_{pd}[j] = sum_{n_L=0}^{M_L-1}h_L[n_L]~E_{pd}[j-n_L]$$ . Mathematically, this means $I_{pd}[j]$ is the result of convolution of $E_{pd}$ and an impulse response $h_L[n]$ where $h_L[n]$ describes the distribution of $T_{Latent}$, and we can write: $$I_{pd}[j] = h_L[j] circledast E_{pd}[j]$$ . From this derivation, we can see the $I_{pd}$ depends on the distribution of $T_{Latent}$ and $E_{pd}$ (the number of new exposures per day) of the previous days. . When initial conditions are the same, this means $T_{Infectious}$ has no impact on $I_{pd}$ and so the rate of new infectious individuals will look the same between the two models (exponential or gamma distributed $T_{Infectious}$). . Mathematical derivation of $R_{pd}$: . We can similarly describe $R_{pd}[j]$ as the result of convolution of $I_{pd}$ and an impulse response $h_I[n]$ where $h_I[n]$ describes the distribution of $T_{Infectious}$. . In other words: $$R_{pd}[j] = sum_{n_I=0}^{M_I-1}h_I[n_I]~I_{pd}[j-n_I] = h_I[j] circledast I_{pd}[j]$$ . When does the peak occur? . It occurs on day j where the threshold below is reached: $$R_{pd}[j] = I_{pd}[j]$$ $$ leftrightarrow sum_{n_I=0}^{M_I-1}h_I[n_I]~I_{pd}[j-n_I] = sum_{n_L=0}^{M_L-1}h_L[n_L]~E_{pd}[j-n_L]$$ . While this is complicated to solve analytically, we can look at the distribution of $T_{Infectious}$ and get some clues as to what might happen with different distributions. . $I_{pd}$ and $R_{pd}$ for $T_{Infectious} sim Exp( lambda)$ vs. $T_{Infectious} sim Weibull( lambda, k, gamma)$ . $T_{Infectious} sim Exp(28.85)$ vs. $T_{Infectious} sim Weibull(2.3, 20.11, 2)$ . Let&#39;s first have another look at the difference between these two distributions: . #collapse_hide locw=2 wk = 2.3 wl = (20-locw)/(math.log(2)**(1/wk)) loce=0 scalee=28.85-loce p=10000 df = pd.DataFrame({ &#39;Exponential&#39;: expon.rvs(loc=loce, scale=scalee,size=p), &#39;Weibull&#39;: weibull_min.rvs(wk, loc=locw, scale=wl,size=p) }) fig = px.histogram(df.stack().reset_index().rename(columns={&quot;level_1&quot;: &quot;Distribution&quot;}), x=0, color=&quot;Distribution&quot;, marginal=&#39;box&#39;) fig.update_layout( title={ &#39;text&#39;:&#39;Exponential vs. Weibull distributions&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; }, barmode=&#39;overlay&#39;, xaxis_title=&#39;Days&#39;, yaxis_title=&#39;Count&#39;, legend=dict( x=1, y=0, traceorder=&quot;normal&quot;, ) ) fig.show() . . . . Impact on $I_{pd}$ $R_{pd}$: . Let&#39;s see how the two distributions of $T_{Infectious}$ result in different curbs for $I_{pd}$ and $R_{pd}$. . #collapse_hide # Define parameters for stochastic model days = 200 p = 10000 num_E = 1 num_I = 0 num_R = 0 beta_stoch = 0.5*np.ones(days) # Run 2 stochastic simulations, 1 with exponential gamma, 1 with weibull gamma results_stoch0 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;expon&#39;) results_stoch1 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, 1) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Ipd_Exp&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[5], line={&#39;dash&#39;:&#39;dashdot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch4&quot;), go.Scatter(name=&#39;Rpd_Exp&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[6], line={&#39;dash&#39;:&#39;dashdot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch4&quot;), go.Scatter(name=&#39;Ipd_Weibull&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[5], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;Rpd_Weibull&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[6], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch3&quot;) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Count&#39;, title={ &#39;text&#39;:r&#39;$ text{Effect of Exponential vs. Weibull distributed } T_{Infectious} text{ on } I_{pd} text{ and } R_{pd}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . Analysis: . As discussed above, we can see here $I_{pd}$ are very similar between the two models. . However, there are two key takeaways from these graphs: . Rise of total infectious individuals: | The Exponential distribution leads to more recoveries early on as the $R_{pd}$ for the Exp distribution leads the $R_{pd}$ of the Weibull distribution by ~6 days in the earl recovery period. . Intuitively, we can thus understand that total infectious people will increase slower with the Eponential distribution than the Weibull distribution (since more people recover faster in the Exp model), and so the peak of infectious individuals will be earlier with the Weibull distribution. . Recoveries: | While the peak appears faster with a Weibull distribution, we also see that after the initial delay, the Weibull distribution leads to larger number of recoveries and so the total number of infectious individuals will tend to 0 much quicker than with the Exp distribution. . Discussion . While $R_0$ seemed to be a good measure of the spread of disease and a good predictor of the peak of infectious individuals in a population, we find here that the actual distribution of $T_{Infectious}$ plays a crucial role in detemining the dynamics of the SEIR model. . That is to say that while estimating $R_0$ is important in times of epidemics or pandemics, finding the actual distributions of $T_{Latent}$ and $T_{Infetious}$ are equally important in predicting the impact of the disease. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/modeling/seir/epidemiology/stochastic/covid-19/reproduction%20number/2020/04/01/diagnosing-R0-and-peak-infectious.html",
            "relUrl": "/modeling/seir/epidemiology/stochastic/covid-19/reproduction%20number/2020/04/01/diagnosing-R0-and-peak-infectious.html",
            "date": "  Apr 1, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Epidemic modeling - Part 4",
            "content": ". Motivation for write-up . This is the 4th part of a multi-part series blog post on modeling in epidemiology. . The COVID-19 pandemic has brought a lot of attention to study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points. . After introducing the concepts of compartmentalization and disease dynamics in the first blog post, the second part looked at a deterministic numerical solution for the SEIR model discussed, and the effects of the parameters $ beta$, $ sigma$, and $ gamma$ in parts 1 and 2. . Part 3 made the argument that most models ignore individual-level disease dynamics in favor of averaging population-level $ sigma$ and $ gamma$ parameters and showed some big discrepancies between actual COVID-19 probability distributions for those parameters and those used in research. . This 4th part is where I build a numerical SEIR model that takes into account these probability distributions in order to tweak the model as close to COVID-19 data as possible. . Building a stochastic model . As opposed to the deterministic model from Part 2, this model is going to focus on individual level disease dynamics to model the disease propagation. . The basic idea of this model is to have a dataframe with the number of rows equal to the population size (each individual is a row) and two columns: . State column to describe the state of each individual (S, E, I, or R) | Day column to save the day of transition of the individual into that state | . However, the population-level rates of transmission still apply here i.e. a person goes from S &rarr; E following three points: . the number of contacts the person has per unit time (given by $r$) | the chance a given contact is with an I - infectious individual (the higher thenumber of I, the higher the chance) | the chance of an S contracting the disease from a contact with an I (given by $ rho$) | This is done stochastically. . Once a person becomes E, their progression is unique to them. This progression is calculated in advance for computational reason, but it allows to use the time ditributions we want. . #collapse_hide !pip install plotly==4.6.0 import pandas as pd import numpy as np import math import plotly.graph_objects as go import plotly.express as px from scipy.stats import expon from scipy.stats import gamma from scipy.stats import weibull_min from numpy.random import default_rng rng = default_rng() # Let&#39;s build a numerical solution def seir_model(init, parms, days): S_0, E_0, I_0, R_0 = init Epd, Ipd, Rpd = [0], [0], [0] S, E, I, R = [S_0], [E_0], [I_0], [R_0] dt=0.1 t = np.linspace(0,days,int(days/dt)) sigma, beta, gam = parms for _ in t[1:]: next_S = S[-1] - beta*S[-1]*I[-1]*dt Epd.append(beta*S[-1]*I[-1]*dt) next_E = E[-1] + (beta*S[-1]*I[-1] - sigma*E[-1])*dt Ipd.append(sigma*E[-1]*dt) next_I = I[-1] + (sigma*E[-1] - gam*I[-1])*dt Rpd.append(gam*I[-1]*dt) next_R = R[-1] + (gam*I[-1])*dt S.append(next_S) E.append(next_E) I.append(next_I) R.append(next_R) return np.stack([S, E, I, R, Epd, Ipd, Rpd]).T . . Collecting plotly==4.6.0 Downloading https://files.pythonhosted.org/packages/15/90/918bccb0ca60dc6d126d921e2c67126d75949f5da777e6b18c51fb12603d/plotly-4.6.0-py2.py3-none-any.whl (7.1MB) || 7.2MB 4.6MB/s Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.3.3) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.12.0) Installing collected packages: plotly Found existing installation: plotly 4.4.1 Uninstalling plotly-4.4.1: Successfully uninstalled plotly-4.4.1 Successfully installed plotly-4.6.0 . Creating the initial population dataframe . Below is a function to create the initial population dataframe: . $p$ is the population number | $num_E$ is the number of people exposed on day 0 | $num_I$ is the number of infectious on day 0 | $num_R$ is the number of people recovered on day 0 | . #collapse_hide # Need this new function for model below: def make_df(p,num_E, num_I, num_R): df = pd.DataFrame(np.full((p,1), &#39;S&#39;).T[0], columns=[&#39;State&#39;]) df[&#39;Day&#39;] = 0 tochange=df.loc[rng.choice(p, size=num_E+num_I+num_R, replace=False),&#39;State&#39;].index df.loc[tochange[0:num_E],&#39;State&#39;] = &#39;E&#39; df.loc[tochange[num_E:num_I+num_E],&#39;State&#39;] = &#39;I&#39; df.loc[tochange[num_E+num_I:num_E+num_I+num_R],&#39;State&#39;] = &#39;R&#39; return df . . Building the model . #collapse_hide def seir_model_stoch(beta, p, num_E, num_I, num_R, days, T_Latent, T_Infectious): # Initialize population dataframe with data given by user df = make_df(p,num_E, num_I, num_R) # This variable is used to track daily value of beta if it varies over time xxbeta=np.array([],dtype=float) # Initialize the arrays to return # Below are numbers of S, E, I, R total S=np.array([],dtype=int) E=np.array([],dtype=int) I=np.array([],dtype=int) R=np.array([],dtype=int) # Below are the daily additions in S, E, I, R Spd=np.array([],dtype=int) Epd=np.array([],dtype=int) Ipd=np.array([],dtype=int) Rpd=np.array([],dtype=int) b=beta # Stochastic model so use random values to decide on progression rand = np.random.random(size=(p,days)) # Depending if you want exponential or gamma distribution for T_Latent if T_Latent == &#39;expon&#39;: EtoI = expon.rvs(loc=0,scale=5.2,size=p) else: EtoI = gamma.rvs(1.8,loc=0.9,scale=(5.2-1.8)/0.9,size=p) # Depending if you want exponential, gamma, or Weibull distribution for T_Infectious # Uses distributions found on blog part 3 if T_Infectious == &#39;expon&#39;: ItoR = expon.rvs(loc=0,scale=28.85,size=p) elif T_Infectious == &#39;gamma&#39;: ItoR = gamma.rvs(4,loc=3,scale=4.25,size=p) else: ItoR = weibull_min.rvs(2.3, loc=2, scale=20.11, size=p) # Iterate over every day the simulation is run for j in range(0,days-1): # Record daily beta values xxbeta=np.append(beta, b) # First we get the index of the individuals that will change state today: # Random number tells you which &#39;S&#39; have been exposed on this day StoE_index = df.loc[(df.State == &#39;S&#39;) &amp; (rand[:,j] &lt; b[j]*len(np.where(df.State==&#39;I&#39;)[0])/p)].index # For each row, if a person has been a certain number of days in E, they will go to I # This follows EtoI variable which is either exponential or gamma distributed according to above EtoI_index = df.loc[(df.State == &#39;E&#39;) &amp; (j-df.Day &gt;= EtoI)].index # Similaraly as above # For each row, if a person has been a certain number of days in I, they will go to R # This follows EtoI variable which is either exponential or gamma distributed according to above ItoR_index = df.loc[(df.State == &#39;I&#39;) &amp; (j-df.Day &gt;= ItoR)].index # Use indexes collected above to populate per day values Epd = np.append(Epd,len(StoE_index)) Ipd = np.append(Ipd,len(EtoI_index)) Rpd = np.append(Rpd,len(ItoR_index)) # Now we use the indexes collected above randomly to change the actual population dataframe to the new states df.iloc[ItoR_index] = [&#39;R&#39;, j] df.iloc[EtoI_index] = [&#39;I&#39;, j] df.iloc[StoE_index] = [&#39;E&#39;, j] # Append the S, E, I, and R arrays S=np.append(S,len(np.where(df.State==&#39;S&#39;)[0])) E=np.append(E,len(np.where(df.State==&#39;E&#39;)[0])) I=np.append(I,len(np.where(df.State==&#39;I&#39;)[0])) R=np.append(R,len(np.where(df.State==&#39;R&#39;)[0])) # Code below for control measures to reduce beta values # if ((I[-1] &gt; 1000) &amp; (Ipd[-1] &gt; 399)): # b = beta2 # elif ((I[-1] &gt; 1000) &amp; (Ipd[-1] &lt; 400)): # b = beta3 Epd[0]+=num_E Ipd[0]+=num_I Rpd[0]+=num_R return S,E,I,R, Epd, Ipd, Rpd, xxbeta . . Sanity check . Let&#39;s first make sure the stochastic model above gives similar result to the deterministic model previously used in part 2 if we use an exponential distribution for $T_{Latent}$ and $T_{Infectious}$. . E &rarr; I . So let&#39;s first set all individuals to exposed on day 0 and see the progression to I with exponential and gamma distributions. . #collapse_hide # Define parameters for stochastc model days = 20 p = 10000 num_E = 10000 num_I = 0 num_R = 0 beta_stoch = 0.5*np.ones(days) # Comparing with previous deterministic model init = 0, p, 0, 0 sigma = 1/5.2 # 1/5 --&gt; 5 days on average to go from E --&gt; I beta_det = 0.5 gam = 1/28.85 # 1/11 --&gt; 11 days on average to go from I --&gt; R parms = sigma, beta_det, gam # Run deterministic simulation results_avg = seir_model(init, parms, days) # Run stochastic simulation with exponential distribution results_stoch_exp = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, &#39;expon&#39;, &#39;expon&#39;) # Run stochastic simulation with gamma distribution results_stoch_gam = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;expon&#39;) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Exponential&#39;, x=np.arange(len(results_stoch_exp[0])), y=100*(1-results_stoch_exp[1]/p), line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;red&#39;}), go.Scatter(name=&#39;Gamma&#39;, x=np.arange(len(results_stoch_gam[0])), y=100*(1-results_stoch_gam[1]/p), line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;green&#39;}), go.Scatter(name=&#39;Deterministic&#39;, x=np.linspace(0,days,days*10), y=100*(1-results_avg.T[1]/p), line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}), ]) fig.update_layout( title=&#39;Number of E moving to I over time when all population is exposed on day 0&#39;, xaxis_title=&#39;Days&#39;, yaxis_title=&#39;Percent of exposed having become infectious&#39;, legend=dict( x=1, y=1, traceorder=&quot;normal&quot;, ) ) fig.show() . . . . So we can see using the exponential distribution for $T_{Latent}$ in our stochastic model very closely resembles the deterministic model from part 2. . We can see using the gamma distribution forces the behaviour of individual-level disease progression also. . I &rarr; R . Now let&#39;s set all individuals to infectious on day 0 and see the progression to R with exponential, gamma, and Weibull distributions. . #collapse_hide # Define parameters for stochastc model days = 100 p = 10000 num_E = 0 num_I = 10000 num_R = 0 beta_stoch = 0.5*np.ones(days) # Comparing with previous average deterministic model init = 0, 0, p, 0 sigma = 1/5.2 # 1/5 --&gt; 5 days on average to go from E --&gt; I beta_det = 0.5 gam = 1/28.85 # 1/11 --&gt; 11 days on average to go from I --&gt; R parms = sigma, beta_det, gam # Run deterministic simulation results_avg = seir_model(init, parms, days) # Run stochastic simulation with exponential distribution results_stoch_exp = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;expon&#39;) # Run stochastic simulation with gamma distribution results_stoch_gam = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;gamma&#39;) # Run stochastic simulation with gamma distribution results_stoch_wei = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;weibull&#39;) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;Exponential&#39;, x=np.arange(len(results_stoch_exp[0])), y=100*(1-results_stoch_exp[2]/p), line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;red&#39;}), go.Scatter(name=&#39;Gamma&#39;, x=np.arange(len(results_stoch_gam[0])), y=100*(1-results_stoch_gam[2]/p), line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;green&#39;}), go.Scatter(name=&#39;Weibull&#39;, x=np.arange(len(results_stoch_wei[0])), y=100*(1-results_stoch_wei[2]/p), line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;orange&#39;}), go.Scatter(name=&#39;Deterministic&#39;, x=np.linspace(0,days,days*10), y=100*(1-results_avg.T[2]/p), line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}), ]) fig.update_layout( title=&#39;Number of I moving to R over time when all population is infectious on day 0&#39;, xaxis_title=&#39;Days&#39;, yaxis_title=&#39;Percent of infectious having become recovered&#39;, legend=dict( x=1, y=1, traceorder=&quot;normal&quot;, ) ) fig.show() . . . . So we can see using the exponential distribution for $ gamma$ in our stochastic model very closely resembles the deterministic model from part 2. . We can see using the gamma or Weibull distributions forces the behaviour of individual-level disease progression also and results in a vastly different picture for progression from I &rarr; R. . Comparing deterministic with stochastic SEIR models . Now that we know our model works, let&#39;s quickly see the effect of stochasticity on the model. . We use the deterministic model from blog pat 2 as basis, and so the stochastic model here will use exponential distributions for $ sigma$ and $ gamma$. . #collapse_hide # Define parameters for stochastic model days = 200 p = 10000 num_E = 1 num_I = 0 num_R = 0 beta_stoch = 0.5*np.ones(days) # Define parameters for deterministic model init = 1-(num_E/p)-(num_I/p)-(num_R/p), num_E/p, num_I/p, num_R/p sigma = 1/5.2 # 1/5 --&gt; 5 days on average to go from E --&gt; I beta_det = 0.5 gam = 1/28.85 # 1/11 --&gt; 11 days on average to go from I --&gt; R parms = sigma, beta_det, gam # Run deterministic simulation results_avg = seir_model(init, parms, days) # Run 3 stochastic simulations results_stoch1 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, &#39;expon&#39;, &#39;expon&#39;) results_stoch2 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, &#39;expon&#39;, &#39;expon&#39;) results_stoch3 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, &#39;expon&#39;, &#39;expon&#39;) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;S_det&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[0], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;E_det&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[1], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;I_det&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[2], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;R_det&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[3], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;S_stoch1&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[0]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;E_stoch1&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[1]/p, line={&#39;dash&#39;:&#39;dot&#39;,&#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;I_stoch1&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;R_stoch1&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[3]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;S_stoch2&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[0]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;E_stoch2&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[1]/p, line={&#39;dash&#39;:&#39;dot&#39;,&#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;I_stoch2&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;R_stoch2&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[3]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;S_stoch3&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[0]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;E_stoch3&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[1]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;I_stoch3&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;R_stoch3&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[3]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch3&quot;) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Proportion of population&#39;, title={ &#39;text&#39;:r&#39;$ text{Effect of stochasticity on Deterministic SEIR model}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We can see very similar curves. The stochasticity appears to influence the time at which the epidemic starts but not the shape of the curves. . $ sigma$: exponential or gamma distribution . In this section we want to examine the effect of a gamma distribution has on the SEIR model (we keep exponential distribution for $ gamma$). . #collapse_hide # Define parameters for stochastic model days = 200 p = 10000 num_E = 1 num_I = 0 num_R = 0 beta_stoch = 0.5*np.ones(days) # Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigma results_stoch0 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, &#39;expon&#39;, &#39;expon&#39;) results_stoch1 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, &#39;expon&#39;, &#39;expon&#39;) results_stoch2 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;expon&#39;) results_stoch3 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;expon&#39;) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;S_stoch_exp1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[0]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;E_stoch_exp1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[1]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;I_stoch_exp1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[2]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;R_stoch_exp1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[3]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;S_stoch_exp2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[0]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;E_stoch_exp2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[1]/p, line={&#39;dash&#39;:&#39;solid&#39;,&#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;I_stoch_exp2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;R_stoch_exp2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[3]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;S_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[0]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;E_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[1]/p, line={&#39;dash&#39;:&#39;dot&#39;,&#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;I_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;R_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[3]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;S_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[0]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;E_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[1]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;I_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;R_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[3]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch3&quot;) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Proportion of population&#39;, title={ &#39;text&#39;:r&#39;$ text{Effect of gamma vs. exponential distributed } sigma text{ on SEIR model}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . As you can see here, it is difficult to tell how much the gamma distributed $ sigma$ differs from the exponential distributed model (other than just timing). . The infectious peak might be a little lower and delayed a bit with gama distribution, but it is hard to tell for sure from this. . The peak of exposed individuals seems to be a bit higher and delayed with gamma distribution versus exponential distribution. . $ gamma$: exponential, gamma, or Weibull distribution . In this section we want to examine the effect of having $T_{Infectious}$ be gamma or Weibull distribution on the SEIR model. . Exponential vs. Gamma . #collapse_hide # Define parameters for stochastic model days = 200 p = 10000 num_E = 1 num_I = 0 num_R = 0 beta_stoch = 0.5*np.ones(days) # Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigma results_stoch0 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;expon&#39;) results_stoch1 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;expon&#39;) results_stoch2 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;gamma&#39;) results_stoch3 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;gamma&#39;) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;S_stoch_exp1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[0]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;E_stoch_exp1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[1]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;I_stoch_exp1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[2]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;R_stoch_exp1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[3]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;S_stoch_exp2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[0]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;E_stoch_exp2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[1]/p, line={&#39;dash&#39;:&#39;solid&#39;,&#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;I_stoch_exp2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;R_stoch_exp2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[3]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;S_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[0]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;E_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[1]/p, line={&#39;dash&#39;:&#39;dot&#39;,&#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;I_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;R_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[3]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;S_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[0]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;E_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[1]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;I_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;R_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[3]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch3&quot;) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Proportion of population&#39;, title={ &#39;text&#39;:r&#39;$ text{Effect of gamma vs. exponential distributed } gamma text{ on SEIR model}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . As you can see here, it is a lot easier to differentiate between the two. . A gamma distributed $ gamma$ results in a higher peak of infectious people and underlines how using the usual deterministic models can vastly underestimate peak infectious people. . Gamma vs. Weibull . #collapse_hide # Define parameters for stochastic model days = 200 p = 10000 num_E = 1 num_I = 0 num_R = 0 beta_stoch = 0.5*np.ones(days) # Run 4 stochastic simulations, 2 with exponential sigma, 2 with gamma sigma results_stoch0 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;weibull&#39;) results_stoch1 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;weibull&#39;) results_stoch2 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;gamma&#39;) results_stoch3 = seir_model_stoch(beta_stoch, p, num_E, num_I, num_R, days, 1, &#39;gamma&#39;) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;S_stoch_wei1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[0]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;E_stoch_wei1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[1]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;I_stoch_wei1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[2]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;R_stoch_wei1&#39;, x=np.arange(len(results_stoch0[0])), y=results_stoch0[3]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;det&quot;), go.Scatter(name=&#39;S_stoch_wei2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[0]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;E_stoch_wei2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[1]/p, line={&#39;dash&#39;:&#39;solid&#39;,&#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;I_stoch_wei2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[2]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;R_stoch_wei2&#39;, x=np.arange(len(results_stoch1[0])), y=results_stoch1[3]/p, line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch1&quot;), go.Scatter(name=&#39;S_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[0]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;E_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[1]/p, line={&#39;dash&#39;:&#39;dot&#39;,&#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;I_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[2]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;R_stoch_gam1&#39;, x=np.arange(len(results_stoch2[0])), y=results_stoch2[3]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch2&quot;), go.Scatter(name=&#39;S_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[0]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;E_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[1]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;I_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[2]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;stoch3&quot;), go.Scatter(name=&#39;R_stoch_gam2&#39;, x=np.arange(len(results_stoch3[0])), y=results_stoch3[3]/p, line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;stoch3&quot;) ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Proportion of population&#39;, title={ &#39;text&#39;:r&#39;$ text{Effect of Weibull vs. gamma distributed } gamma text{ on SEIR model}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . Overall both the gamma and Weibull distributions were very close to the actual distribution for COVID-19 $T_{Infectious}$ so it makes sense that the simulations results in similar curbs here. . Impact of distribution of $T_{Infectious}$ on Infectious Peak . In the plots above we can see the peak of infectious individuals is higher in the simulations done with Gamma or Weibull distributions than in those done with the exponential distribution. . Note we have not changed anything for $ beta$ and in the simulations above we have the following: . Exponential distribution: $$E[T_{Infectious}] = 28.85 days$$ $$R_0 = beta * E[T_{Infectious}] = 14.43$$ | Gamma distribution: $$E[T_{Infectious}] = 20.05 days$$ $$R_0 = beta * E[T_{Infectious}] = 10.03$$ | Weibull distribution: $$E[T_{Infectious}] = 20.77 days$$ $$R_0 = beta * E[T_{Infectious}] = 10.39$$ | . So while we have a higher $R_0$ when using the exonential distribution for $T_{Infectious}$, the peak of infectious individuals is lower than in the simulations using gamma and Weibull distributions with lower $R_0$. . We had previously seen that increasing $R_0$ resulted in high infectious peaks, but this is only true when comparing similar distributions. . Discussion . We can see the actual distribution of $ sigma$ and $ gamma$ carry importance in the resulting SEIR models. . $R_0$ . In part 1 we saw that $R_0$ was fully characterized by $ beta$ and $ gamma$ in the sense that $$R_0 = frac{ beta}{ gamma}$$ . We can clearly see here however that $R_0$ is not a good enough measure the indicate peak infectious individuals - which is closely related to the peak number of sick individuals which in turn determines required sanitary resources. . The actual distribution of $T_{Infectious}$ mus tbe taken into account to estimate true values of peaks. . Further questions . A couple questions are left to be answered: . How can we control the spread of an epidemic? | How can we evaluate $ beta$ from the data collected on a population level? | . See further blog posts. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/probability%20distributions/modeling/seir/epidemiology/stochastic/covid-19/2020/03/31/stochastic_model.html",
            "relUrl": "/probability%20distributions/modeling/seir/epidemiology/stochastic/covid-19/2020/03/31/stochastic_model.html",
            "date": "  Mar 31, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Epidemic modeling - Part 3",
            "content": ". Motivation for write-up . This is the 3rd part of a multi-part series blog post on modeling in epidemiology. . The COVID-19 pandemic has brought a lot of attention to study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points. . After introducing the concepts of compartmentalization and disease dynamics in the first blog post, the second part looked at a deterministic numerical solution for the SEIR model discussed, and the effects of the parameters $ beta$, $ sigma$, and $ gamma$. . While arguments can be made that the compartments themselves don&#39;t reflect the reality of COVID-19, this is not the point of this discussion; I want to focus on the idea that the population level dynamics forget about the individual progression of the disease. . With this mind, this third part is going to discuss the problems that arise when averaging the latent period ($ frac{1}{ sigma}$) and infectious period ($ frac{1}{ gamma}$) on the simulations. . Let&#39;s have a look at the individual progression of disease to understand what is wrong. . Implications of deterministic model . Latent period $= T_{Latent} = frac{1}{ sigma}$ . Using the numerical model in part 2 and in order to see the distribution of E &rarr; I, we set the initial number of E to be the same as the population, and plot the number of E over time as below: . #collapse_hide # Define parameters days = 30 N = 10000 init = 0, N, 0, 0 sigma = 1/5.2 beta = 0.5 gam = 1/28.85 parms = sigma, beta, gam # Plot simulation fig = go.Figure(data=[ go.Scatter(name=&#39;E to I&#39;, x=np.linspace(0,days,days*10), y=100*(1-seir_model(init, parms, days).T[1]/N)), go.Scatter(name=&#39;$ text{Exponential distribution with} Scale = frac{1}{ sigma}$&#39;, x=np.arange(days), y=100*expon.cdf(np.arange(days),loc=0,scale=1/sigma)) ]) fig.update_layout( title=&#39;Number of E moving to I over time when all population is exposed on day 0&#39;, xaxis_title=&#39;Days&#39;, yaxis_title=&#39;Percent of exposed having become infectious&#39;, legend=dict( x=0.6, y=0, traceorder=&quot;normal&quot;, ) ) fig.show() . . . . The plot above confirms the numerical model from part 2 assumes people go from E &rarr; I according to the exponential distribution. . Infectious period $= T_{Infectious} = frac{1}{ gamma}$ . The same discussion above applies for the time from I &rarr; R here. . From the discussion above, we know the numerical model in part 2 approximates the time from I &rarr; R as an exponential distribution. . Let&#39;s verifiy this in the plot below: . #collapse_hide # Define parameters days = 100 N = 10000 init = 0, 0, N, 0 sigma = 1/5.2 # 1/5 --&gt; 5 days on average to go from E --&gt; I beta = 0.5 gam = 1/28.85 # 1/11 --&gt; 11 days on average to go from I --&gt; R parms = sigma, beta, gam # Plot simulation fig = go.Figure(data=[ go.Scatter(name=&#39;I to R&#39;, x=np.linspace(0,days,days*10), y=100*(1-seir_model(init, parms, days).T[2]/N)), go.Scatter(name=&#39;$ text{Exponential distribution with} Scale = frac{1}{ gamma}$&#39;, x=np.arange(days), y=100*expon.cdf(np.arange(days),loc=0,scale=1/gam)) ]) fig.update_layout( title=&#39;Number of I moving to R over time when all population is infectious on day 0&#39;, xaxis_title=&#39;Days&#39;, yaxis_title=&#39;Percent of infectious having become recovered&#39;, legend=dict( x=0.6, y=0, traceorder=&quot;normal&quot;, ) ) fig.show() . . . . The plot above confirms the numerical model from part 2 assumes people go from I &rarr; R according to the exponential distribution. . Comparing exponential distribution to COVID-19 data . As we have seen above, this deterministic model implies $T_{Latent}$ and $T_{Infectious}$ are exponentially distributed and we know the exponential distribution is uniquely characterized by its scale where: $$scale = frac{1}{mean}$$ . Latent period . For COVID-19, as we have seen in part 2 of the blog, research has shown the following for $T_{Latent}$: . mean = 5.2 days | range is [2,14] days | 95th percentile is 12.5 days | . Assuming an exponential distribution, however, we would obtain the following: . $mean = frac{1}{scale} = 5.2 days$ | 95th percentile would be 16 days | After the first day in state E, 18% would move into the state I (the fastest in real-world data was 2 days so this is not possible) | . While we can adjust to scale to fit the real-world mean, the distribution does not match the real-world data. . Infectious period . Similarly as above, for COVID-19 we have seen research has shown the following for $T_{Infectious}$: . median = 20 days | range is [8,37] days | . Assuming an exponential distribution, however, we would obtain the following: . $mean = frac{median}{ ln2} = frac{20}{ ln2} = 28.85 days$ | 95th percentile would be 87 days, while we&#39;d likely want it to be around 37 days | After the first day in state I, 18% would move into the state R (the fastest in real-world data was 8 days so this is not possible) | . While we can adjust to scale to fit the real-world mean, the distribution does not match the real-world data - and for a parameter that influences the overall simulation, it is pretty far off. . Let&#39;s see what distribution looks more likely. . Finding a better fit: Gamma or Weibull distributions? . We have seen how different the actual COVID-19 $T_{Latent}$ and $T_{Infectious}$ were from the deterministic model using exponential distributions. . Here we want to find a better distribution, and one that immediatly comes to mind is the Gamma distribution. . Another is the Weibull distribution. . Characterizing the Gamma distribution . The gamma distribution is characterized by its shape parameter $k$ and its scale parameter $ theta$, where: $$Mean = k~ theta$$ . Characterizing the Weibull distribution . Similarly, the Weibull distribution is characterized by its shape parameter $k$ and its scale parameter $ theta$, where: $$Mean= lambda Gamma left(1+{ frac {1}{k}} right)$$ And: $$Median = lambda ( ln 2)^{1/k}$$ . Gamma distributed latent period . Let&#39;s first find a Gamma distribution to match $T_{Latent}$ data for COVID-19. . The mean is 5.2 days. . The range is [2,14] days and 95th percentile is 12.5 days, so we could translate this as follows: . 5th percentile = 2 days | 95th percentil = 12.5 days | . $$Mean = k~ theta$$ $$ leftrightarrow k~ theta = 5.2$$ $$ leftrightarrow k = frac{5.2}{ theta}$$ . We find the following parameters result in a pretty close distribution: . $loc = 1.8$ | $k = 0.9$ | $ theta = frac{5.2-loc}{k} = 3. dot{7}$ | . #collapse_hide p=100000 days=30 k=0.9 locg=1.8 theta=(5.2-locg)/k scalee=5.2 df = pd.DataFrame({ &#39;Exponential&#39;: expon.rvs(scale=scalee,size=p), &#39;Gamma&#39;: gamma.rvs(k,loc=locg,scale=theta,size=p) }) t=PrettyTable([&#39;Distribution&#39;, &#39;Mean&#39;, &#39;Median&#39;, &#39;5th percentile&#39;, &#39;95th percentile&#39;]) t.add_row([&#39;Exponential&#39;, df.Exponential.mean(), df.Exponential.median(), df.Exponential.quantile(q=0.05), df.Exponential.quantile(q=0.95)]) t.add_row([&#39;Gamma&#39;, df.Gamma.mean(), df.Gamma.median(), df.Gamma.quantile(q=0.05), df.Gamma.quantile(q=0.95)]) print(t) fig = go.Figure(data=[ go.Scatter(name=&#39;Gamma E --&gt; I&#39;, x=np.arange(days), y=gamma.cdf(np.arange(days), k, loc=locg, scale=theta), line={&#39;color&#39;:&#39;red&#39;}), go.Scatter(name=&#39;Expon E --&gt; I&#39;, x=np.arange(days), y=expon.cdf(np.arange(days), scale=scalee), line={&#39;color&#39;:&#39;blue&#39;}), ]) fig.update_layout( title={ &#39;text&#39;:&#39;Exponential vs. Gamma CDF&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; }, xaxis_title=&#39;Days&#39;, yaxis_title=&#39;Percent of exposed having become infectious&#39;, legend=dict( x=1, y=0, traceorder=&quot;reversed&quot;, ) ) fig.show() . . +--+-+-++--+ | Distribution | Mean | Median | 5th percentile | 95th percentile | +--+-+-++--+ | Exponential | 5.207469481987089 | 3.594278143717058 | 0.27075286628595824 | 15.622868963942889 | | Gamma | 5.196922877842177 | 4.054896063203861 | 1.9329997424767378 | 12.339859622859123 | +--+-+-++--+ . . . #collapse_hide p=10000 k=0.9 locg=1.8 theta=(5.2-locg)/k scalee=5.2 df = pd.DataFrame({ &#39;Exponential&#39;: expon.rvs(scale=scalee,size=p), &#39;Gamma&#39;: gamma.rvs(k,loc=locg,scale=theta,size=p) }) fig = px.histogram(df.stack().reset_index().rename(columns={&quot;level_1&quot;: &quot;Distribution&quot;}), x=0, color=&quot;Distribution&quot;, marginal=&#39;box&#39;) fig.update_layout( title={ &#39;text&#39;:&#39;Exponential vs. Gamma distributions&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; }, xaxis_title=&#39;Days&#39;, yaxis_title=&#39;Count&#39;, legend=dict( x=1, y=0, traceorder=&quot;normal&quot;, ) ) fig.show() . . . . Gamma or Weibull distributed infectious period . While we used a Gamma distribution for $T_{Latent}$ above, we do not have a mean for $T_{Infectious}$ data for COVID-19. We can still try to find a Gamma distribution that matches but it may be a bit more difficult to do. . With the median however, we could use the Weibull distribution as described earlier. . $$Median = lambda ( ln 2)^{1/k}$$ $$ leftrightarrow lambda = frac{Median}{( ln 2)^{1/k}}$$ $$ leftrightarrow lambda = frac{20}{( ln 2)^{1/k}}$$ . The range is [8,37] days so we could translate this as follows: . 5th percentile = 8 days | 95th percentil = 37 days | . We find the following parameters result in a pretty close Gamma distribution: . $loc = 3$ | $k = 4$ | $ theta = 4.25$ | . Similarly, we find the following parameters result in a pretty close Weibull distribution: . $loc = 2$ | $k = 2.3$ | $ lambda = frac{20-2}{( ln 2)^{1/k}} = 21.11$ | . #collapse_hide p = 10000 days=80 k=4 locg=3 theta=(20-locg)/k locw=2 wk = 2.3 wl = (20-locw)/(math.log(2)**(1/wk)) loce=0 scale=28.85-loce df = pd.DataFrame({ &#39;Exponential&#39;: expon.rvs(loc=loce, scale=scale,size=p), &#39;Gamma&#39;: gamma.rvs(k,loc=locg,scale=theta,size=p), &#39;Weibull&#39;: weibull_min.rvs(wk, loc=locw, scale=wl,size=p) }) t=PrettyTable([&#39;Distribution&#39;, &#39;Mean&#39;, &#39;Median&#39;, &#39;5th percentile&#39;, &#39;95th percentile&#39;]) t.add_row([&#39;Exponential&#39;, df.Exponential.mean(), df.Exponential.median(), df.Exponential.quantile(q=0.05), df.Exponential.quantile(q=0.95)]) t.add_row([&#39;Gamma&#39;, df.Gamma.mean(), df.Gamma.median(), df.Gamma.quantile(q=0.05), df.Gamma.quantile(q=0.95)]) t.add_row([&#39;Weibull&#39;, df.Weibull.mean(), df.Weibull.median(), df.Weibull.quantile(q=0.05), df.Weibull.quantile(q=0.95)]) print(t) fig = go.Figure(data=[ go.Scatter(name=&#39;Expon I --&gt; R&#39;, x=np.arange(days), y=expon.cdf(np.arange(days), loc=loce, scale=scale), line={&#39;color&#39;:&#39;blue&#39;}), go.Scatter(name=&#39;Gamma I --&gt; R&#39;, x=np.arange(days), y=gamma.cdf(np.arange(days), k, loc=locg, scale=theta), line={&#39;color&#39;:&#39;red&#39;}), go.Scatter(name=&#39;Weibull I --&gt; R&#39;, x=np.arange(days), y=weibull_min.cdf(np.arange(days), wk, loc=locw, scale=wl), line={&#39;color&#39;:&#39;green&#39;}) ]) fig.update_layout( title={ &#39;text&#39;:&#39;Exponential vs. Gamma vs. Weibull CDF&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; }, xaxis_title=&#39;Days&#39;, yaxis_title=&#39;Percent of exposed having become infectious&#39;, legend=dict( x=1, y=0, traceorder=&quot;normal&quot;, ) ) fig.show() . . +--+--+--+--+-+ | Distribution | Mean | Median | 5th percentile | 95th percentile | +--+--+--+--+-+ | Exponential | 28.476757690391107 | 19.996705193640878 | 1.5333146455434519 | 85.121378779367 | | Gamma | 20.075920288429813 | 18.6884046455234 | 8.612411926089568 | 36.12390356768179 | | Weibull | 20.624411927557794 | 19.93798874457553 | 7.7287057647939 | 35.71926543212359 | +--+--+--+--+-+ . . . #collapse_hide k=4 locg=3 theta=(20-locg)/k locw=2 wk = 2.3 wl = (20-locw)/(math.log(2)**(1/wk)) loce=0 scalee=28.85-loce p=10000 df = pd.DataFrame({ &#39;Exponential&#39;: expon.rvs(loc=loce, scale=scalee,size=p), &#39;Gamma&#39;: gamma.rvs(k,loc=locg,scale=theta,size=p), &#39;Weibull&#39;: weibull_min.rvs(wk, loc=locw, scale=wl,size=p) }) fig = px.histogram(df.stack().reset_index().rename(columns={&quot;level_1&quot;: &quot;Distribution&quot;}), x=0, color=&quot;Distribution&quot;, marginal=&#39;box&#39;) fig.update_layout( title={ &#39;text&#39;:&#39;Exponential vs. Gamma vs. Weibull distributions&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; }, xaxis_title=&#39;Days&#39;, yaxis_title=&#39;Count&#39;, legend=dict( x=1, y=0, traceorder=&quot;normal&quot;, ) ) fig.show() . . . . Discussion . $T_{Latent}$ is nicely matched with a Gamma distribution. . $T_{Infectious}$ is nicely matched by a Weibull distribution. . The take away however is that the exponential distribution matches neither - and in the case of $T_{Infectious}$ it is very far off. . We have seen in the previous posts that both of these periods have an impact on the peak proportion of infectious people and the duration of that peak. . Naturally, we need to investigate further the impact of changing the distributions from exponential to Gamma and Weibull on the simulations. . This is done in the next blog post where I build a new model to be able to take into account the actual distributions. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/probability%20distributions/modeling/seir/epidemiology/2020/03/25/proba-distrib.html",
            "relUrl": "/probability%20distributions/modeling/seir/epidemiology/2020/03/25/proba-distrib.html",
            "date": "  Mar 25, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Epidemic modeling - Part 2",
            "content": ". Motivation for write-up . This is the 2nd part of a multi-part series blog post on modeling in epidemiology. . The COVID-19 pandemic has brought a lot of attention to the study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on two key points. . After introducing the concepts of compartmentalization and disease dynamics in the first blog post, this second part is focused on developing a deterministic numerical solution for the SEIR model discussed there. . While normally the goal is to use real-world data to infer characteristics of the underlying disease (as will be done in later blog posts), here we want to use simulate the spread of a COVID-19 like disease in a population of 10000, and look at the effects of the different parameters on the spread. . Recall SEIR model equations . See the first blog post for derivation. . Continuous-time: . $ frac{ds(t)}{dt}=- beta i(t) s(t)$ | $ frac{de(t)}{dt}= beta i(t) s(t) - sigma e(t)$ | $ frac{di(t)}{dt}= sigma e(t) - gamma i(t)$ | $ frac{dr(t)}{dt}= gamma i(t)$ | . | Discrete-time: . $ Delta S = - beta I S Delta T$ | $ Delta E = ( beta I S - sigma E) Delta T$ | $ Delta I = ( sigma E - gamma I) Delta T$ | $ Delta R = gamma I Delta T$ | . | . Numerical solution to this deteministic population level model . Coding the SEIR model . To build the SEIR model we simply use the discrete-time set of equations above. . The model will thus take as input the following: . Initial proportion of S, E, I, and R in the population | $ beta$ parameter pertaining to the population in question | $ sigma$ and $ gamma$ parameters pertaining to the disease | Numbers of days to run the simulation | . #collapse_hide # Import required libraries import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go . . # Let&#39;s build a numerical solution def seir_model(init, parms, days): S_0, E_0, I_0, R_0 = init Epd, Ipd, Rpd = [0], [0], [0] S, E, I, R = [S_0], [E_0], [I_0], [R_0] dt=0.1 t = np.linspace(0,days,int(days/dt)) sigma, beta, gam = parms for _ in t[1:]: next_S = S[-1] - beta*S[-1]*I[-1]*dt Epd.append(beta*S[-1]*I[-1]*dt) next_E = E[-1] + (beta*S[-1]*I[-1] - sigma*E[-1])*dt Ipd.append(sigma*E[-1]*dt) next_I = I[-1] + (sigma*E[-1] - gam*I[-1])*dt Rpd.append(gam*I[-1]*dt) next_R = R[-1] + (gam*I[-1])*dt S.append(next_S) E.append(next_E) I.append(next_I) R.append(next_R) return np.stack([S, E, I, R, Epd, Ipd, Rpd]).T . COVID-19 parameters . Simulation parameters used for plot below: . Days = 100 | Population = 10000 | Number of susceptible people on day 0 = 9999 | Number of exposed people on day 0 = 1 | No infected or recovered people on day 0 | . A lot of research is ongoing into the COVID-19 characteristics of $ beta$, $ sigma$, and $ gamma$. . However, these are complex studies that require a lot of data and so far we have little information to go on. . The literature suggests the following: . $ underline{T_{Incubation}}$: | . The mean is 5-6 days but it can range anywhere from 2-14 days 1 2 . Another paper reports a mean incubation period of 5.2 days and the 95th percentile at 12.5 days 3. . There are reports of pre-symptomatic infections[^2], but these are reportedly rare [^1] so in the following models we will assume: $$T_{Incubation} = T_{Latent}$$ And so: $$ sigma = frac{1}{5.2} days^{-1}$$ . $ underline{T_{Infectious}}$: | . Again it is very difficult to say for sure and the period of communicability is very uncertain for COVID-19. . Research suggests a median of 20 days of viral shedding after onset of symptoms 4. . Ranging from 8 to 37 days in survivors. . While it is noted PCR positivity does not necessarily reflect the infectious period (virus may not be viable but the PCR amplification will result in a positive), for the purpose of this blog post we will assume the following: $$T_{Infectious} = T_{Clinical}$$ To obtain an exponential distribution with median M, the scale A is calculated as follows: $$A = frac{M}{ ln2} = frac{20}{ ln2}$$ This results in $$ gamma = frac{ ln2}{20} = frac{1}{28.85} days^{-1}$$ . $ underline{Beta= beta}$: | . While difficult to estimate this parameter as there is a lot of variation between countries, cultures, societal norms, etc.. a little thought experiment can help us evaluate the value for $ beta = r rho$ in Switerland or France for example. . If no control measures are put in place and people do not change habits (as is the case in this blog post), we can expect the following: . Average number of contacts per day: | $$r = 10 contacts per day$$ . Average probability of transmission from contact: | $$ rho = 5 %$$ . And so: $$ beta = r rho = 0.5$$ . . WHO COVID-19 Situation Report 73&#8617; . | CDC COVID-19 FAQ&#8617; . | Early Transmission Dynamics in Wuhan, China, of Novel CoronavirusInfected Pneumonia&#8617; . | Clinical course and mortality risk of server COVID-1930633-4/fulltext)&#8617; . | Running the simulation . #collapse_hide #Define parameters days = 200 N = 10000 init = 1 - 1/N, 1/N, 0, 0 sigma = 1/5.2 beta = 0.5 gam = 1/28.85 parms = sigma, beta, gam # Run simulation results_avg = seir_model(init, parms, days) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=&#39;S&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[0], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}), go.Scatter(name=&#39;E&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[1], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;yellow&#39;}), go.Scatter(name=&#39;I&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[2], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}), go.Scatter(name=&#39;R&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[3], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Proportion of population&#39;, title={ &#39;text&#39;:&#39;Deterministic SEIR model - COVID-19 parameters&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . Qualitative analysis of $ beta$, $ sigma$, and $ gamma$ . Effect of $ sigma$ ($T_{Latent}$) . Let&#39;s have a look at the effect of $ sigma$ (or inversely, the latent period) on the SEIR simulation. . A higher $ sigma$ means shorter average latent period, and vice-versa. . #collapse_hide ## Let&#39;s try to see how the model changes days = 1000 N = 10000 init = 1 - 1/N, 1/N, 0, 0 sigma_high = 1 # 1 --&gt; Average 1 day from E --&gt; I (ressembles SIR model) sigma_low = 1/100 #10 days on average, twice as long as COVID-19 sigma_covid = 1/5.2 beta = 0.5 gam = 1/28.85 parms_fastEI = sigma_high, beta, gam parms_slowEI = sigma_low, beta, gam parms_avg = sigma_covid, beta, gam # Run simulation results_fastEtoI = seir_model(init, parms_fastEI, days) results_slowEtoI = seir_model(init, parms_slowEI, days) results_avg = seir_model(init, parms_avg, days) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=r&#39;$S: sigma_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[0], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;COVID&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[1], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;COVID&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[2], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;COVID&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[3], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;COVID&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_fastEtoI.T[0], line={&#39;dash&#39;:&#39;dash&#39;,&#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;high&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_fastEtoI.T[1], line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;high&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_fastEtoI.T[2], line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;high&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_fastEtoI.T[3], line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;high&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_slowEtoI.T[0], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;slow&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_slowEtoI.T[1], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;slow&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_slowEtoI.T[2], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;slow&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: sigma_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_slowEtoI.T[3], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;slow&quot;, hoverinfo=&#39;x+y&#39;), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Proportion of population&#39;, title={ &#39;text&#39;:r&#39;$ text{Effect of } sigma text{on Deterministic SEIR model}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We notice a few things from the plot above on the impact of the average time from E &rarr; I: . The shorter the latent period: the faster the epidemic propogates in the population | the higher the peak of infected individuals will be (meaning a higher chance hospital resources will be saturated) | . | However, the latent period has no impact on the total number of individuals infected over the entire time of the epidemic. | . Effect of $ beta = r~ rho$ . Let&#39;s have a look at the effect of $ beta$ on the SEIR simulation. . A higher $ beta$ can either mean a higher average number of contacts per day ($r$) in the population and/or a higher probability of transmission of disease from I &rarr; S. . The opposite holds also. . #collapse_hide ## Let&#39;s try to see how the model changes days = 500 N = 10000 init = 1 - 1/N, 1/N, 0, 0 sigma_avg = 1/5.2 beta_avg = 0.5 beta_noepi = 1/30 beta_low = 0.1 beta_high = 4 gam = 1/28.85 parms_avg = sigma_avg, beta_avg, gam parms_noepi = sigma_avg, beta_noepi, gam parms_low = sigma_avg, beta_low, gam parms_high = sigma_avg, beta_high, gam # Run simulation results_avg = seir_model(init, parms_avg, days) results_noepi = seir_model(init, parms_noepi, days) results_low = seir_model(init, parms_low, days) results_high = seir_model(init, parms_high, days) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=r&#39;$S: beta_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[0], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;COVID&quot;), go.Scatter(name=r&#39;$E: beta_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[1], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;COVID&quot;), go.Scatter(name=r&#39;$I: beta_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[2], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;COVID&quot;), go.Scatter(name=r&#39;$R: beta_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[3], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;COVID&quot;), go.Scatter(name=r&#39;$S: beta_{noepi}$&#39;, x=np.linspace(0,days,days*10), y=results_noepi.T[0], line={&#39;dash&#39;:&#39;dashdot&#39;,&#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;noepi&quot;), go.Scatter(name=r&#39;$E: beta_{noepi}$&#39;, x=np.linspace(0,days,days*10), y=results_noepi.T[1], line={&#39;dash&#39;:&#39;dashdot&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;noepi&quot;), go.Scatter(name=r&#39;$I: beta_{noepi}$&#39;, x=np.linspace(0,days,days*10), y=results_noepi.T[2], line={&#39;dash&#39;:&#39;dashdot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;noepi&quot;), go.Scatter(name=r&#39;$R: beta_{noepi}$&#39;, x=np.linspace(0,days,days*10), y=results_noepi.T[3], line={&#39;dash&#39;:&#39;dashdot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;noepi&quot;), go.Scatter(name=r&#39;$S: beta_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_low.T[0], line={&#39;dash&#39;:&#39;dash&#39;,&#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;low&quot;), go.Scatter(name=r&#39;$E: beta_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_low.T[1], line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;low&quot;), go.Scatter(name=r&#39;$I: beta_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_low.T[2], line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;low&quot;), go.Scatter(name=r&#39;$R: beta_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_low.T[3], line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;low&quot;), go.Scatter(name=r&#39;$S: beta_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_high.T[0], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;high&quot;), go.Scatter(name=r&#39;$E: beta_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_high.T[1], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;high&quot;), go.Scatter(name=r&#39;$I: beta_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_high.T[2], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;high&quot;), go.Scatter(name=r&#39;$R: beta_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_high.T[3], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;high&quot;), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Proportion of population&#39;, title={ &#39;text&#39;:r&#39;$ text{Effect of } beta text{on Deterministic SEIR model}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We notice a few things from the plot above on the impact of $ beta$: . The higher $ beta$ is: the faster the epidemic seems to propogate in the population | the higher the peak of infected individuals seems to be (meaning a higher chance hospital resources will be saturated) | . | $ beta$ also appears to affect the overall number of people infected over the course of the epidemic A low $ beta$ means a low $R_0$ and we have seen in the first part of this blog that no epidemic occurs when $R_0 &lt; 1$ | But even if $R_0 &gt; 1$, keeping $ beta$ low reduces the total number of people infected | . | . Effect of $ gamma$ ($T_{Infectious}$) . Let&#39;s have a look at the effect of $ gamma$ on the SEIR simulation. . A higher $ gamma$ means a shorter infectious period, and vice-versa. . #collapse_hide ## Let&#39;s try to see how the model changes days = 500 N = 10000 init = 1 - 1/N, 1/N, 0, 0 sigma_avg = 1/5.2 beta = 0.5 gam_avg = 1/28.85 gam_low = 1/200 gam_high = 0.2 parms_fastIR = sigma_avg, beta, gam_high parms_slowIR = sigma_avg, beta, gam_low parms_avg = sigma_avg, beta, gam_avg # Run simulation results_fastItoR = seir_model(init, parms_fastIR, days) results_slowItoR = seir_model(init, parms_slowIR, days) results_avg = seir_model(init, parms_avg, days) . . #collapse_hide fig = go.Figure(data=[ go.Scatter(name=r&#39;$S: gamma_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[0], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;COVID&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[1], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;COVID&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[2], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;COVID&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{COVID}$&#39;, x=np.linspace(0,days,days*10), y=results_avg.T[3], line={&#39;dash&#39;:&#39;solid&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;COVID&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_fastItoR.T[0], line={&#39;dash&#39;:&#39;dash&#39;,&#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;fast&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_fastItoR.T[1], line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;fast&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_fastItoR.T[2], line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;fast&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{high}$&#39;, x=np.linspace(0,days,days*10), y=results_fastItoR.T[3], line={&#39;dash&#39;:&#39;dash&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;fast&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_slowItoR.T[0], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;blue&#39;}, legendgroup=&quot;slow&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_slowItoR.T[1], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;yellow&#39;}, legendgroup=&quot;slow&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_slowItoR.T[2], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;red&#39;}, legendgroup=&quot;slow&quot;, hoverinfo=&#39;x+y&#39;), go.Scatter(name=r&#39;$S: gamma_{low}$&#39;, x=np.linspace(0,days,days*10), y=results_slowItoR.T[3], line={&#39;dash&#39;:&#39;dot&#39;, &#39;color&#39;:&#39;green&#39;}, legendgroup=&quot;slow&quot;, hoverinfo=&#39;x+y&#39;), ]) fig.update_layout( xaxis_title = &#39;Day&#39;, yaxis_title = &#39;Proportion of population&#39;, title={ &#39;text&#39;:r&#39;$ text{Effect of } gamma text{on Deterministic SEIR model}$&#39;, &#39;x&#39;:0.5, &#39;xanchor&#39;:&#39;center&#39; } ) fig.show() . . . . We notice a few things from the plot above on the impact of the infectious period: . The longer the infectious period: the faster the epidemic propogates in the population | the higher the peak of infectious individuals will be and the longer it will last (meaning a higher chance hospital resources will be saturated) | . | As opposed to the latent period above, but similarly as $ beta$, the infectious period has an impact on the total number of individuals infected over the entire time of the epidemic With no epidemic if $ gamma &gt; beta$ | . | . Discussion . So we can see the latent and infectious periods, along with the value of $ beta$ are critical components in how the model will react. . Worth noting also is that the higher $R_0$ is, the faster the epidemic spreads and the higher the peak of infectious individuals will be (see further blog posts for some nuance on this). . Notably, and as predicted in part 1 of the blog series, no epidemic occurs if: $$R_0 &lt; 1$$ In other words, no epidemic if: $$ beta &lt; gamma$$ . There are major flaws with this model however. While this model is deterministic and uses average time to model $ sigma$ and $ gamma$, this is a major flaw and does not represent the reality for most diseases. . Part 3 of this blog series will discuss this further. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/modeling/seir/epidemiology/2020/03/18/deterministic-numerical-solutions.html",
            "relUrl": "/modeling/seir/epidemiology/2020/03/18/deterministic-numerical-solutions.html",
            "date": "  Mar 18, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Epidemic modeling - Part 1",
            "content": ". Motivation for write-up . This is the 1st part of a multi-part series blog post on modeling in epidemiology. . The COVID-19 pandemic has brought a lot of attention to the study of epidemiology and more specifically to the various mathematical models that are used to inform public health policies. Everyone has been trying to understand the growth or slowing of new cases and trying to predict the necessary sanitary resources. This blog post attempts to explain the foundations for some of the most used models and enlighten the reader on a few important points. . In this first post I want to introduce the concept of compartmentalization and how it forms the basis for studying disease dynamics on the population level. . How to model infectious diseases on population level ? . Compartments . When modelling infectious diseases, and pandemics in particular, a key ask is to predict the number of infected people at any given time in order to estimate the sanitary resources that will be necessary. . From this simple qestion results the idea of compartmentalization of the population i.e. the division of the population into the two most basic categories: . those that are infected | those that are not | . This is ultimately the foundation for all compartmental models in epidemiology. . The nuances between the models then come from how the above two groups are further compartmentalized. That is to say, how we decide the composition of the infected and the not-infected groups. . For example, the non-infected group could be further sub-categorized into: . Susceptible | Immune | . And the infected group into: . Asymptomatic | Symptomatic | . Or, another option, into: . No treatment necessary | Require treatment: Local Doctor visit | Hospitalization | Admitted to intensive care unit | . | . As you can see there are many ways to do this, but the more categories you have, the more difficult it might become to model. Usually we determine these subcategories in order to match available data. . Dynamics . While the compartments describe the state any individual can be in at a certain point in time, the dynamics describe the ways in which the compartments interact with each other. . I want to underline the separation between disease dynamics on the individual level, and that on the population level below. . Individual level disease dynamics: | . This describes, on the individual level, the progression of the disease i.e. how one person can go from one state to another (one compartment to another) . For example: how does a healthy person become ill and what is the clinical course of the disease for this person? . Population-level dynamics: | . On the other hand, the population level dynamics describe, on a population level, how the total number of individuals in each compartment vary over time. . We will see more on this in the next blog posts. . Two simple examples - the SIR and SEIR models . Let&#39;s have a look at a basic compartmental model, first the SIR model. . S --&gt; Susceptible state: | . An S individual is simply someone susceptible to the disease, that is anyone in the population who is healthy and not immune to the disease. . I --&gt; Infectious state: | . Once an individual is exposed to the disease he will develop this disease and become infectious. . R --&gt; Recovered state: | . An individual will either fight off the infection (with the help or not of treatment) or die. These are all included in the R state. . In the basic SIR model, anyone R has aquired full and infinite immunity and cannot catch the disease again (of course many variations can be included to reflect more closely a disease). . In this write-up and in the following blog posts we will focus on the SEIR models, which are similar to the SIR compartments above with the additional E state between S and I. . E --&gt; Exposed state: | . The exposed state is the state when an individual has been exposed to the disease, but has not become infectious yet. . Some important vocabulary . $ underline{Infectious period:}$ | . Also called the period of communicability, the infectious period is the time during which an individual can transmit the disease to another: $T_{Infectious}$ . $ underline{Clinical infection period:}$ | . This period corresponds to the period where the infected indvidual shows symptoms: $T_{Clinical}$ . $ underline{Latent period:}$ | . The latent period is the time between exposure of an individual and the start of the period of communicability of that individual: $T_{Latent}$ . $ underline{Incubation period:}$ | . The incubation period on the other hand, is the time from exposure of an individual to development of the infection (appearance of disease): $T_{Incubation}$ . It should be noted the latent period and incubation period are not necessarily the same. . $ underline{T_{Latent} &lt; T_{Incubation}}$: | . In this case, an individual who has been exposed becomes infectious before the development of disease. . We call this a subclinical infection and during that time the individual is called an asymptomatic carrier. . $ underline{T_{Latent} &gt; T_{Incubation}:}$ | . In other cases, the latent period can be longer than the incubation period, eg: smallpox. . $ underline{T_{Latent} + T_{Infectious} &gt; T_{Incubation} + T_{Clinical}:}$ | . Another case of subclinical infection resulting in asymptomatic carriers occurs when the end of clinical infection (of disease) happens earlier than the end of the period of communicability (see Wikipedia figure below) . Overall, these asymptomatic carriers can be a significant difficutly to overcome epidemics. . . $ underline{Basic reproduction number:}$ | . The basic reproduction number $R_0$ is the measure of secondary infections in a susceptible population. . In other words, it is the number of people that each infectious individual will infect over the time of their infectious period. . Example: . If an infectious individual infects 3 other individuals over the course of his infection, his $R_0$ is 3. . This number is a very important element in the spreading dynamics (see derivation below). . A closer look at the SEIR model . Individual-level disease dynamic . As explained above, the individual-level disease dynamic describes the progression of disease within an individual i.e. the progression of an individual from one state to another. . In the models used here (SEIR model), an individual starts at S (although an initial exposed or infectious person is injected into the population at time t=0). . | If exposed to the disease he will move into the state E. . | After which he will move to the I state with probability 1, but in a time unique to himself. . | Again after which he will move into the state R with probability 1, and again in a time unique to him. . | From state R he will stay in state R (either dead or has aquired full and inifite immunity). . | . Let&#39;s have a closer look: . S &rarr; E . The chances of an individual going from S &rarr; E depends on three things: . the number of contacts the person has per unit time (given by $r$) | the chance a given contact is with an I - infectious individual (the higher the number of I in the population, the higher the chance) | the chance of an S contracting the disease from a contact with an I (given by $ rho$) | E &rarr; I . The latent period . All people exposed will eventually develop disease. . However, individually, a person might go from E to I on the first day, or after 10 days, this is unique to the individual. . Every additional day following exposure the probability of this individual to go from E &rarr; I increases (we will have a look at the probability distribution and its importance later). . I &rarr; R . The period of communicability . Similarly, all infectious people will recover (or die). . Again, individually, a person might go from I to R in 5 days or in 15 days, this time is the recovery time and is proper to the individual. . Population level dynamics . Most basic models tend to disregard the notion of individual dynamics above in favor of poopulation level dynamics. . That is to say the models tend to model disease on a population level without looking at the specific pogression of disease within the individuals and using averages instead (although the S &rarr; E uses the same logic as above). . Below is an explanation for such an SEIR model with its mathematical formulation. . Note no births or deaths are included. . S &rarr; E . As stated above, going from S to E on a particular day depends on these three characteristics: . the proportion of infectious people in the population on that day: $i(t) = frac{I(t)}{N}$ | the number of contacts an individual has per day: $r$ | the chance for an S to contract the disease after contact with an I: $ rho$ | . We can combine the last two into $ beta = r rho$ . On a population-level however, the number of S that will become E also depends on the proportion of S in the population (of course if there are no S, no one will become E of course). . So we add the following requirement: . the proportion of susceptible people in the population on that day: $s(t) = frac{S(t)}{N}$ | . So the change in the number of S in a population on a given day is equal to: . $- beta i(t) s(t)$ . (note the negative sign to indicate the number of S is diminishing as they become exposed) . Hence we can formulate this mathematically as follows: . Discrete-time: $$ Delta S = - beta I S Delta T$$ | Continuous-time: $$ frac{ds(t)}{dt}=- beta i(t) s(t)$$ | . E &rarr; I . We have seen above how each individual goes from E to I. . On a population level, the number of E changes in two ways: . new additions following S &rarr; E | reduction following E &rarr; I | . We already know the number from S &rarr; E is: . $$ beta i(t) s(t)$$ . So how can we model the number of E &rarr; I? . While individually this is a bit more complicated to model and pertains to the specific probability distribution of the latent period, on a population level we can use the average time it takes - this is what most models do (part 3 of this blog post will show why this is wrong for COVID-19). . Let&#39;s say average latent period is . $$ frac{1}{ sigma}$$ . then we know that every unit time that goes by, we have . $$ sigma E$$ . individuals that transition from E &rarr; I. . Mathematically, we write this as : . Discrete-time: $$ Delta E = ( beta I S- sigma E) Delta T$$ | Continuous-time: $$ frac{de(t)}{dt}= beta i(t) s(t) - sigma e(t)$$ | . I &rarr; R . Similarly as above, we have seen above how each individual goes from I to R but this does not tell us about the population level dynamics. . On a population level, the number of I changes in two ways: . new additions following E &rarr; I | reduction following I &rarr; R | . We know the number from E &rarr; I is: . $$ sigma e(t)$$ . How can we model the number of I &rarr; R? . Again, while individually this is complicated, on a population level, how about averaging out the period of infectiousness, this is what most models do. . Let&#39;s say average time of infectiousness is . $$ frac{1}{ gamma}$$ . Then we have: . Discrete-time: $$ Delta I = ( sigma E - gamma I) Delta T$$ | Continuous-time: $$ frac{di(t)}{dt}= sigma e(t) - gamma i(t)$$ | . R &rarr; R . Finally, it is simple to model the number of individuals in R state with the following equation: . Discrete-time: $$ Delta R = gamma I Delta T$$ | Continuous-time: $$ frac{dr(t)}{dt}= gamma i(t)$$ | . Effective and Basic Reproduction Numbers: $R$ and $R_0$ respectively . As stated above, $R_0$ is the measure of secondary infections. Let&#39;s have a look how we can characterize it. . Understanding how the infection spreads . Any individual in state I (infectious) will contaminate others according to the following: . Number of contacts the individual has per day given by: $r_i$ | Probability to infect an S after contact given by: $ rho_i$ | Probability of a contact being with an S given by: $ frac{S(t)}{N} = s(t)$ | The period of infectiousness of the individual given by [$j_i, j_i+ frac{1}{ tau_i}$] (where $j_i$ is the first day of infectiousness for that individual and $ frac{1}{ tau_i}$ is that individuals&#39; time of infectiousness) | . Remember $r_i rho_i= beta_i$ . Derivation of $R$ for each individual . Let&#39;s call the measure of $R$ for any individual $R_i$. . From the parameters above we can write $R_i$ for each infectious individual as the sum of secondary infections per day of infectiousness as below: . Discrete-time: $$R_i = sum_{Day=j_i}^{j_i+ frac{1}{ tau_i}} beta_i frac{S(Day)}{N}$$ *Continuous-time: $$R_i = int_{j_i}^{j_i+ frac{1}{ tau_i}} beta_i s(t) dt$$ | . Finding $R_0$ of each individual by making assumptions . $R_{0,i}$ is the measure of $R_i$ in a susceptible population, i.e. when: $$S = N$$ . In other words: $$R_{0,i} = R_i ~ frac{N}{S}$$ . If we make the following assumptions: . s(t) is constant over the course of infectiousness of an individual: $$s(t) = s(t+ frac{1}{ tau_i})$$ | $ beta_i$ is a constant and does not vary over the course of time (no control measures) | . Then the equation for $R_{0,i}$ reduces to the following: $$R_{0,i} = [ beta_i]_{j_i}^{j_i+ frac{1}{ tau_i}} = frac{ beta_i}{ tau_i}$$ . We can see the basic reproduction number of an individual is fully characterized by the $ beta_i$ and the $ tau_i$ of that individual. . $R_0$ for a population . To a generalize to a population-level, we can simply find the expected value for the equation above: $$R_0 = E[R_{0,i}] = frac{E[ beta_i]}{E[ tau_i]}$$ . Assuming: . $E[ beta_i] = beta$ | $E[ tau_i] = gamma$ | . We can write: $$R_0 = frac{ beta}{ gamma}$$ . Herd Immunity . As just described, $R$ is the measure of secondary infections, and $R_0$ is the measure of secondary infections in a susceptible population where $R=R_0~s(t)$. . It is easy to understand that if each infectious individual contaminates less than 1 other individual on average ($R &lt; 1$) then the number of exposed, and eventually infectious, individuals will diminish and tend to 0. . On the other hand, if each infectious individual contaminates more than 1 other individual ($R &gt; 1$) then the number of infectious individuals will rise (chance of epidemic). . Mathematical formulation: . $$ frac{d~e(t)}{dt} = beta~i(t)~s(t) - gamma~i(t)$$ $$ leftrightarrow frac{d~e(t)}{dt} = R_0~ gamma~i(t)~s(t) - gamma~i(t)$$ $$ leftrightarrow frac{d~e(t)}{dt} = gamma~i(t)~(R_0~s(t) - 1)= gamma~i(t)~(R - 1)$$ . And so we find that in a population where $ gamma(t)&gt;0$: $$ frac{d~e(t)}{dt} = 0$$ $$ leftrightarrow R-1 = 0$$ $$ leftrightarrow R = 1$$ . If $R&lt;1$ then $ frac{d~e(t)}{dt} &lt; 0$ . Herd immunity threshold: . The herd immunity threshold is the point at which enough of the population is immune to the disease (not susceptible) in order to have $R &lt; 1$ and can be calculated as follows: $$R = R_0 ~ s(t)$$ . We know the proportion of the population immune to the disease is: $$Immune(t) = 1-s(t)$$ $$ leftrightarrow s(t)=1-Immune(t)$$ . The threshold of $R = 1$ is achieved when: $$R_0 ~s(t) = 1$$ $$ leftrightarrow R_0 ~(1-Immune(t)) = 1$$ $$ leftrightarrow 1-Immune(t) = frac{1}{R_0}$$ $$ leftrightarrow Immune(t) = 1- frac{1}{R_0}$$ . When the proportion of immune individuals in a population reaches $1- frac{1}{R_0}$ then $R$ will become smaller than 1 and the number of infectious individuals will diminish and tend to 0. . Conclusion . This was a brief introduction to compartmentalization models and the dynamics associated with them. . Of course these sort of derivations can be done for many different types of comprtaments and their relevant dynamics, but the SEIR is simple enough to understand and model quickly. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/modeling/compartmentalization/seir/epidemiology/disease%20dynamics/2020/03/15/compartmentalization.html",
            "relUrl": "/modeling/compartmentalization/seir/epidemiology/disease%20dynamics/2020/03/15/compartmentalization.html",
            "date": "  Mar 15, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "COVID-19 Map",
            "content": ". Link to tracker . Follow this link. . About . There are already many great ways to visualize the spread of sars-cov-2 around the world. Ive made another one for family and friends that has the world on one page and France on another. . You can click on the countries for more details on the world map. . Likewise you can click on specific departments to see more detail on the French map. . The world map uses data aggregated by the John Hopkins University CSSE. . The french map uses hospitalization and intensive care data from Sante Publique France. . Future blog post about building the site . The site is built using Plotly Dash and deployed to Heroku. . I will likely make a blog post detailing how to build the app and deploy it. .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/covid-19/2020/03/12/COVID-19-Tracker.html",
            "relUrl": "/covid-19/2020/03/12/COVID-19-Tracker.html",
            "date": "  Mar 12, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote. . 2. This is the other footnote. You can even have a link! .",
            "url": "https://jeffufpost.github.io/scattered-thoughts/jupyter/2020/01/07/test.html",
            "relUrl": "/jupyter/2020/01/07/test.html",
            "date": "  Jan 7, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Homepage . Electrical engineer by background, but enthusiast of all things data, electronics, governance, development, global health, skiing, football, and many other things. .",
          "url": "https://jeffufpost.github.io/scattered-thoughts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ sitemap.xml | absolute_url }} | .",
          "url": "https://jeffufpost.github.io/scattered-thoughts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}